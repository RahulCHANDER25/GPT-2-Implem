{
    "cells": [
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Transformers from scratch"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<a href=\"https://colab.research.google.com/github/EffiSciencesResearch/ML4G-2.0/blob/master/workshops/transformer/transformer-arena.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
                "\n",
                "Thanks to Callum McDougall for this Notebook.\n",
                "\n",
                "This notebook should take 2 full days to complete, you should not do parts 3 and 4. **Do only part 1 and 2**.\n",
                "We also advise you not to look too much at the bonus or collapsed nerd-sniping stuff."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<img src=\"https://raw.githubusercontent.com/callummcdougall/TransformerLens-intro/main/images/page_images/transformer-building.png\" width=\"350\">\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Introduction"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "This is a clean, first principles implementation of GPT-2 in PyTorch. The architectural choices closely follow those used by the TransformerLens library (which you'll be using a lot more in later exercises).\n",
                "\n",
                "Each exercise will have a difficulty and importance rating out of 5, as well as an estimated maximum time you should spend on these exercises and sometimes a short annotation. You should interpret the ratings & time estimates relatively (e.g. if you find yourself spending about 50% longer on the exercises than the time estimates, adjust accordingly). Please do skip exercises / look at solutions if you don't feel like they're important enough to be worth doing, and you'd rather get to the good stuff!\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Content & Learning Objectives\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### 1️⃣ Understanding Inputs & Outputs of a Transformer\n",
                "\n",
                "In this section, we'll take a first look at transformers - what their function is, how information moves inside a transformer, and what inputs & outputs they take.\n",
                "\n",
                "> ##### Learning objectives\n",
                ">\n",
                "> - Understand what a transformer is used for\n",
                "> - Understand causal attention, and what a transformer's output representsalgebra operations on tensors\n",
                "> - Learn what tokenization is, and how models do it\n",
                "> - Understand what logits are, and how to use them to derive a probability distribution over the vocabulary\n",
                "\n",
                "#### 2️⃣ Clean Transformer Implementation\n",
                "\n",
                "Here, we'll implement a transformer from scratch, using only PyTorch's tensor operations. This will give us a good understanding of how transformers work, and how to use them. We do this by going module-by-module, in an experience which should feel somewhat similar to last week's ResNet exercises. Much like with ResNets, you'll conclude by loading in pretrained weights and verifying that your model works as expected.\n",
                "\n",
                "> ##### Learning objectives\n",
                ">\n",
                "> * Understand that a transformer is composed of attention heads and MLPs, with each one performing operations on the residual stream\n",
                "> * Understand that the attention heads in a single layer operate independently, and that they have the role of calculating attention patterns (which determine where information is moved to & from in the residual stream)\n",
                "> * Learn about & implement the following transformer modules:\n",
                ">     * (Bonus) LayerNorm (transforming the input to have zero mean and unit variance)\n",
                ">     * Positional embedding (a lookup table from position indices to residual stream vectors)\n",
                ">     * Attention (the method of computing attention patterns for residual stream vectors)\n",
                ">     * MLP (the collection of linear and nonlinear transformations which operate on each residual stream vector in the same way)\n",
                ">     * Embedding (a lookup table from tokens to residual stream vectors)\n",
                ">     * Unembedding (a matrix for converting residual stream vectors into a distribution over tokens)\n",
                "\n",
                "#### 3️⃣ (Bonus) Training a Transformer\n",
                "\n",
                "Next, you'll learn how to train your transformer from scratch. This will be quite similar to the training loops you wrote for ResNet in your first week.\n",
                "\n",
                "> ##### Learning objectives\n",
                ">\n",
                "> * Understand how to train a transformer from scratch\n",
                "> * Write a basic transformer training loop\n",
                "> * Interpret the transformer's falling cross entropy loss with reference to features of the training data (e.g. bigram frequencies)\n",
                "\n",
                "#### 4️⃣ (Bonus) Sampling from a Transformer\n",
                "\n",
                "Lastly, you'll learn how to sample from a transformer. This will involve implementing a few different sampling methods, and writing a caching system which can reuse computations from previous forward passes to improve your model's text generation speed.\n",
                "\n",
                "*The second half of this section is less important, and you can skip it if you want.*\n",
                "\n",
                "> ##### Learning objectives\n",
                ">\n",
                "> * Learn how to sample from a transformer\n",
                ">     * This includes basic methods like greedy search or top-k, and more advanced methods like beam search\n",
                "> * Learn how to cache the output of a transformer, so that it can be used to generate text more efficiently\n",
                ">     * Optionally, rewrite your sampling functions to make use of your caching methods\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Setup (don't read, just run!)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "# %pip install transformer_lens einops jaxtyping circuitsvis"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Loaded pretrained model gpt2-small into HookedTransformer\n",
                        "Moving model to device:  cpu\n"
                    ]
                }
            ],
            "source": [
                "import os\n",
                "\n",
                "# os.environ['ACCELERATE_DISABLE_RICH'] = \"1\"\n",
                "# os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
                "import sys\n",
                "import einops\n",
                "from dataclasses import dataclass\n",
                "from transformer_lens import HookedTransformer\n",
                "from transformer_lens.utils import gelu_new, tokenize_and_concatenate\n",
                "import torch\n",
                "from torch import Tensor\n",
                "import torch.nn as nn\n",
                "import numpy as np\n",
                "import math\n",
                "from tqdm.notebook import tqdm\n",
                "from typing import Tuple, List, Optional, Dict\n",
                "from jaxtyping import Float, Int\n",
                "from transformers.models.gpt2.tokenization_gpt2_fast import GPT2TokenizerFast\n",
                "from collections import defaultdict\n",
                "from rich.table import Table\n",
                "from rich import print as rprint\n",
                "import datasets\n",
                "from torch.utils.data import DataLoader\n",
                "import wandb\n",
                "from pathlib import Path\n",
                "import webbrowser\n",
                "\n",
                "device = torch.device(\"cpu\")\n",
                "\n",
                "reference_gpt2 = HookedTransformer.from_pretrained(\n",
                "    \"gpt2-small\", fold_ln=False, center_unembed=False, center_writing_weights=False\n",
                ").to(device)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 1️⃣ Understanding Inputs & Outputs of a Transformer\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "> ### Learning Objectives\n",
                ">\n",
                "> * Understand what a transformer is used for\n",
                "> * Understand causal attention, and what a transformer's output represents\n",
                "> * Learn what tokenization is, and how models do it\n",
                "> * Understand what logits are, and how to use them to derive a probability distribution over the vocabulary\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## What is the point of a transformer?\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Transformers exist to model text!**\n",
                "\n",
                "We're going to focus GPT-2 style transformers. Key feature: They generate text! You feed in language, and the model generates a probability distribution over tokens. And you can repeatedly sample from this to generate text!\n",
                "\n",
                "(To explain this in more detail - you feed in a sequence of length $N$, then sample from the probability distribution over the $N+1$-th word, use this to construct a new sequence of length $N+1$, then feed this new sequence into the model to get a probability distribution over the $N+2$-th word, and so on.)\n",
                "\n",
                "### How is the model trained?\n",
                "\n",
                "You give it a bunch of text, and train it to predict the next token.\n",
                "\n",
                "Importantly, if you give a model 100 tokens in a sequence, it predicts the next token for *each* prefix, i.e. it produces 100 logit vectors (= probability distributions) over the set of all words in our vocabulary, with the `i`-th logit vector representing the probability distribution over the token *following* the `i`-th token in the sequence.\n",
                "\n",
                "<details>\n",
                "<summary>Aside - logits</summary>\n",
                "\n",
                "If you haven't encountered the term \"logits\" before, here's a quick refresher.\n",
                "\n",
                "Given an arbitrary vector $x$, we can turn it into a probability distribution via the **softmax** function: $x_i \\to \\frac{e^{x_i}}{\\sum e^{x_j}}$. The exponential makes everything positive; the normalization makes it add to one.\n",
                "\n",
                "The model's output is the vector $x$ (one for each prediction it makes). We call this vector a logit because it represents a probability distribution, and it is related to the actual probabilities via the softmax function.\n",
                "</details>\n",
                "\n",
                "How do we stop the transformer by \"cheating\" by just looking at the tokens it's trying to predict? Answer - we make the transformer have *causal attention* (as opposed to *bidirectional attention*). Causal attention only allows information to move forwards in the sequence, never backwards. The prediction of what comes after token 50 is only a function of the first 50 tokens, *not* of token 51. We say the transformer is **autoregressive**, because it only predicts future words based on past data.\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<img src=\"https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/transformer-overview-new.png\" width=\"900\">\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Tokens - Transformer Inputs\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Our tranformer's input is natural language (i.e. a sequence of characters, strings, etc). But ML models generally take vectors as input, not langage. How do we convert language to vectors?\n",
                "\n",
                "We can factor this into 2 questions:\n",
                "\n",
                "1. How do we split up language into small sub-units?\n",
                "2. How do we convert these sub-units into vectors?\n",
                "\n",
                "Let's start with the second of these questions.\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Converting sub-units to vectors\n",
                "\n",
                "We basically make a massive lookup table, which is called an **embedding**. It has one vector for each possible sub-unit of language we might get (we call this set of all sub-units our **vocabulary**). We label every element in our vocabulary with an integer (this labelling never changes), and we use this integer to index into the embedding.\n",
                "\n",
                "A key intuition is that one-hot encodings let you think about each integer independently. We don't bake in any relation between words when we perform our embedding, because every word has a completely separate embedding vector.\n",
                "\n",
                "<details>\n",
                "<summary>Aside - one-hot encodings</summary>\n",
                "\n",
                "We sometimes think about **one-hot encodings** of words. These are vectors with zeros everywhere, except for a single one in the position corresponding to the word's index in the vocabulary. This means that indexing into the embedding is equivalent to multiplying the **embedding matrix** by the one-hot encoding (where the embedding matrix is the matrix we get by stacking all the embedding vectors on top of each other).\n",
                "\n",
                "$$\n",
                "\\begin{aligned}\n",
                "W_E &= \\begin{bmatrix}\n",
                "\\leftarrow v_0 \\rightarrow \\\\\n",
                "\\leftarrow v_1 \\rightarrow \\\\\n",
                "\\vdots \\\\\n",
                "\\leftarrow v_{d_{vocab}-1} \\rightarrow \\\\\n",
                "\\end{bmatrix} \\quad \\text{is the embedding matrix (size }d_{vocab} \\times d_{embed}\\text{),} \\\\\n",
                "\\\\\n",
                "t_i &= (0, \\dots, 0, 1, 0, \\dots, 0) \\quad \\text{is the one-hot encoding for the }i\\text{th word (length }d_{vocab}\\text{)} \\\\\n",
                "\\\\\n",
                "v_i &= t_i W_E \\quad \\text{is the embedding vector for the }i\\text{th word (length }d_{embed}\\text{).} \\\\\n",
                "\\end{aligned}\n",
                "$$\n",
                "\n",
                "</details>\n",
                "\n",
                "Now, let's answer the first question - how do we split language into sub-units?\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### (Bonus) Splitting language into sub-units\n",
                "\n",
                "We need to define a standard way of splitting up language into a series of substrings, where each substring is a member of our **vocabulary** set.\n",
                "\n",
                "Could we use a dictionary, and have our vocabulary be the set of all words in the dictionary? No, because this couldn't handle arbitrary text (e.g. URLs, punctuation, etc). We need a more general way of splitting up language.\n",
                "\n",
                "Could we just use the 256 ASCII characters? This fixes the previous problem, but it loses structure of language - some sequences of characters are more meaningful than others. For example, \"language\" is a lot more meaningful than \"hjksdfiu\". We want \"language\" to be a single token, but not \"hjksdfiu\" - this is a more efficient use of our vocab.\n",
                "\n",
                "What actually happens? The most common strategy is called **Byte-Pair encodings**.\n",
                "\n",
                "We begin with the 256 ASCII characters as our tokens, and then find the most common pair of tokens, and merge that into a new token. Note that we do have a space character as one of our 256 tokens, and merges using space are very common. For instance, here are the five first merges for the tokenizer used by GPT-2 (you'll be able to verify this below).\n",
                "\n",
                "```\n",
                "\" t\"\n",
                "\" a\"\n",
                "\"he\"\n",
                "\"in\"\n",
                "\"re\"\n",
                "```\n",
                "\n",
                "Note - you might see the character `Ġ` in front of some tokens. This is a special character that indicates that the token begins with a space. Tokens with a leading space vs not are different.\n",
                "\n",
                "You can run the code below to see some more of GPT-2's tokenizer's vocabulary:\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "[('!', 0), ('\"', 1), ('#', 2), ('$', 3), ('%', 4), ('&', 5), (\"'\", 6), ('(', 7), (')', 8), ('*', 9), ('+', 10), (',', 11), ('-', 12), ('.', 13), ('/', 14), ('0', 15), ('1', 16), ('2', 17), ('3', 18), ('4', 19)]\n",
                        "\n",
                        "[('ľ', 250), ('Ŀ', 251), ('ŀ', 252), ('Ł', 253), ('ł', 254), ('Ń', 255), ('Ġt', 256), ('Ġa', 257), ('he', 258), ('in', 259), ('re', 260), ('on', 261), ('Ġthe', 262), ('er', 263), ('Ġs', 264), ('at', 265), ('Ġw', 266), ('Ġo', 267), ('en', 268), ('Ġc', 269)]\n",
                        "\n",
                        "[('Ġprodu', 990), ('Ġstill', 991), ('led', 992), ('ah', 993), ('Ġhere', 994), ('Ġworld', 995), ('Ġthough', 996), ('Ġnum', 997), ('arch', 998), ('imes', 999), ('ale', 1000), ('ĠSe', 1001), ('ĠIf', 1002), ('//', 1003), ('ĠLe', 1004), ('Ġret', 1005), ('Ġref', 1006), ('Ġtrans', 1007), ('ner', 1008), ('ution', 1009)]\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "sorted_vocab = sorted(list(reference_gpt2.tokenizer.vocab.items()), key=lambda n: n[1])\n",
                "print(sorted_vocab[:20])\n",
                "print()\n",
                "print(sorted_vocab[250:270])\n",
                "print()\n",
                "print(sorted_vocab[990:1010])\n",
                "print()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "As you get to the end of the vocabulary, you'll be producing some pretty weird-looking esoteric tokens (because you'll already have exhausted all of the short frequently-occurring ones):\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "[('Revolution', 50237), ('Ġsnipers', 50238), ('Ġreverted', 50239), ('Ġconglomerate', 50240), ('Terry', 50241), ('794', 50242), ('Ġharsher', 50243), ('Ġdesolate', 50244), ('ĠHitman', 50245), ('Commission', 50246), ('Ġ(/', 50247), ('âĢ¦.\"', 50248), ('Compar', 50249), ('Ġamplification', 50250), ('ominated', 50251), ('Ġregress', 50252), ('ĠCollider', 50253), ('Ġinformants', 50254), ('Ġgazed', 50255), ('<|endoftext|>', 50256)]\n"
                    ]
                }
            ],
            "source": [
                "print(sorted_vocab[-20:])"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Transformers in the `transformer_lens` library have a `to_tokens` method that converts text to numbers. It also prepends them with a special token called BOS (beginning of sequence) to indicate the start of a sequence. You can disable this with the `prepend_bos=False` argument.\n",
                "\n",
                "\n",
                "\n",
                "### Some tokenization annoyances\n",
                "\n",
                "There are a few funky and frustrating things about tokenization, which causes it to behave differently than you might expect. For instance:\n",
                "\n",
                "#### Whether a word begins with a capital or space matters!\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "['<|endoftext|>', 'R', 'alph']\n",
                        "['<|endoftext|>', ' Ralph']\n",
                        "['<|endoftext|>', ' r', 'alph']\n",
                        "['<|endoftext|>', 'ral', 'ph']\n"
                    ]
                }
            ],
            "source": [
                "print(reference_gpt2.to_str_tokens(\"Ralph\"))\n",
                "print(reference_gpt2.to_str_tokens(\" Ralph\"))\n",
                "print(reference_gpt2.to_str_tokens(\" ralph\"))\n",
                "print(reference_gpt2.to_str_tokens(\"ralph\"))"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### Arithmetic is a mess.\n",
                "\n",
                "Length is inconsistent, common numbers bundle together.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "['<|endoftext|>', '568', '73', '+', '318', '46', '23', '=', '123', '45', '67', '89', '-', '1', '000000', '000']\n"
                    ]
                }
            ],
            "source": [
                "print(reference_gpt2.to_str_tokens(\"56873+3184623=123456789-1000000000\"))"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "> ### Key Takeaways\n",
                ">\n",
                "> * We learn a dictionary of vocab of tokens (sub-words).\n",
                "> * We (approx) losslessly convert language to integers via tokenizing it.\n",
                "> * We convert integers to vectors via a lookup table.\n",
                "> * Note: input to the transformer is a sequence of *tokens* (ie integers), not vectors\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Text generation\n",
                "\n",
                "Now that we understand the basic ideas here, let's go through the entire process of text generation, from our original string to a new token which we can append to our string and plug back into the model.\n",
                "\n",
                "#### **Step 1:** Convert text to tokens\n",
                "\n",
                "The sequence gets tokenized, so it has shape `[batch, seq_len]`. Here, the batch dimension is just one (because we only have one sequence).\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "tensor([[50256,    40,   716,   281,  4998,  1960,   382, 19741,    11,   875,\n",
                        "         12342,    12,  8807,    11,   402, 11571,    12,    17,  3918, 47385,\n",
                        "            13,  1881,  1110,   314,   481,  7074,  1692,  1241,  4430,   290,\n",
                        "          1011,   625,   262,   995,     0]])\n",
                        "torch.Size([1, 35])\n",
                        "['<|endoftext|>', 'I', ' am', ' an', ' amazing', ' aut', 'ore', 'gressive', ',', ' dec', 'oder', '-', 'only', ',', ' G', 'PT', '-', '2', ' style', ' transformer', '.', ' One', ' day', ' I', ' will', ' exceed', ' human', ' level', ' intelligence', ' and', ' take', ' over', ' the', ' world', '!']\n"
                    ]
                }
            ],
            "source": [
                "reference_text = \"I am an amazing autoregressive, decoder-only, GPT-2 style transformer. One day I will exceed human level intelligence and take over the world!\"\n",
                "tokens = reference_gpt2.to_tokens(reference_text).to(device)\n",
                "print(tokens)\n",
                "print(tokens.shape)\n",
                "print(reference_gpt2.to_str_tokens(tokens))"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### **Step 2:** Map tokens to logits\n",
                "\n",
                "\n",
                "From our input of shape `[batch, seq_len]`, we get output of shape `[batch, seq_len, vocab_size]`. The `[i, j, :]`-th element of our output is a vector of logits representing our prediction for the `j+1`-th token in the `i`-th sequence.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "torch.Size([1, 35, 50257])\n"
                    ]
                }
            ],
            "source": [
                "logits, cache = reference_gpt2.run_with_cache(tokens)\n",
                "print(logits.shape)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "(`run_with_cache` tells the model to cache all intermediate activations. This isn't important right now; we'll look at it in more detail later.)\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### **Step 3:** Convert the logits to a distribution with a softmax\n",
                "\n",
                "This doesn't change the shape, it is still `[batch, seq_len, vocab_size]`.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "torch.Size([1, 35, 50257])\n"
                    ]
                }
            ],
            "source": [
                "probs = logits.softmax(dim=-1)\n",
                "print(probs.shape)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### **Bonus step:** What is the most likely next token at each position?\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "          token --> next_token\n",
                        "'<|endoftext|>' --> '\\n'\n",
                        "            'I' --> \"'m\"\n",
                        "          ' am' --> ' a'\n",
                        "          ' an' --> ' avid'\n",
                        "     ' amazing' --> ' person'\n",
                        "         ' aut' --> 'od'\n",
                        "          'ore' --> 'sp'\n",
                        "     'gressive' --> '.'\n",
                        "            ',' --> ' and'\n",
                        "         ' dec' --> 'ently'\n",
                        "         'oder' --> ','\n",
                        "            '-' --> 'driven'\n",
                        "         'only' --> ' programmer'\n",
                        "            ',' --> ' and'\n",
                        "           ' G' --> 'IM'\n",
                        "           'PT' --> '-'\n",
                        "            '-' --> 'only'\n",
                        "            '2' --> '.'\n",
                        "       ' style' --> ','\n",
                        " ' transformer' --> '.'\n",
                        "            '.' --> ' I'\n",
                        "         ' One' --> ' of'\n",
                        "         ' day' --> ' I'\n",
                        "           ' I' --> ' will'\n",
                        "        ' will' --> ' be'\n",
                        "      ' exceed' --> ' my'\n",
                        "       ' human' --> 'ly'\n",
                        "       ' level' --> ' of'\n",
                        "' intelligence' --> ' and'\n",
                        "         ' and' --> ' I'\n",
                        "        ' take' --> ' over'\n",
                        "        ' over' --> ' the'\n",
                        "         ' the' --> ' world'\n",
                        "       ' world' --> '.'\n",
                        "            '!' --> ' I'\n"
                    ]
                }
            ],
            "source": [
                "most_likely_next_tokens = reference_gpt2.tokenizer.batch_decode(logits.argmax(dim=-1)[0])\n",
                "\n",
                "print(\"          token\", \"-->\", \"next_token\")\n",
                "for token, next_token in zip(reference_gpt2.to_str_tokens(tokens), most_likely_next_tokens):\n",
                "    print(f\"{token!r:>15} --> {next_token!r}\")"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We can see that, in a few cases (particularly near the end of the sequence), the model accurately predicts the next token in the sequence. We might guess that `\"take over the world\"` is a common phrase that the model has seen in training, which is why the model can predict it.\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### **Step 4:** Map distribution to a token\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "' I'\n"
                    ]
                }
            ],
            "source": [
                "next_token = logits[0, -1].argmax(dim=-1)\n",
                "next_char = reference_gpt2.to_string(next_token)\n",
                "print(repr(next_char))"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Note that we're indexing `logits[0, -1]`. This is because logits have shape `[1, sequence_length, vocab_size]`, so this indexing returns the vector of length `vocab_size` representing the model's prediction for what token follows the **last** token in the input sequence.\n",
                "\n",
                "In this case, we can see that the model predicts the token `' I'`.\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### **Step 5:** Add this to the end of the input, re-run\n",
                "\n",
                "There are more efficient ways to do this (e.g. where we cache some of the values each time we run our input, so we don't have to do as much calculation each time we generate a new value), but this doesn't matter conceptually right now.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Sequence so far: '<|endoftext|>I am an amazing autoregressive, decoder-only, GPT-2 style transformer. One day I will exceed human level intelligence and take over the world!'\n",
                        "36th char = ' I'\n",
                        "37th char = ' am'\n",
                        "38th char = ' a'\n",
                        "39th char = ' very'\n",
                        "40th char = ' talented'\n",
                        "41th char = ' and'\n",
                        "42th char = ' talented'\n",
                        "43th char = ' person'\n",
                        "44th char = ','\n",
                        "45th char = ' and'\n"
                    ]
                }
            ],
            "source": [
                "print(f\"Sequence so far: {reference_gpt2.to_string(tokens)[0]!r}\")\n",
                "\n",
                "for i in range(10):\n",
                "    print(f\"{tokens.shape[-1]+1}th char = {next_char!r}\")\n",
                "    # Define new input sequence, by appending the previously generated token\n",
                "    tokens = torch.cat([tokens, next_token[None, None]], dim=-1)\n",
                "    # Pass our new sequence through the model, to get new output\n",
                "    logits = reference_gpt2(tokens)\n",
                "    # Get the predicted token at the end of our sequence\n",
                "    next_token = logits[0, -1].argmax(dim=-1)\n",
                "    # Decode and print the result\n",
                "    next_char = reference_gpt2.to_string(next_token)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Key takeaways\n",
                "\n",
                "* Transformer takes in language, predicts next token (for *each* token in a causal way)\n",
                "* We convert language to a sequence of integers with a tokenizer.\n",
                "* We convert integers to vectors with a lookup table.\n",
                "* Output is a vector of logits (one for each input token), we convert to a probability distn with a softmax, and can then convert this to a token (eg taking the largest logit, or sampling).\n",
                "* We append this to the input + run again to generate more text (Jargon: *autoregressive*)\n",
                "* Meta level point: Transformers are sequence operation models, they take in a sequence, do processing in parallel at each position, and use attention to move information between positions!\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 2️⃣ Clean Transformer Implementation\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "> ##### Learning objectives\n",
                ">\n",
                "> * Understand that a transformer is composed of attention heads and MLPs, with each one performing operations on the residual stream\n",
                "> * Understand that the attention heads in a single layer operate independently, and that they have the role of calculating attention patterns (which determine where information is moved to & from in the residual stream)\n",
                "> * Learn about & implement the following transformer modules:\n",
                ">     * (Bonus) LayerNorm (transforming the input to have zero mean and unit variance)\n",
                ">     * Positional embedding (a lookup table from position indices to residual stream vectors)\n",
                ">     * Attention (the method of computing attention patterns for residual stream vectors)\n",
                ">     * MLP (the collection of linear and nonlinear transformations which operate on each residual stream vector in the same way)\n",
                ">     * Embedding (a lookup table from tokens to residual stream vectors)\n",
                ">     * Unembedding (a matrix for converting residual stream vectors into a distribution over tokens)\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## High-Level architecture\n",
                "\n",
                "Go watch Neel's [Transformer Circuits walkthrough](https://www.youtube.com/watch?v=KV5gbOmHbjU) if you want more intuitions!\n",
                "\n",
                "(Diagram is bottom to top.)\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<img src=\"https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/transformer-new.png\" width=\"850\">\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Tokenization & Embedding\n",
                "\n",
                "The input tokens $t$ are integers. We get them from taking a sequence, and tokenizing it (like we saw in the previous section).\n",
                "\n",
                "The token embedding is a lookup table mapping tokens to vectors, which is implemented as a matrix $W_E$. The matrix consists of a stack of token embedding vectors (one for each token).\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Residual stream\n",
                "\n",
                "The residual stream is the sum of all previous outputs of layers of the model, is the input to each new layer. It has shape `[batch, seq_len, d_model]` (where `d_model` is the length of a single embedding vector).\n",
                "\n",
                "The initial value of the residual stream is denoted $x_0$ in the diagram, and $x_i$ are later values of the residual stream (after more attention and MLP layers have been applied to the residual stream).\n",
                "\n",
                "The residual stream is *really* fundamental. It's the central object of the transformer. It's how model remembers things, moves information between layers for composition, and it's the medium used to store the information that attention moves between positions.\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Transformer blocks\n",
                "\n",
                "Then we have a series of `n_layers` **transformer blocks** (also sometimes called **residual blocks**).\n",
                "\n",
                "Note - a block contains an attention layer *and* an MLP layer, but we say a transformer has $k$ layers if it has $k$ blocks (i.e. $2k$ total layers).\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### Attention\n",
                "\n",
                "First we have attention. This moves information from prior positions in the sequence to the current token.\n",
                "\n",
                "We do this for *every* token in parallel using the same parameters. The only difference is that we look backwards only (to avoid \"cheating\"). This means later tokens have more of the sequence that they can look at.\n",
                "\n",
                "Attention layers are the only bit of a transformer that moves information between positions (i.e. between vectors at different sequence positions in the residual stream).\n",
                "\n",
                "Attention layers are made up of `n_heads` heads - each with their own parameters, own attention pattern, and own information how to copy things from source to destination. The heads act independently and additively, we just add their outputs together, and back to the stream.\n",
                "\n",
                "Each head does the following:\n",
                "* Produces an **attention pattern** for each destination token, a probability distribution of prior source tokens (including the current one) weighting how much information to copy.\n",
                "* Moves information (via a linear map) in the same way from each source token to each destination token.\n",
                "\n",
                "A few key points:\n",
                "\n",
                "* What information we copy depends on the source token's *residual stream*, but this doesn't mean it only depends on the value of that token, because the residual stream can store more information than just the token identity (the purpose of the attention heads is to move information between tokens).\n",
                "* We can think of each attention head as consisting of two different **circuits**:\n",
                "    * One circuit determines **where to move information to and from** (this is a function of the residual stream for the source/key and destination/query tokens)\n",
                "    * The other circuit determines **what information to move** (this is a function of only the source token's residual stream)\n",
                "    * For reasons which will become clear later, we refer to the first circuit as the **QK circuit**, and the second circuit as the **OV circuit**\n",
                "\n",
                "\n",
                "Below is a schematic diagram of the attention layers. Don't worry if you don't follow this right now, we'll go into more detail during implementation.\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<img src=\"https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/transformer-attn-new.png\" width=\"1100\">\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### MLP\n",
                "\n",
                "The MLP layers are just a standard neural network, with a singular hidden layer and a nonlinear activation function. The exact activation isn't conceptually important ([GELU](https://paperswithcode.com/method/gelu) seems to perform best).\n",
                "\n",
                "Our hidden dimension is normally `d_mlp = 4 * d_model`. Exactly why the ratios are what they are isn't super important (people basically cargo-cult what GPT did back in the day!).\n",
                "\n",
                "Importantly, **the MLP operates on positions in the residual stream independently, and in exactly the same way**. It doesn't move information between positions.\n",
                "\n",
                "Intuition - once attention has moved relevant information to a single position in the residual stream, MLPs can actually do computation, reasoning, lookup information, etc. *What the hell is going on inside MLPs* is a pretty big open problem in transformer mechanistic interpretability - see the [Toy Model of Superposition Paper](https://transformer-circuits.pub/2022/toy_model/index.html) for more on why this is hard.\n",
                "\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<img src=\"https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/transformer-mlp-new-2.png\" width=\"650\">\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Unembedding\n",
                "\n",
                "Finally, we unembed!\n",
                "\n",
                "This just consists of applying a linear map $W_U$, going from final residual stream to a vector of logits - this is the output.\n",
                "\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<details>\n",
                "<summary>Bonus things - less conceptually important but key technical details</summary>\n",
                "\n",
                "#### LayerNorm\n",
                "\n",
                "* Simple normalization function applied at the start of each layer (i.e. before each MLP, attention layer, and before the unembedding)\n",
                "* Converts each input vector (independently in parallel for each batch x position residual stream vector) to have mean zero and variance 1.\n",
                "* Then applies an elementwise scaling and translation\n",
                "* Cool maths tangent: The scale & translate is just a linear map. LayerNorm is only applied immediately before another linear map. Linear compose linear = linear, so we can just fold this into a single effective linear layer and ignore it.\n",
                "    * `fold_ln=True` flag in `from_pretrained` does this for you.\n",
                "* LayerNorm is annoying for interpertability - the scale part is not linear, so you can't think about different bits of the input independently. But it's *almost* linear - if you're changing a small part of the input it's linear, but if you're changing enough to alter the norm substantially it's not linear.\n",
                "\n",
                "\n",
                "\n",
                "#### Positional embeddings\n",
                "\n",
                "* **Problem:** Attention operates over all pairs of positions. This means it's symmetric with regards to position - the attention calculation from token 5 to token 1 and token 5 to token 2 are the same by default\n",
                "    * This is dumb because nearby tokens are more relevant.\n",
                "* There's a lot of dumb hacks for this.\n",
                "* We'll focus on **learned, absolute positional embeddings**. This means we learn a lookup table mapping the index of the position of each token to a residual stream vector, and add this to the embed.\n",
                "    * Note that we *add* rather than concatenate. This is because the residual stream is shared memory, and likely under significant superposition (the model compresses more features in there than the model has dimensions)\n",
                "    * We basically never concatenate inside a transformer, unless doing weird shit like generating text efficiently.\n",
                "* This connects to **attention as generalized convolution**\n",
                "    * We argued that language does still have locality, and so it's helpful for transformers to have access to the positional information so they \"know\" two tokens are next to each other (and hence probably relevant to each other).\n",
                "</details>"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Actual Code!\n",
                "\n",
                "Key (for the results you get when running the code immediately below)\n",
                "\n",
                "```\n",
                "batch = 1\n",
                "position = 35\n",
                "d_model = 768\n",
                "n_heads = 12\n",
                "n_layers = 12\n",
                "d_mlp = 3072 (= 4 * d_model)\n",
                "d_head = 64 (= d_model / n_heads)\n",
                "```\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Parameters and Activations\n",
                "\n",
                "It's important to distinguish between parameters and activations in the model.\n",
                "\n",
                "* **Parameters** are the weights and biases that are learned during training.\n",
                "    * These don't change when the model input changes.\n",
                "* **Activations** are temporary numbers calculated during a forward pass, that are functions of the input.\n",
                "    * We can think of these values as only existing for the duration of a single forward pass, and disappearing afterwards.\n",
                "    * We can use hooks to access these values during a forward pass (more on hooks later), but it doesn't make sense to talk about a model's activations outside the context of some particular input.\n",
                "    * Attention scores and patterns are activations (this is slightly non-intuitve because they're used in a matrix multiplication with another activation).\n",
                "\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### Print All Parameters Shapes of Reference Model\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "embed.W_E          (50257, 768)\n",
                        "pos_embed.W_pos    (1024, 768)\n",
                        "blocks.0.ln1.w     (768,)\n",
                        "blocks.0.ln1.b     (768,)\n",
                        "blocks.0.ln2.w     (768,)\n",
                        "blocks.0.ln2.b     (768,)\n",
                        "blocks.0.attn.W_Q  (12, 768, 64)\n",
                        "blocks.0.attn.W_O  (12, 64, 768)\n",
                        "blocks.0.attn.b_Q  (12, 64)\n",
                        "blocks.0.attn.b_O  (768,)\n",
                        "blocks.0.attn.W_K  (12, 768, 64)\n",
                        "blocks.0.attn.W_V  (12, 768, 64)\n",
                        "blocks.0.attn.b_K  (12, 64)\n",
                        "blocks.0.attn.b_V  (12, 64)\n",
                        "blocks.0.mlp.W_in  (768, 3072)\n",
                        "blocks.0.mlp.b_in  (3072,)\n",
                        "blocks.0.mlp.W_out (3072, 768)\n",
                        "blocks.0.mlp.b_out (768,)\n",
                        "ln_final.w         (768,)\n",
                        "ln_final.b         (768,)\n",
                        "unembed.W_U        (768, 50257)\n",
                        "unembed.b_U        (50257,)\n"
                    ]
                }
            ],
            "source": [
                "for name, param in reference_gpt2.named_parameters():\n",
                "    # Only print for first layer\n",
                "    if \".0.\" in name or \"blocks\" not in name:\n",
                "        print(f\"{name:18} {tuple(param.shape)}\")"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Config\n",
                "\n",
                "The config object contains all the hyperparameters of the model. We can print the config of the reference model to see what it contains:\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "HookedTransformerConfig:\n",
                        "{'act_fn': 'gelu_new',\n",
                        " 'attention_dir': 'causal',\n",
                        " 'attn_only': False,\n",
                        " 'attn_scale': 8.0,\n",
                        " 'attn_scores_soft_cap': -1.0,\n",
                        " 'attn_types': None,\n",
                        " 'checkpoint_index': None,\n",
                        " 'checkpoint_label_type': None,\n",
                        " 'checkpoint_value': None,\n",
                        " 'd_head': 64,\n",
                        " 'd_mlp': 3072,\n",
                        " 'd_model': 768,\n",
                        " 'd_vocab': 50257,\n",
                        " 'd_vocab_out': 50257,\n",
                        " 'decoder_start_token_id': None,\n",
                        " 'default_prepend_bos': True,\n",
                        " 'device': 'cpu',\n",
                        " 'dtype': torch.float32,\n",
                        " 'eps': 1e-05,\n",
                        " 'experts_per_token': None,\n",
                        " 'final_rms': False,\n",
                        " 'from_checkpoint': False,\n",
                        " 'gated_mlp': False,\n",
                        " 'init_mode': 'gpt2',\n",
                        " 'init_weights': False,\n",
                        " 'initializer_range': 0.02886751345948129,\n",
                        " 'load_in_4bit': False,\n",
                        " 'model_name': 'gpt2',\n",
                        " 'n_ctx': 1024,\n",
                        " 'n_devices': 1,\n",
                        " 'n_heads': 12,\n",
                        " 'n_key_value_heads': None,\n",
                        " 'n_layers': 12,\n",
                        " 'n_params': 84934656,\n",
                        " 'normalization_type': 'LN',\n",
                        " 'num_experts': None,\n",
                        " 'original_architecture': 'GPT2LMHeadModel',\n",
                        " 'output_logits_soft_cap': -1.0,\n",
                        " 'parallel_attn_mlp': False,\n",
                        " 'positional_embedding_type': 'standard',\n",
                        " 'post_embedding_ln': False,\n",
                        " 'relative_attention_max_distance': None,\n",
                        " 'relative_attention_num_buckets': None,\n",
                        " 'rotary_adjacent_pairs': False,\n",
                        " 'rotary_base': 10000,\n",
                        " 'rotary_dim': None,\n",
                        " 'scale_attn_by_inverse_layer_idx': False,\n",
                        " 'seed': None,\n",
                        " 'tie_word_embeddings': False,\n",
                        " 'tokenizer_name': 'gpt2',\n",
                        " 'tokenizer_prepends_bos': False,\n",
                        " 'trust_remote_code': False,\n",
                        " 'use_attn_in': False,\n",
                        " 'use_attn_result': False,\n",
                        " 'use_attn_scale': True,\n",
                        " 'use_hook_mlp_in': False,\n",
                        " 'use_hook_tokens': False,\n",
                        " 'use_local_attn': False,\n",
                        " 'use_normalization_before_and_after': False,\n",
                        " 'use_split_qkv_input': False,\n",
                        " 'window_size': None}\n"
                    ]
                }
            ],
            "source": [
                "# As a reference - note there's a lot of stuff we don't care about in here, to do with library internals or other architectures\n",
                "print(reference_gpt2.cfg)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We define a stripped down config for our model:\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Config(d_model=768, debug=True, d_vocab=50257, n_ctx=1024, d_head=64, d_mlp=3072, n_heads=12, n_layers=12, layer_norm_eps=1e-05, init_range=0.02)\n"
                    ]
                }
            ],
            "source": [
                "@dataclass\n",
                "class Config:\n",
                "    d_model: int = 768  # dimension of the residual_stream\n",
                "    debug: bool = True\n",
                "    d_vocab: int = 50257\n",
                "    n_ctx: int = 1024  # max nb of tokens that the model can handle\n",
                "    d_head: int = 64  # dimension of each key/query/value\n",
                "    d_mlp: int = 3072  # dimension of the hidden layer inside the MLPs\n",
                "    n_heads: int = 12  # Nb of heads\n",
                "    n_layers: int = 12  # Nb of (Attention+ MLP) in the GPT\n",
                "\n",
                "    layer_norm_eps: float = 1e-5  # (Bonus)\n",
                "    init_range: float = 0.02  # (bonus) standard deviation of 0.02 for weight initialization\n",
                "\n",
                "\n",
                "cfg = Config()\n",
                "print(cfg)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Tests\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Tests are great, write lightweight ones to use as you go!\n",
                "\n",
                "**Naive test:** Generate random inputs of the right shape, input to your model, check whether there's an error and print the correct output.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "metadata": {},
            "outputs": [],
            "source": [
                "def rand_float_test(cls, shape):\n",
                "    cfg = Config(debug=True)\n",
                "    layer = cls(cfg).to(device)\n",
                "    random_input = torch.randn(shape).to(device)\n",
                "    print(\"Input shape:\", random_input.shape)\n",
                "    output = layer(random_input)\n",
                "    if isinstance(output, tuple):\n",
                "        output = output[0]\n",
                "    print(\"Output shape:\", output.shape, \"\\n\")\n",
                "\n",
                "\n",
                "def rand_int_test(cls, shape):\n",
                "    cfg = Config(debug=True)\n",
                "    layer = cls(cfg).to(device)\n",
                "    random_input = torch.randint(100, 1000, shape).to(device)\n",
                "    print(\"Input shape:\", random_input.shape)\n",
                "    output = layer(random_input)\n",
                "    if isinstance(output, tuple):\n",
                "        output = output[0]\n",
                "    print(\"Output shape:\", output.shape, \"\\n\")\n",
                "\n",
                "\n",
                "def load_gpt2_test(cls, gpt2_layer, input):\n",
                "    cfg = Config(debug=True)\n",
                "    layer = cls(cfg).to(device)\n",
                "    layer.load_state_dict(gpt2_layer.state_dict(), strict=False)\n",
                "    print(\"Input shape:\", input.shape)\n",
                "    output = layer(input)\n",
                "    if isinstance(output, tuple):\n",
                "        output = output[0]\n",
                "    print(\"Output shape:\", output.shape)\n",
                "    try:\n",
                "        reference_output = gpt2_layer(input)\n",
                "    except:\n",
                "        reference_output = gpt2_layer(input, input, input)\n",
                "    print(\"Reference output shape:\", reference_output.shape, \"\\n\")\n",
                "    comparison = torch.isclose(output, reference_output, atol=1e-4, rtol=1e-3)\n",
                "    print(f\"{comparison.sum()/comparison.numel():.2%} of the values are correct\\n\")"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Embedding\n",
                "\n",
                "```c\n",
                "Difficulty: 🟠🟠⚪⚪⚪\n",
                "Importance: 🟠🟠🟠⚪⚪\n",
                "\n",
                "You should spend up to 5-10 minutes on this exercise.\n",
                "```\n",
                "\n",
                "This is basically a lookup table from tokens to residual stream vectors.\n",
                "\n",
                "(Hint - you can implement this in just one line, without any complicated functions.)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Input shape: torch.Size([2, 4])\n",
                        "Output shape: torch.Size([2, 4, 768]) \n",
                        "\n",
                        "Input shape: torch.Size([1, 45])\n",
                        "Output shape: torch.Size([1, 45, 768])\n",
                        "Reference output shape: torch.Size([1, 45, 768]) \n",
                        "\n",
                        "100.00% of the values are correct\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "class Embed(nn.Module):\n",
                "    def __init__(self, cfg: Config):\n",
                "        super().__init__()\n",
                "        self.cfg = cfg\n",
                "        self.W_E = nn.Parameter(torch.empty((cfg.d_vocab, cfg.d_model)))\n",
                "        nn.init.normal_(self.W_E, std=self.cfg.init_range)\n",
                "\n",
                "    def forward(\n",
                "        self, tokens: Int[Tensor, \"batch position\"]\n",
                "    ) -> Float[Tensor, \"batch position d_model\"]: # (batch, indexes)\n",
                "        return self.W_E[tokens]\n",
                "\n",
                "\n",
                "rand_int_test(Embed, [2, 4])\n",
                "load_gpt2_test(Embed, reference_gpt2.embed, tokens)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<details>\n",
                "<summary>Help - I keep getting <code>RuntimeError: CUDA error: device-side assert triggered</code>.</summary>\n",
                "\n",
                "This is a uniquely frustrating type of error message, because it (1) forces you to restart the kernel, and (2) often won't tell you where the error message actually originated from!\n",
                "\n",
                "You can fix the second problem by adding the line `os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"` to the very top of your file (after importing `os`). This won't fix your bug, but it makes sure the correct origin point is identified.\n",
                "\n",
                "As for actually fixing the bug, this error usually ends up being the result of bad indexing, e.g. you're trying to apply an embedding layer to tokens which are larger than your maximum embedding.\n",
                "</details>\n",
                "\n",
                "<details>\n",
                "<summary>Solution</summary>\n",
                "\n",
                "\n",
                "```python\n",
                "class Embed(nn.Module):\n",
                "    def __init__(self, cfg: Config):\n",
                "        super().__init__()\n",
                "        self.cfg = cfg\n",
                "        self.W_E = nn.Parameter(t.empty((cfg.d_vocab, cfg.d_model)))\n",
                "        nn.init.normal_(self.W_E, std=self.cfg.init_range)\n",
                "\n",
                "    def forward(self, tokens: Int[Tensor, \"batch position\"]) -> Float[Tensor, \"batch position d_model\"]:\n",
                "        # SOLUTION\n",
                "        return self.W_E[tokens]\n",
                "```\n",
                "</details>\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Positional Embedding\n",
                "\n",
                "```c\n",
                "Difficulty: 🟠🟠⚪⚪⚪\n",
                "Importance: 🟠🟠🟠⚪⚪\n",
                "\n",
                "You should spend up to 10-15 minutes on this exercise.\n",
                "```\n",
                "\n",
                "Positional embedding can also be thought of as a lookup table, but rather than the indices being our token IDs, the indices are just the numbers `0`, `1`, `2`, ..., `seq_len-1` (i.e. the position indices of the tokens in the sequence).\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Input shape: torch.Size([2, 4])\n",
                        "Output shape: torch.Size([2, 4, 768]) \n",
                        "\n",
                        "Input shape: torch.Size([1, 45])\n",
                        "Output shape: torch.Size([1, 45, 768])\n",
                        "Reference output shape: torch.Size([1, 45, 768]) \n",
                        "\n",
                        "100.00% of the values are correct\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "class PosEmbed(nn.Module): # For Attention ?\n",
                "    def __init__(self, cfg: Config):\n",
                "        super().__init__()\n",
                "        self.cfg = cfg\n",
                "        self.W_pos = nn.Parameter(torch.empty((cfg.n_ctx, cfg.d_model)))\n",
                "        nn.init.normal_(self.W_pos, std=self.cfg.init_range)\n",
                "\n",
                "    def forward(\n",
                "        self, tokens: Int[Tensor, \"batch position\"]\n",
                "    ) -> Float[Tensor, \"batch position d_model\"]:\n",
                "        # Hint: You should use the einops.repeat function to repeat batch-wise the positional embedding.\n",
                "        batch, seq_len = tokens.shape\n",
                "        return einops.repeat(self.W_pos[:seq_len], \"seq d_model -> batch seq d_model\", batch=batch)\n",
                "\n",
                "\n",
                "rand_int_test(PosEmbed, [2, 4])\n",
                "load_gpt2_test(PosEmbed, reference_gpt2.pos_embed, tokens)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<details>\n",
                "<summary>Solution</summary>\n",
                "\n",
                "\n",
                "```python\n",
                "class PosEmbed(nn.Module):\n",
                "    def __init__(self, cfg: Config):\n",
                "        super().__init__()\n",
                "        self.cfg = cfg\n",
                "        self.W_pos = nn.Parameter(t.empty((cfg.n_ctx, cfg.d_model)))\n",
                "        nn.init.normal_(self.W_pos, std=self.cfg.init_range)\n",
                "\n",
                "    def forward(self, tokens: Int[Tensor, \"batch position\"]) -> Float[Tensor, \"batch position d_model\"]:\n",
                "        # SOLUTION\n",
                "        batch, seq_len = tokens.shape\n",
                "        return einops.repeat(self.W_pos[:seq_len], \"seq d_model -> batch seq d_model\", batch=batch)\n",
                "```\n",
                "</details>\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Attention\n",
                "\n",
                "```c\n",
                "Difficulty: 🟠🟠🟠🟠⚪\n",
                "Importance: 🟠🟠🟠🟠🟠\n",
                "\n",
                "You should spend up to 30-45 minutes on this exercise.\n",
                "```\n",
                "\n",
                "* **Step 1:** Produce an attention pattern - for each destination token, probability distribution over previous tokens (including current token)\n",
                "    * Linear map from input -> query, key shape `[batch, seq_posn, head_index, d_head]`\n",
                "    * Dot product every *pair* of queries and keys to get attn_scores `[batch, head_index, query_pos, key_pos]` (query = dest, key = source)\n",
                "    * **Scale** and mask `attn_scores` to make it lower triangular, i.e. causal\n",
                "    * Softmax along the `key_pos` dimension, to get a probability distribution for each query (destination) token - this is our attention pattern!\n",
                "* **Step 2:** Move information from source tokens to destination token using attention pattern\n",
                "    * Linear map from input -> value `[batch, key_pos, head_index, d_head]`\n",
                "    * Mix along the `key_pos` with attn pattern to get `z`, which is a weighted average of the value vectors `[batch, query_pos, head_index, d_head]`\n",
                "    * Map to output, `[batch, position, d_model]` (position = query_pos, we've summed over all heads)\n",
                "\n",
                "Note - when we say **scale**, we mean dividing by `sqrt(d_head)`. The purpose of this is to avoid vanishing gradients (which is a big problem when we're dealing with a function like softmax - if one of the values is much larger than all the others, the probabilities will be close to 0 or 1, and the gradients will be close to 0).\n",
                "\n",
                "Below is a much larger, more detailed version of the attention head diagram from earlier. This should give you an idea of the actual tensor operations involved. A few clarifications on this diagram:\n",
                "\n",
                "* Whenever there is a third dimension shown in the pictures, this refers to the `head_index` dimension. We can see that all operations within the attention layer are done independently for each head.\n",
                "* The objects in the box are activations; they have a batch dimension (for simplicity, we assume the batch dimension is 1 in the diagram). The objects to the right of the box are our parameters (weights and biases); they have no batch dimension."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<img src=\"https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/transformer-attn-21.png\" width=\"1400\">\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<details>\n",
                "<summary><b>A few extra notes on attention (optional)</b></summary>\n",
                "\n",
                "Usually we have the relation `e = n * h` (i.e. `d_model = num_heads * d_head`). There are some computational justifications for this, but mostly this is just done out of convention (just like how we usually have `d_mlp = 4 * d_model`!).\n",
                "\n",
                "---\n",
                "\n",
                "The names **keys**, **queries** and **values** come from their analogy to retrieval systems. Broadly speaking:\n",
                "\n",
                "* The **queries** represent some information that a token is **\"looking for\"**\n",
                "* The **keys** represent the information that a token **\"contains\"**\n",
                "    * So the attention score being high basically means that the source (key) token contains the information which the destination (query) token **is looking for**\n",
                "* The **values** represent the information that is actually taken from the source token, to be moved to the destination token\n",
                "\n",
                "---\n",
                "\n",
                "This diagram can better help us understand the difference between the **QK** and **OV** circuit. We'll discuss this just briefly here, and will go into much more detail later on.\n",
                "\n",
                "The **QK** circuit consists of the operation of the $W_Q$ and $W_K$ matrices. In other words, it determines the attention pattern, i.e. where information is moved to and from in the residual stream. The functional form of the attention pattern $A$ is:\n",
                "\n",
                "$$\n",
                "A = \\text{softmax}\\left(\\frac{x^T W_Q W_K^T x}{\\sqrt{d_{head}}}\\right)\n",
                "$$\n",
                "\n",
                "where $x$ is the residual stream (shape `[seq_len, d_model]`), and $W_Q$, $W_K$ are the weight matrices for a single head (i.e. shape `[d_model, d_head]`).\n",
                "\n",
                "The **OV** circuit consists of the operation of the $W_V$ and $W_O$ matrices. Once attention patterns are fixed, these matrices operate on the residual stream at the source position, and their output is the thing which gets moved from source to destination position.\n",
                "\n",
                "The functional form of an entire attention head is:\n",
                "\n",
                "$$\n",
                "\\begin{aligned}\n",
                "\\text{output} &= \\text{softmax}\\left(\\frac{x W_Q W_K^T x^T}{\\sqrt{d_{head}}}\\right) (x W_V W_O) \\\\\n",
                "    &= Ax W_V W_O\n",
                "\\end{aligned}\n",
                "$$\n",
                "\n",
                "where $W_V$ has shape `[d_model, d_head]`, and $W_O$ has shape `[d_head, d_model]`.\n",
                "\n",
                "Here, we can clearly see that the **QK circuit** and **OV circuit** are doing conceptually different things, and should be thought of as two distinct parts of the attention head.\n",
                "\n",
                "Again, don't worry if you don't follow all of this right now - we'll go into **much** more detail on all of this in subsequent exercises. The purpose of the discussion here is just to give you a flavour of what's to come!\n",
                "\n",
                "</details>\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "First, it's useful to visualize and play around with attention patterns - what exactly are we looking at here? (Click on a head to lock onto just showing that head's pattern, it'll make it easier to interpret)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "metadata": {},
            "outputs": [],
            "source": [
                "# import circuitsvis as cv # Sado\n",
                "# from IPython.display import display\n",
                "# \n",
                "# html = cv.attention.attention_patterns(\n",
                "    # tokens=reference_gpt2.to_str_tokens(reference_text),\n",
                "    # attention=cache[\"pattern\", 0][0],\n",
                "# )\n",
                "# display(html)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "You can also use the `attention_heads` function, which has similar syntax but presents the information in a different (sometimes more helpful) way.\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<details>\n",
                "<summary>Help - my <code>attention_heads</code> plots are behaving weirdly.</summary>\n",
                "\n",
                "This seems to be a bug in `circuitsvis` - on VSCode, the attention head plots continually shrink in size.\n",
                "\n",
                "Until this is fixed, one way to get around it is to open the plots in your browser. You can do this inline with the `webbrowser` library:\n",
                "\n",
                "```python\n",
                "attn_heads = cv.attention.attention_heads(\n",
                "    tokens=reference_gpt2.to_str_tokens(reference_text),\n",
                "    attention=cache[\"pattern\", 0][0]\n",
                ")\n",
                "\n",
                "path = \"attn_heads.html\"\n",
                "\n",
                "with open(path, \"w\") as f:\n",
                "    f.write(str(attn_heads))\n",
                "\n",
                "webbrowser.open(path)\n",
                "```\n",
                "\n",
                "To check exactly where this is getting saved, you can print your current working directory with `os.getcwd()`.\n",
                "</details>\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "Note - don't worry if you don't get 100% accuracy here; the tests are pretty stringent. Even things like having your `einsum` input arguments in a different order might result in the output being very slightly different. You should be getting at least 99% accuracy though, so if the value is lower then this it probably means you've made a mistake somewhere.\n",
                "\n",
                "Also, this implementation will probably be the most challenging exercise on this page, so don't worry if it takes you some time! You should look at parts of the solution if you're stuck.\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Note: The `\"IGNORE\"` buffer is a very large negative number. This is the value you should mask your attention scores with (i.e. set them to this number wherever you want the probabilities to be zero).\n",
                "\n",
                "<details>\n",
                "<summary>Question - why do you think we mask the attention scores by setting them to a large negative number, rather than the attention probabilities by setting them to zero?</summary>\n",
                "\n",
                "If we masked the attention probabilities, then the probabilities would no longer sum to 1.\n",
                "\n",
                "We want to mask the scores and *then* take softmax, so that the probabilities are still valid probabilities (i.e. they sum to 1), and the values in the masked positions have no influence on the model's output.\n",
                "</details>\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 20,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Input shape: torch.Size([2, 4, 768])\n",
                        "Output shape: torch.Size([2, 4, 768]) \n",
                        "\n",
                        "Input shape: torch.Size([1, 35, 768])\n",
                        "Output shape: torch.Size([1, 35, 768])\n",
                        "Reference output shape: torch.Size([1, 35, 768]) \n",
                        "\n",
                        "100.00% of the values are correct\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "class Attention(nn.Module):\n",
                "    IGNORE: Float[Tensor, \"\"]\n",
                "\n",
                "    def __init__(self, cfg: Config):\n",
                "        super().__init__()\n",
                "        self.cfg = cfg\n",
                "        self.W_Q = nn.Parameter(torch.empty((cfg.n_heads, cfg.d_model, cfg.d_head)))\n",
                "        self.W_K = nn.Parameter(torch.empty((cfg.n_heads, cfg.d_model, cfg.d_head)))\n",
                "        self.W_V = nn.Parameter(torch.empty((cfg.n_heads, cfg.d_model, cfg.d_head)))\n",
                "        self.W_O = nn.Parameter(torch.empty((cfg.n_heads, cfg.d_head, cfg.d_model)))\n",
                "        self.b_Q = nn.Parameter(torch.zeros((cfg.n_heads, cfg.d_head)))\n",
                "        self.b_K = nn.Parameter(torch.zeros((cfg.n_heads, cfg.d_head)))\n",
                "        self.b_V = nn.Parameter(torch.zeros((cfg.n_heads, cfg.d_head)))\n",
                "        self.b_O = nn.Parameter(torch.zeros((cfg.d_model)))\n",
                "        nn.init.normal_(self.W_Q, std=self.cfg.init_range)\n",
                "        nn.init.normal_(self.W_K, std=self.cfg.init_range)\n",
                "        nn.init.normal_(self.W_V, std=self.cfg.init_range)\n",
                "        nn.init.normal_(self.W_O, std=self.cfg.init_range)\n",
                "        self.register_buffer(\"IGNORE\", torch.tensor(-1e5, dtype=torch.float32, device=device))\n",
                "\n",
                "    def forward(\n",
                "        self,\n",
                "        x: Float[Tensor, \"batch posn d_model\"] # normalized_resid_pre\n",
                "    ) -> Float[Tensor, \"batch posn d_model\"]:\n",
                "        # Use einops!\n",
                "        # And to help us understand your code quickly, try to use only the following names with einops:\n",
                "        # batch nheads posn_Q posn_K d_model d_head\n",
                "\n",
                "        # If you're stuck, check the hints below\n",
                "        keys = einops.einsum(\n",
                "            self.W_K, x,\n",
                "            \"nheads d_model d_head, batch posn_K d_model -> batch posn_K nheads d_head\"\n",
                "        ) + self.b_K\n",
                "\n",
                "        queries = einops.einsum(\n",
                "            self.W_Q, x,\n",
                "            \"nheads d_model d_head, batch posn_Q d_model -> batch posn_Q nheads d_head\"\n",
                "        ) + self.b_Q\n",
                "\n",
                "        values = einops.einsum(\n",
                "            self.W_V, x,\n",
                "            \"nheads d_model d_head, batch posn_K d_model -> batch posn_K nheads d_head\"\n",
                "        ) + self.b_V\n",
                "\n",
                "        attn_scores = einops.einsum(\n",
                "            keys, queries,\n",
                "            \"batch posn_K nheads d_head, batch posn_Q nheads d_head -> batch nheads posn_Q posn_K\"\n",
                "        )\n",
                "\n",
                "        attn_probs = self.apply_causal_mask(\n",
                "            attn_scores / torch.sqrt(torch.tensor(self.cfg.d_head))\n",
                "        ).softmax(dim=-1)\n",
                "\n",
                "        z = einops.einsum(\n",
                "            attn_probs, values,\n",
                "            \"batch nheads posn_Q posn_K, batch posn_K nheads d_head -> batch posn_Q nheads d_head\"\n",
                "        )\n",
                "\n",
                "        attn_out = einops.einsum(\n",
                "            z, self.W_O,\n",
                "            \"batch posn_Q nheads d_head, nheads d_head d_model -> batch posn_Q d_model\"\n",
                "        ) + self.b_O\n",
                "\n",
                "        return attn_out\n",
                "\n",
                "    def apply_causal_mask(\n",
                "        self, attn_scores: Float[Tensor, \"batch n_heads query_pos key_pos\"]\n",
                "    ) -> Float[Tensor, \"batch n_heads query_pos key_pos\"]:\n",
                "        \"\"\"\n",
                "        Applies a causal mask to attention scores, and returns masked scores.\n",
                "        \"\"\"\n",
                "        # Define a mask that is True for all positions we want to set probabilities to zero for\n",
                "        all_ones = torch.ones(attn_scores.size(-2), attn_scores.size(-1), device=attn_scores.device)\n",
                "        mask = torch.triu(all_ones, diagonal=1).bool()\n",
                "        # Apply the mask to attention scores, then return the masked scores\n",
                "        attn_scores.masked_fill_(mask, self.IGNORE)\n",
                "        return attn_scores\n",
                "\n",
                "\n",
                "rand_float_test(Attention, [2, 4, 768])\n",
                "load_gpt2_test(Attention, reference_gpt2.blocks[0].attn, cache[\"normalized\", 0, \"ln1\"])"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<details>\n",
                "<summary>Hint: high level steps</summary>\n",
                "\n",
                "```python\n",
                "# Calculate query, key and value vectors\n",
                "# Calculate attention scores\n",
                "# Then scale and apply mask and apply softmax on the correct dimension to get probabilities\n",
                "# Take weighted sum of value vectors, according to attention probabilities\n",
                "# Calculate output (by applying matrix W_O and summing over heads, then adding bias b_O)\n",
                "```\n",
                "</details>\n",
                "\n",
                "<details>\n",
                "<summary>Hint: detailed steps with only a few blanks to fill.</summary>\n",
                "\n",
                "```python\n",
                "# Calculate query, key and value vectors\n",
                "q = (\n",
                "    einops.einsum(\n",
                "        normalized_resid_pre,\n",
                "        self.W_Q,\n",
                "        \"batch posn d_model, nheads d_model d_head -> ???\",\n",
                "    )\n",
                "    + self.b_Q\n",
                ")\n",
                "\n",
                "k = ...\n",
                "\n",
                "v = ...\n",
                "\n",
                "# Calculate attention scores\n",
                "attn_scores = einops.einsum(\n",
                "    q,\n",
                "    k,\n",
                "    \"???,??? -> batch nheads posn_Q posn_K\",\n",
                ")\n",
                "\n",
                "# then scale and apply mask and apply softmax on the correct dimension to get probabilities\n",
                "attn_scores_masked = self.apply_causal_mask(attn_scores / self.cfg.d_head**0.5)\n",
                "attn_pattern = attn_scores_masked.softmax(dim=...)\n",
                "\n",
                "# Take weighted sum of value vectors, according to attention probabilities\n",
                "z = einops.einsum(\n",
                "    v,\n",
                "    attn_pattern,\n",
                "    \"???,??? -> batch posn_Q nheads d_head\",\n",
                ")\n",
                "\n",
                "# Calculate output (by applying matrix W_O and summing over heads, then adding bias b_O)\n",
                "attn_out = (\n",
                "    einops.einsum(\n",
                "        z,\n",
                "        self.W_O,\n",
                "        \"batch posn_Q nheads d_head, nheads d_head d_model -> batch posn_Q d_model\",\n",
                "    )\n",
                "    + self.b_O\n",
                ")\n",
                "return attn_out\n",
                "```\n",
                "</details>\n",
                "\n",
                "\n",
                "\n",
                "<details>\n",
                "<summary>Solution</summary>\n",
                "\n",
                "\n",
                "```python\n",
                "class Attention(nn.Module):\n",
                "    IGNORE: Float[Tensor, \"\"]\n",
                "\n",
                "    def __init__(self, cfg: Config):\n",
                "        super().__init__()\n",
                "        self.cfg = cfg\n",
                "        self.W_Q = nn.Parameter(t.empty((cfg.n_heads, cfg.d_model, cfg.d_head)))\n",
                "        self.W_K = nn.Parameter(t.empty((cfg.n_heads, cfg.d_model, cfg.d_head)))\n",
                "        self.W_V = nn.Parameter(t.empty((cfg.n_heads, cfg.d_model, cfg.d_head)))\n",
                "        self.W_O = nn.Parameter(t.empty((cfg.n_heads, cfg.d_head, cfg.d_model)))\n",
                "        self.b_Q = nn.Parameter(t.zeros((cfg.n_heads, cfg.d_head)))\n",
                "        self.b_K = nn.Parameter(t.zeros((cfg.n_heads, cfg.d_head)))\n",
                "        self.b_V = nn.Parameter(t.zeros((cfg.n_heads, cfg.d_head)))\n",
                "        self.b_O = nn.Parameter(t.zeros((cfg.d_model)))\n",
                "        nn.init.normal_(self.W_Q, std=self.cfg.init_range)\n",
                "        nn.init.normal_(self.W_K, std=self.cfg.init_range)\n",
                "        nn.init.normal_(self.W_V, std=self.cfg.init_range)\n",
                "        nn.init.normal_(self.W_O, std=self.cfg.init_range)\n",
                "        self.register_buffer(\"IGNORE\", t.tensor(-1e5, dtype=t.float32, device=device))\n",
                "\n",
                "    def forward(\n",
                "        self, normalized_resid_pre: Float[Tensor, \"batch posn d_model\"]\n",
                "    ) -> Float[Tensor, \"batch posn d_model\"]:\n",
                "        # SOLUTION\n",
                "        # Calculate query, key and value vectors\n",
                "        q = einops.einsum(\n",
                "            normalized_resid_pre, self.W_Q,\n",
                "            \"batch posn d_model, nheads d_model d_head -> batch posn nheads d_head\",\n",
                "        ) + self.b_Q\n",
                "        k = einops.einsum(\n",
                "            normalized_resid_pre, self.W_K,\n",
                "            \"batch posn d_model, nheads d_model d_head -> batch posn nheads d_head\",\n",
                "        ) + self.b_K\n",
                "        v = einops.einsum(\n",
                "            normalized_resid_pre, self.W_V,\n",
                "            \"batch posn d_model, nheads d_model d_head -> batch posn nheads d_head\",\n",
                "        ) + self.b_V\n",
                "\n",
                "        # Calculate attention scores, then scale and mask, and apply softmax to get probabilities\n",
                "        attn_scores = einops.einsum(\n",
                "            q, k,\n",
                "            \"batch posn_Q nheads d_head, batch posn_K nheads d_head -> batch nheads posn_Q posn_K\",\n",
                "        )\n",
                "        attn_scores_masked = self.apply_causal_mask(attn_scores / self.cfg.d_head ** 0.5)\n",
                "        attn_pattern = attn_scores_masked.softmax(-1)\n",
                "\n",
                "        # Take weighted sum of value vectors, according to attention probabilities\n",
                "        z = einops.einsum(\n",
                "            v, attn_pattern,\n",
                "            \"batch posn_K nheads d_head, batch nheads posn_Q posn_K -> batch posn_Q nheads d_head\",\n",
                "        )\n",
                "\n",
                "        # Calculate output (by applying matrix W_O and summing over heads, then adding bias b_O)\n",
                "        attn_out = einops.einsum(\n",
                "            z, self.W_O,\n",
                "            \"batch posn_Q nheads d_head, nheads d_head d_model -> batch posn_Q d_model\",\n",
                "        ) + self.b_O\n",
                "\n",
                "        return attn_out\n",
                "\n",
                "    def apply_causal_mask(\n",
                "        self, attn_scores: Float[Tensor, \"batch n_heads query_pos key_pos\"]\n",
                "    ) -> Float[Tensor, \"batch n_heads query_pos key_pos\"]:\n",
                "        \"\"\"\n",
                "        Applies a causal mask to attention scores, and returns masked scores.\n",
                "        \"\"\"\n",
                "        # SOLUTION\n",
                "        # Define a mask that is True for all positions we want to set probabilities to zero for\n",
                "        all_ones = t.ones(attn_scores.size(-2), attn_scores.size(-1), device=attn_scores.device)\n",
                "        mask = t.triu(all_ones, diagonal=1).bool()\n",
                "        # Apply the mask to attention scores, then return the masked scores\n",
                "        attn_scores.masked_fill_(mask, self.IGNORE)\n",
                "        return attn_scores\n",
                "```\n",
                "</details>\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## MLP\n",
                "\n",
                "```c\n",
                "Difficulty: 🟠🟠⚪⚪⚪\n",
                "Importance: 🟠🟠🟠🟠⚪\n",
                "\n",
                "You should spend up to 10-15 minutes on this exercise.\n",
                "```\n",
                "\n",
                "Next, you should implement the MLP layer, which consists of:\n",
                "\n",
                "* A linear layer, with weight `W_in`, bias `b_in`\n",
                "* A nonlinear functino (we usually use GELU; the function `gelu_new` has been imported for this purpose)\n",
                "* A linear layer, with weight `W_out`, bias `b_out`\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 21,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Input shape: torch.Size([2, 4, 768])\n",
                        "Output shape: torch.Size([2, 4, 768]) \n",
                        "\n",
                        "Input shape: torch.Size([1, 35, 768])\n",
                        "Output shape: torch.Size([1, 35, 768])\n",
                        "Reference output shape: torch.Size([1, 35, 768]) \n",
                        "\n",
                        "100.00% of the values are correct\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "class MLP(nn.Module):\n",
                "    def __init__(self, cfg: Config):\n",
                "        super().__init__()\n",
                "        self.cfg = cfg\n",
                "        self.W_in = nn.Parameter(torch.empty((cfg.d_model, cfg.d_mlp)))\n",
                "        self.W_out = nn.Parameter(torch.empty((cfg.d_mlp, cfg.d_model)))\n",
                "        self.b_in = nn.Parameter(torch.zeros((cfg.d_mlp)))\n",
                "        self.b_out = nn.Parameter(torch.zeros((cfg.d_model)))\n",
                "        nn.init.normal_(self.W_in, std=self.cfg.init_range)\n",
                "        nn.init.normal_(self.W_out, std=self.cfg.init_range)\n",
                "\n",
                "    def forward(\n",
                "        self, x: Float[Tensor, \"batch posn d_model\"] # normalized_resid_mid\n",
                "    ) -> Float[Tensor, \"batch posn d_model\"]:\n",
                "        out = einops.einsum(\n",
                "            x, self.W_in,\n",
                "            \"b posn d_model, d_model d_mlp -> b posn d_mlp\"\n",
                "        ) + self.b_in\n",
                "        out = gelu_new(out)\n",
                "        mlp_out = einops.einsum(\n",
                "            out, self.W_out,\n",
                "            \"b posn d_mlp, d_mlp d_model -> b posn d_model\"\n",
                "        ) + self.b_out\n",
                "        return mlp_out\n",
                "\n",
                "\n",
                "rand_float_test(MLP, [2, 4, 768])\n",
                "load_gpt2_test(MLP, reference_gpt2.blocks[0].mlp, cache[\"normalized\", 0, \"ln2\"])"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<details>\n",
                "<summary>Solution</summary>\n",
                "\n",
                "\n",
                "```python\n",
                "class MLP(nn.Module):\n",
                "    def __init__(self, cfg: Config):\n",
                "        super().__init__()\n",
                "        self.cfg = cfg\n",
                "        self.W_in = nn.Parameter(t.empty((cfg.d_model, cfg.d_mlp)))\n",
                "        self.W_out = nn.Parameter(t.empty((cfg.d_mlp, cfg.d_model)))\n",
                "        self.b_in = nn.Parameter(t.zeros((cfg.d_mlp)))\n",
                "        self.b_out = nn.Parameter(t.zeros((cfg.d_model)))\n",
                "        nn.init.normal_(self.W_in, std=self.cfg.init_range)\n",
                "        nn.init.normal_(self.W_out, std=self.cfg.init_range)\n",
                "\n",
                "    def forward(\n",
                "        self, normalized_resid_mid: Float[Tensor, \"batch posn d_model\"]\n",
                "    ) -> Float[Tensor, \"batch posn d_model\"]:\n",
                "        # SOLUTION\n",
                "        pre = einops.einsum(\n",
                "            normalized_resid_mid, self.W_in,\n",
                "            \"batch position d_model, d_model d_mlp -> batch position d_mlp\",\n",
                "        ) + self.b_in\n",
                "        post = gelu_new(pre)\n",
                "        mlp_out = einops.einsum(\n",
                "            post, self.W_out,\n",
                "            \"batch position d_mlp, d_mlp d_model -> batch position d_model\",\n",
                "        ) + self.b_out\n",
                "        return mlp_out\n",
                "```\n",
                "</details>\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## (Bonus) LayerNorm\n",
                "\n",
                "```c\n",
                "Difficulty: 🟠🟠🟠⚪⚪\n",
                "Importance: 🟠🟠🟠⚪⚪\n",
                "\n",
                "You should spend up to 10-15 minutes on this exercise.\n",
                "```\n",
                "\n",
                "You should fill in the code below, and then run the tests to verify that your layer is working correctly.\n",
                "\n",
                "Your LayerNorm should do the following:\n",
                "\n",
                "* Make mean 0\n",
                "* Normalize to have variance 1\n",
                "* Scale with learned weights\n",
                "* Translate with learned bias\n",
                "\n",
                "You can use the PyTorch [LayerNorm documentation](https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html) as a reference. A few more notes:\n",
                "\n",
                "* Your layernorm implementation always has `affine=True`, i.e. you do learn parameters `w` and `b` (which are represented as $\\gamma$ and $\\beta$ respectively in the PyTorch documentation).\n",
                "* Remember that, after the centering and normalization, each vector of length `d_model` in your input should have mean 0 and variance 1.\n",
                "* As the PyTorch documentation page says, your variance should be computed using `unbiased=False`.\n",
                "* The `layer_norm_eps` argument in your config object corresponds to the $\\epsilon$ term in the PyTorch documentation (it is included to avoid division-by-zero errors).\n",
                "* We've given you a `debug` argument in your config. If `debug=True`, then you can print output like the shape of objects in your `forward` function to help you debug (this is a very useful trick to improve your coding speed).\n",
                "\n",
                "Fill in the function, where it says `pass` (this will be the basic pattern for most other exercises in this section).\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 22,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Input shape: torch.Size([2, 4, 768])\n",
                        "Output shape: torch.Size([2, 4, 768]) \n",
                        "\n",
                        "Input shape: torch.Size([1, 35, 768])\n",
                        "Output shape: torch.Size([1, 35, 768])\n",
                        "Reference output shape: torch.Size([1, 35, 768]) \n",
                        "\n",
                        "100.00% of the values are correct\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "class LayerNorm(nn.Module):\n",
                "    def __init__(self, cfg: Config):\n",
                "        super().__init__()\n",
                "        self.cfg = cfg\n",
                "        self.w = nn.Parameter(torch.ones(cfg.d_model))\n",
                "        self.b = nn.Parameter(torch.zeros(cfg.d_model))\n",
                "\n",
                "    def forward(\n",
                "        self, residual: Float[Tensor, \"batch posn d_model\"] # residual\n",
                "    ) -> Float[Tensor, \"batch posn d_model\"]:\n",
                "        residual_num = residual - residual.mean(dim=-1, keepdim=True)\n",
                "        residual_denum = torch.sqrt(residual.var(dim=-1, keepdim=True, unbiased=False) + self.cfg.layer_norm_eps)\n",
                "        return (residual_num / residual_denum) * self.w + self.b\n",
                "\n",
                "rand_float_test(LayerNorm, [2, 4, 768])\n",
                "load_gpt2_test(LayerNorm, reference_gpt2.ln_final, cache[\"resid_post\", 11])"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<details>\n",
                "<summary>Solution</summary>\n",
                "\n",
                "\n",
                "```python\n",
                "class LayerNorm(nn.Module):\n",
                "    def __init__(self, cfg: Config):\n",
                "        super().__init__()\n",
                "        self.cfg = cfg\n",
                "        self.w = nn.Parameter(t.ones(cfg.d_model))\n",
                "        self.b = nn.Parameter(t.zeros(cfg.d_model))\n",
                "\n",
                "    def forward(self, residual: Float[Tensor, \"batch posn d_model\"]) -> Float[Tensor, \"batch posn d_model\"]:\n",
                "        # SOLUTION\n",
                "        residual_mean = residual.mean(dim=-1, keepdim=True)\n",
                "        residual_std = (residual.var(dim=-1, keepdim=True, unbiased=False) + self.cfg.layer_norm_eps).sqrt()\n",
                "\n",
                "        residual = (residual - residual_mean) / residual_std\n",
                "        return residual * self.w + self.b\n",
                "```\n",
                "</details>\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Transformer Block\n",
                "\n",
                "```c\n",
                "Difficulty: 🟠🟠⚪⚪⚪\n",
                "Importance: 🟠🟠🟠⚪⚪\n",
                "\n",
                "You should spend up to ~10 minutes on this exercise.\n",
                "```\n",
                "\n",
                "Now, we can put together the attention, MLP and layernorms into a single transformer block. Remember to implement the residual connections correctly!\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 23,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Input shape: torch.Size([2, 4, 768])\n",
                        "Output shape: torch.Size([2, 4, 768]) \n",
                        "\n",
                        "Input shape: torch.Size([1, 35, 768])\n",
                        "Output shape: torch.Size([1, 35, 768])\n",
                        "Reference output shape: torch.Size([1, 35, 768]) \n",
                        "\n",
                        "100.00% of the values are correct\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "class TransformerBlock(nn.Module):\n",
                "    def __init__(self, cfg: Config):\n",
                "        super().__init__()\n",
                "        self.cfg = cfg\n",
                "        self.ln1 = LayerNorm(cfg)\n",
                "        self.attn = Attention(cfg)\n",
                "        self.ln2 = LayerNorm(cfg)\n",
                "        self.mlp = MLP(cfg)\n",
                "\n",
                "    def forward(\n",
                "        self, resid_pre: Float[Tensor, \"batch position d_model\"]\n",
                "    ) -> Float[Tensor, \"batch position d_model\"]:\n",
                "        # First, we compute the attention, but the residual stream needs to be normalized beforehand\n",
                "        resid_mid = resid_pre + self.attn(self.ln1(resid_pre))\n",
                "\n",
                "        # Then, we compute the MLP, again, the input of the MLP needs to be normalized beforehand\n",
                "        resid_post = resid_mid + self.mlp(self.ln2(resid_mid))\n",
                "\n",
                "        return resid_post\n",
                "\n",
                "\n",
                "rand_float_test(TransformerBlock, [2, 4, 768])\n",
                "load_gpt2_test(TransformerBlock, reference_gpt2.blocks[0], cache[\"resid_pre\", 0])"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<details>\n",
                "<summary>Solution</summary>\n",
                "\n",
                "\n",
                "```python\n",
                "class TransformerBlock(nn.Module):\n",
                "    def __init__(self, cfg: Config):\n",
                "        super().__init__()\n",
                "        self.cfg = cfg\n",
                "        self.ln1 = LayerNorm(cfg)\n",
                "        self.attn = Attention(cfg)\n",
                "        self.ln2 = LayerNorm(cfg)\n",
                "        self.mlp = MLP(cfg)\n",
                "\n",
                "    def forward(\n",
                "        self, resid_pre: Float[Tensor, \"batch position d_model\"]\n",
                "    ) -> Float[Tensor, \"batch position d_model\"]:\n",
                "        # SOLUTION\n",
                "        resid_mid = self.attn(self.ln1(resid_pre)) + resid_pre\n",
                "        resid_post = self.mlp(self.ln2(resid_mid)) + resid_mid\n",
                "        return resid_post\n",
                "```\n",
                "</details>\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Unembedding\n",
                "\n",
                "```c\n",
                "Difficulty: 🟠🟠⚪⚪⚪\n",
                "Importance: 🟠🟠🟠⚪⚪\n",
                "\n",
                "You should spend up to ~10 minutes on this exercise.\n",
                "```\n",
                "\n",
                "The unembedding is jus a linear layer (with weight `W_U` and bias `b_U`).\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 24,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Input shape: torch.Size([2, 4, 768])\n",
                        "Output shape: torch.Size([2, 4, 50257]) \n",
                        "\n",
                        "Input shape: torch.Size([1, 35, 768])\n",
                        "Output shape: torch.Size([1, 35, 50257])\n",
                        "Reference output shape: torch.Size([1, 35, 50257]) \n",
                        "\n",
                        "100.00% of the values are correct\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "class Unembed(nn.Module):\n",
                "    def __init__(self, cfg):\n",
                "        super().__init__()\n",
                "        self.cfg = cfg\n",
                "        self.W_U = nn.Parameter(torch.empty((cfg.d_model, cfg.d_vocab)))\n",
                "        nn.init.normal_(self.W_U, std=self.cfg.init_range)\n",
                "        self.b_U = nn.Parameter(torch.zeros((cfg.d_vocab), requires_grad=False))\n",
                "\n",
                "    def forward(\n",
                "        self, normalized_resid_final: Float[Tensor, \"batch position d_model\"]\n",
                "    ) -> Float[Tensor, \"batch position d_vocab\"]:\n",
                "        return einops.einsum(\n",
                "            normalized_resid_final, self.W_U,\n",
                "            \"batch position d_model, d_model d_vocab -> batch position d_vocab\"\n",
                "        ) + self.b_U\n",
                "\n",
                "\n",
                "rand_float_test(Unembed, [2, 4, 768])\n",
                "load_gpt2_test(Unembed, reference_gpt2.unembed, cache[\"ln_final.hook_normalized\"])"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<details>\n",
                "<summary>Solution</summary>\n",
                "\n",
                "\n",
                "```python\n",
                "class Unembed(nn.Module):\n",
                "    def __init__(self, cfg):\n",
                "        super().__init__()\n",
                "        self.cfg = cfg\n",
                "        self.W_U = nn.Parameter(t.empty((cfg.d_model, cfg.d_vocab)))\n",
                "        nn.init.normal_(self.W_U, std=self.cfg.init_range)\n",
                "        self.b_U = nn.Parameter(t.zeros((cfg.d_vocab), requires_grad=False))\n",
                "\n",
                "    def forward(\n",
                "        self, normalized_resid_final: Float[Tensor, \"batch position d_model\"]\n",
                "    ) -> Float[Tensor, \"batch position d_vocab\"]:\n",
                "        # SOLUTION\n",
                "        return einops.einsum(\n",
                "            normalized_resid_final, self.W_U,\n",
                "            \"batch posn d_model, d_model d_vocab -> batch posn d_vocab\",\n",
                "        ) + self.b_U\n",
                "        # Or, could just do `normalized_resid_final @ self.W_U + self.b_U`\n",
                "```\n",
                "</details>\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Full Transformer\n",
                "\n",
                "```c\n",
                "Difficulty: 🟠🟠⚪⚪⚪\n",
                "Importance: 🟠🟠🟠⚪⚪\n",
                "\n",
                "You should spend up to ~10 minutes on this exercise.\n",
                "```\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 25,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Input shape: torch.Size([2, 4])\n",
                        "Output shape: torch.Size([2, 4, 50257]) \n",
                        "\n",
                        "Input shape: torch.Size([1, 45])\n",
                        "Output shape: torch.Size([1, 45, 50257])\n",
                        "Reference output shape: torch.Size([1, 45, 50257]) \n",
                        "\n",
                        "100.00% of the values are correct\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "class DemoTransformer(nn.Module):\n",
                "    def __init__(self, cfg: Config):\n",
                "        super().__init__()\n",
                "        self.cfg = cfg\n",
                "        self.embed = Embed(cfg)\n",
                "        self.pos_embed = PosEmbed(cfg)\n",
                "        self.blocks = nn.ModuleList([TransformerBlock(cfg) for _ in range(cfg.n_layers)])\n",
                "        self.ln_final = LayerNorm(cfg)\n",
                "        self.unembed = Unembed(cfg)\n",
                "\n",
                "    def forward(\n",
                "        self, tokens: Int[Tensor, \"batch position\"]\n",
                "    ) -> Float[Tensor, \"batch position d_vocab\"]:\n",
                "\n",
                "        # embed => Ce que veut dire le token\n",
                "        # pos_embed => Son sens selon un contexte\n",
                "        residual = self.embed(tokens) + self.pos_embed(tokens)\n",
                "        for block in self.blocks:\n",
                "            residual = block(residual)\n",
                "        ln_residual = self.ln_final(residual)\n",
                "        logits = self.unembed(ln_residual)\n",
                "        return logits\n",
                "\n",
                "rand_int_test(DemoTransformer, [2, 4])\n",
                "load_gpt2_test(DemoTransformer, reference_gpt2, tokens)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<details>\n",
                "<summary>Solution</summary>\n",
                "\n",
                "\n",
                "```python\n",
                "class DemoTransformer(nn.Module):\n",
                "    def __init__(self, cfg: Config):\n",
                "        super().__init__()\n",
                "        self.cfg = cfg\n",
                "        self.embed = Embed(cfg)\n",
                "        self.pos_embed = PosEmbed(cfg)\n",
                "        self.blocks = nn.ModuleList([TransformerBlock(cfg) for _ in range(cfg.n_layers)])\n",
                "        self.ln_final = LayerNorm(cfg)\n",
                "        self.unembed = Unembed(cfg)\n",
                "\n",
                "    def forward(self, tokens: Int[Tensor, \"batch position\"]) -> Float[Tensor, \"batch position d_vocab\"]:\n",
                "        # SOLUTION\n",
                "        residual = self.embed(tokens) + self.pos_embed(tokens)\n",
                "        for block in self.blocks:\n",
                "            residual = block(residual)\n",
                "        logits = self.unembed(self.ln_final(residual))\n",
                "        return logits\n",
                "```\n",
                "</details>\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Try it out!**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 26,
            "metadata": {},
            "outputs": [],
            "source": [
                "demo_gpt2 = DemoTransformer(Config(debug=False)).to(device)\n",
                "demo_gpt2.load_state_dict(reference_gpt2.state_dict(), strict=False)\n",
                "\n",
                "demo_logits = demo_gpt2(tokens)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Let's take a test string, and calculate the loss!\n",
                "\n",
                "We're using the formula for **cross-entropy loss**. The cross entropy loss between a modelled distribution $Q$ and target distribution $P$ is:\n",
                "\n",
                "$$\n",
                "-\\sum_x P(x) \\log Q(x)\n",
                "$$\n",
                "\n",
                "In the case where $P$ is just the empirical distribution from target classes (i.e. $P(x^*) = 1$ for the correct class $x^*$) then this becomes:\n",
                "\n",
                "$$\n",
                "-\\log Q(x^*)\n",
                "$$\n",
                "\n",
                "in other words, the negative log prob of the true classification.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 27,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Avg cross entropy loss: 4.0441\n",
                        "Avg cross entropy loss for uniform distribution: 10.824905\n",
                        "Avg probability assigned to correct token: 0.098629\n"
                    ]
                }
            ],
            "source": [
                "def get_log_probs(\n",
                "    logits: Float[Tensor, \"batch posn d_vocab\"], tokens: Int[Tensor, \"batch posn\"]\n",
                ") -> Float[Tensor, \"batch posn-1\"]:\n",
                "    log_probs = logits.log_softmax(dim=-1)\n",
                "    # Get logprobs the first seq_len-1 predictions (so we can compare them with the actual next tokens)\n",
                "    log_probs_for_tokens = (\n",
                "        log_probs[:, :-1].gather(dim=-1, index=tokens[:, 1:].unsqueeze(-1)).squeeze(-1)\n",
                "    )\n",
                "\n",
                "    return log_probs_for_tokens\n",
                "\n",
                "\n",
                "pred_log_probs = get_log_probs(demo_logits, tokens)\n",
                "print(f\"Avg cross entropy loss: {-pred_log_probs.mean():.4f}\")\n",
                "print(f\"Avg cross entropy loss for uniform distribution: {math.log(demo_gpt2.cfg.d_vocab):4f}\")\n",
                "print(f\"Avg probability assigned to correct token: {pred_log_probs.exp().mean():4f}\")"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We can also greedily generate text, by taking the most likely next token and continually appending it to our prompt before feeding it back into the model:\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 28,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "d582cd99fca8411d80ed2da5b9762197",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "  0%|          | 0/100 [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "The Total Perspective Vortex derives its picture of the whole Universe on the principle of the total perspective. The total perspective is the view of the whole Universe from the point of view of the observer. The total perspective is the view of the whole Universe from the point of view of the observer. The total perspective is the view of the whole Universe from the point of view of the observer. The total perspective is the view of the whole Universe from the point of view of the observer. The total perspective is the view of the whole Universe from the point of view of the observer. The\n"
                    ]
                }
            ],
            "source": [
                "test_string = (\n",
                "    \"\"\"The Total Perspective Vortex derives its picture of the whole Universe on the principle of\"\"\"\n",
                ")\n",
                "for i in tqdm(range(100)):\n",
                "    test_tokens = reference_gpt2.to_tokens(test_string).to(device)\n",
                "    demo_logits = demo_gpt2(test_tokens)\n",
                "    test_string += reference_gpt2.tokenizer.decode(demo_logits[-1, -1].argmax())\n",
                "\n",
                "print(test_string)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "In later sections, we'll learn to generate text in slightly more interesting ways than just argmaxing the output.\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 3️⃣ (Bonus) Training a Transformer\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "> ##### Learning objectives\n",
                ">\n",
                "> * Understand how to train a transformer from scratch\n",
                "> * Write a basic transformer training loop\n",
                "> * Interpret the transformer's falling cross entropy loss with reference to features of the training data (e.g. bigram frequencies)\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Now that we've built our transformer, and verified that it performs as expected when we load in weights, let's try training it from scratch!\n",
                "\n",
                "This is a lightweight demonstration of how you can actually train your own GPT-2 with this code! Here we train a tiny model on a tiny dataset, but it's fundamentally the same code for training a larger/more real model (though you'll need beefier GPUs and data parallelism to do it remotely efficiently, and fancier parallelism for much bigger ones).\n",
                "\n",
                "For our purposes, we'll train 2L 4 heads per layer model, with context length 256, for 10*200 steps of batch size 16, just to show what it looks like (and so the notebook doesn't melt your colab / machine!)."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Create Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 29,
            "metadata": {},
            "outputs": [],
            "source": [
                "model_cfg = Config(\n",
                "    debug=False,\n",
                "    d_model=256,\n",
                "    n_heads=4,\n",
                "    d_head=64,\n",
                "    d_mlp=1024,\n",
                "    n_layers=2,\n",
                "    n_ctx=256,\n",
                "    d_vocab=reference_gpt2.cfg.d_vocab,\n",
                ")\n",
                "model = DemoTransformer(model_cfg)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Training Args\n",
                "\n",
                "\n",
                "Note, for this optimization we'll be using **weight decay**."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 30,
            "metadata": {},
            "outputs": [],
            "source": [
                "@dataclass\n",
                "class TransformerTrainingArgs:\n",
                "    batch_size = 16\n",
                "    epochs = 10\n",
                "    max_steps_per_epoch = 200\n",
                "    lr = 1e-3\n",
                "    weight_decay = 1e-2\n",
                "    wandb_project: Optional[str] = \"day1-demotransformer\"\n",
                "    wandb_name: Optional[str] = None\n",
                "\n",
                "\n",
                "args = TransformerTrainingArgs()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Create Data\n",
                "\n",
                "We load in a tiny dataset made by Neel Nanda, with the first 10K entries in the Pile (inspired by Stas' version for OpenWebText!)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 31,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Dataset({\n",
                        "    features: ['text'],\n",
                        "    num_rows: 10000\n",
                        "})\n",
                        "It is done, and submitted. You can play “Survival of the Tastiest” on Android, and on the web. Playing on the web works, but you have to simulate multi-touch for table moving and that can be a bit confusing.\n",
                        "\n",
                        "There’s a lot I’d like to talk about. I’ll go through every topic, insted of making the typical what went right/wrong list.\n",
                        "\n",
                        "Concept\n",
                        "\n",
                        "Working over the theme was probably one of the hardest tasks I had to face.\n",
                        "\n",
                        "Originally, I had an idea of what kind of game I wanted to develop, gameplay wise – something with lots of enemies/actors, simple graphics, maybe set in space, controlled from a top-down view. I was confident I could fit any theme around it.\n",
                        "\n",
                        "In the end, the problem with a theme like “Evolution” in a game is that evolution is unassisted. It happens through several seemingly random mutations over time, with the most apt permutation surviving. This genetic car simulator is, in my opinion, a great example of actual evolution of a species facing a challenge. But is it a game?\n",
                        "\n",
                        "In a game, you need to control something to reach an objective. That control goes against what evolution is supposed to be like. If you allow the user to pick how to evolve something, it’s not evolution anymore – it’s the equivalent of intelligent design, the fable invented by creationists to combat the very idea of evolution. Being agnostic and a Pastafarian, that’s not something that rubbed me the right way.\n",
                        "\n",
                        "Hence, my biggest dillema when deciding what to create was not with what I wanted to create, but with what I did not. I didn’t want to create an “intelligent design” simulator and wrongly call it evolution.\n",
                        "\n",
                        "This is a problem, of course, every other contestant also had to face. And judging by the entries submitted, not many managed to work around it. I’d say the only real solution was through the use of artificial selection, somehow. So far, I haven’t seen any entry using this at its core gameplay.\n",
                        "\n",
                        "Alas, this is just a fun competition and after a while I decided not to be as strict with the game idea, and allowed myself to pick whatever I thought would work out.\n",
                        "\n",
                        "My initial idea was to create something where humanity tried to evolve to a next level but had some kind of foe trying to stop them from doing so. I kind of had this image of human souls flying in space towards a monolith or a space baby (all based in 2001: A Space Odyssey of course) but I couldn’t think of compelling (read: serious) mechanics for that.\n",
                        "\n",
                        "Borgs were my next inspiration, as their whole hypothesis fit pretty well into the evolution theme. But how to make it work? Are you the borg, or fighting the Borg?\n",
                        "\n",
                        "The third and final idea came to me through my girlfriend, who somehow gave me the idea of making something about the evolution of Pasta. The more I thought about it the more it sounded like it would work, so I decided to go with it.\n",
                        "\n",
                        "Conversations with my inspiring co-worker Roushey (who also created the “Mechanical Underdogs” signature logo for my intros) further matured the concept, as it involved into the idea of having individual pieces of pasta flying around and trying to evolve until they became all-powerful. A secondary idea here was that the game would work to explain how the Flying Spaghetti Monster came to exist – by evolving from a normal dinner table.\n",
                        "\n",
                        "So the idea evolved more or less into this: you are sitting a table. You have your own plate, with is your “base”. There are 5 other guests at the table, each with their own plate.\n",
                        "\n",
                        "Your plate can spawn little pieces of pasta. You do so by “ordering” them through a menu. Some pastas are better than others; some are faster, some are stronger. They have varying costs, which are debited from your credits (you start with a number of credits).\n",
                        "\n",
                        "Once spawned, your pastas start flying around. Their instinct is to fly to other plates, in order to conquer them (the objective of the game is having your pasta conquer all the plates on the table). But they are really autonomous, so after being spawned, you have no control over your pasta (think DotA or LoL creeps).\n",
                        "\n",
                        "Your pasta doesn’t like other people’s pasta, so if they meet, they shoot sauce at each other until one dies. You get credits for other pastas your own pasta kill.\n",
                        "\n",
                        "Once a pasta is in the vicinity of a plate, it starts conquering it for its team. It takes around 10 seconds for a plate to be conquered; less if more pasta from the same team are around. If pasta from other team are around, though, they get locked down in their attempt, unable to conquer the plate, until one of them die (think Battlefield’s standard “Conquest” mode).\n",
                        "\n",
                        "You get points every second for every plate you own.\n",
                        "\n",
                        "Over time, the concept also evolved to use an Italian bistro as its main scenario.\n",
                        "\n",
                        "Carlos, Carlos’ Bistro’s founder and owner\n",
                        "\n",
                        "Setup\n",
                        "\n",
                        "No major changes were made from my work setup. I used FDT and Starling creating an Adobe AIR (ActionScript) project, all tools or frameworks I already had some knowledge with.\n",
                        "\n",
                        "One big change for me was that I livestreamed my work through a twitch.tv account. This was a new thing for me. As recommended by Roushey, I used a program called XSplit and I got to say, it is pretty amazing. It made the livestream pretty effortless and the features are awesome, even for the free version. It was great to have some of my friends watch me, and then interact with them and random people through chat. It was also good knowing that I was also recording a local version of the files, so I could make a timelapse video later.\n",
                        "\n",
                        "Knowing the video was being recorded also made me a lot more self-conscious about my computer use, as if someone was watching over my shoulder. It made me realize that sometimes I spend too much time in seemingly inane tasks (I ended up wasting the longest time just to get some text alignment the way I wanted – it’ll probably drive someone crazy if they watch it) and that I do way too many typos where writing code. I pretty much spend half of the time writing a line and the other half fixing the crazy characters in it.\n",
                        "\n",
                        "My own stream was probably boring to watch since I was coding for the most time. But livestreaming is one of the cool things to do as a spectator too. It was great seeing other people working – I had a few tabs opened on my second monitor all the time. It’s actually a bit sad, because if I could, I could have spent the whole weekend just watching other people working! But I had to do my own work, so I’d only do it once in a while, when resting for a bit.\n",
                        "\n",
                        "Design\n",
                        "\n",
                        "Although I wanted some simple, low-fi, high-contrast kind of design, I ended up going with somewhat realistic (vector) art. I think it worked very well, fitting the mood of the game, but I also went overboard.\n",
                        "\n",
                        "For example: to know the state of a plate (who owns it, who’s conquering it and how much time they have left before conquering it, which pasta units are in the queue, etc), you have to look at the plate’s bill.\n",
                        "\n",
                        "The problem I realized when doing some tests is that people never look at the bill! They think it’s some kind of prop, so they never actually read its details.\n",
                        "\n",
                        "Plus, if you’re zoomed out too much, you can’t actually read it, so it’s hard to know what’s going on with the game until you zoom in to the area of a specific plate.\n",
                        "\n",
                        "One other solution that didn’t turn out to be as perfect as I thought was how to indicate who a plate base belongs to. In the game, that’s indicated by the plate’s decoration – its color denotes the team owner. But it’s something that fits so well into the design that people never realized it, until they were told about it.\n",
                        "\n",
                        "In the end, the idea of going with a full physical metaphor is one that should be done with care. Things that are very important risk becoming background noise, unless the player knows its importance.\n",
                        "\n",
                        "Originally, I wanted to avoid any kind of heads-up display in my game. In the end, I ended up adding it at the bottom to indicate your credits and bases owned, as well as the hideous out-of-place-and-still-not-obvious “Call Waiter” button. But in hindsight, I should have gone with a simple HUD from the start, especially one that indicated each team’s colors and general state of the game without the need for zooming in and out.\n",
                        "\n",
                        "Development\n",
                        "\n",
                        "Development went fast. But not fast enough.\n",
                        "\n",
                        "Even though I worked around 32+ hours for this Ludum Dare, the biggest problem I had to face in the end was overscoping. I had too much planned, and couldn’t get it all done.\n",
                        "\n",
                        "Content-wise, I had several kinds of pasta planned (Wikipedia is just amazing in that regard), split into several different groups, from small Pastina to huge Pasta al forno. But because of time constraints, I ended up scratching most of them, and ended up with 5 different types of very small pasta – barely something to start when talking about the evolution of Pasta.\n",
                        "\n",
                        "Pastas used in the game. Unfortunately, the macs where never used\n",
                        "\n",
                        "Which is one of the saddest things about the project, really. It had the framework and the features to allow an endless number of elements in there, but I just didn’t have time to draw the rest of the assets needed (something I loved to do, by the way).\n",
                        "\n",
                        "Other non-obvious features had to be dropped, too. For example, when ordering some pasta, you were supposed to select what kind of sauce you’d like with your pasta, each with different attributes. Bolognese, for example, is very strong, but inaccurate; Pesto is very accurate and has great range, but it’s weaker; and my favorite, Vodka, would triggers 10% loss of speed on the pasta hit by it.\n",
                        "\n",
                        "The code for that is mostly in there. But in the end, I didn’t have time to implement the sauce selection interface; all pasta ended up using bolognese sauce.\n",
                        "\n",
                        "To-do list: lots of things were not done\n",
                        "\n",
                        "Actual programming also took a toll in the development time. Having been programming for a while, I like to believe I got to a point where I know how to make things right, but at the expense of forgetting how to do things wrong in a seemingly good way. What I mean is that I had to take a lot of shortcuts in my code to save time (e.g. a lot of singletons references for cross-communication rather than events or observers, all-encompassing check loops, not fast enough) that left a very sour taste in my mouth. While I know I used to do those a few years ago and survive, I almost cannot accept the state my code is in right now.\n",
                        "\n",
                        "At the same time, I do know it was the right thing to do given the timeframe.\n",
                        "\n",
                        "One small thing that had some impact was using a somewhat new platform for me. That’s Starling, the accelerated graphics framework I used in Flash. I had tested it before and I knew how to use it well – the API is very similar to Flash itself. However, there were some small details that had some impact during development, making me feel somewhat uneasy the whole time I was writing the game. It was, again, the right thing to do, but I should have used Starling more deeply before (which is the conundrum: I used it for Ludum Dare just so I could learn more about it).\n",
                        "\n",
                        "Argument and user experience\n",
                        "\n",
                        "One final aspect of the game that I learned is that making the game obvious for your players goes a long way into making it fun. If you have to spend the longest time explaining things, your game is doing something wrong.\n",
                        "\n",
                        "And that’s exactly the problem Survival of the Tastiest ultimately faced. It’s very hard for people to understand what’s going on with the game, why, and how. I did have some introductory text at the beginning, but that was a last-minute thing. More importantly, I should have had a better interface or simplified the whole concept so it would be easier for people to understand.\n",
                        "\n",
                        "That doesn’t mean the game itself should be simple. It just means that the experience and interface should be approachable and understandable.\n",
                        "\n",
                        "Conclusion\n",
                        "\n",
                        "I’m extremely happy with what I’ve done and, especially given that this was my first Ludum Dare. However, I feel like I’ve learned a lot of what not to do.\n",
                        "\n",
                        "The biggest problem is overscoping. Like Eric Decker said, the biggest lesson we can learn with this is probably with scoping – deciding what to do beforehand in a way you can complete it without having to rush and do something half-assed.\n",
                        "\n",
                        "I’m sure I will do more Ludum Dares in the future. But if there are any lessons I can take of it, they are to make it simple, to use frameworks and platforms you already have some absolute experience with (otherwise you’ll spend too much time trying to solve easy questions), and to scope for a game that you can complete in one day only (that way, you can actually take two days and make it cool).\n",
                        "\n",
                        "This entry was posted\n",
                        "on Monday, August 27th, 2012 at 10:54 am and is filed under LD #24.\n",
                        "You can follow any responses to this entry through the RSS 2.0 feed.\n",
                        "You can skip to the end and leave a response. Pinging is currently not allowed.\n",
                        "\n",
                        "3 Responses to ““Survival of the Tastiest” Post-mortem”\n",
                        "\n",
                        "darn it , knowing that I missed your livestream makes me a sad panda ;( but more to the point, the game is … well for a startup its original to say the least ;D it has some really neat ideas and more importantly its designed arround touch screens whitch by the looks of the submission is something rare ;o or that could be just me and my short memory -_-! awesum game, love et <3\n"
                    ]
                }
            ],
            "source": [
                "dataset = datasets.load_dataset(\"NeelNanda/pile-10k\", split=\"train\").remove_columns(\"meta\")\n",
                "print(dataset)\n",
                "print(dataset[0][\"text\"])"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "`tokenize_and_concatenate` is a useful function which takes our dataset of strings, and returns a dataset of token IDs ready to feed into the model. We then create a dataloader from this tokenized dataset. The useful method `train_test_split` can give us a training and testing set.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 32,
            "metadata": {},
            "outputs": [],
            "source": [
                "tokenized_dataset = tokenize_and_concatenate(\n",
                "    dataset,\n",
                "    reference_gpt2.tokenizer,\n",
                "    streaming=False,\n",
                "    max_length=model.cfg.n_ctx,\n",
                "    column_name=\"text\",\n",
                "    add_bos_token=True,\n",
                "    num_proc=4,\n",
                ")\n",
                "\n",
                "dataset_dict = tokenized_dataset.train_test_split(test_size=1000)\n",
                "train_loader = DataLoader(\n",
                "    dataset_dict[\"train\"],\n",
                "    batch_size=args.batch_size,\n",
                "    shuffle=True,\n",
                "    num_workers=4,\n",
                "    pin_memory=True,\n",
                ")\n",
                "test_loader = DataLoader(\n",
                "    dataset_dict[\"test\"],\n",
                "    batch_size=args.batch_size,\n",
                "    shuffle=False,\n",
                "    num_workers=4,\n",
                "    pin_memory=True,\n",
                ")"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "When we iterate through these dataloaders, we will find dictionaries with the single key `'tokens'`, which maps to a tensor of token IDs with shape `(batch, seq_len)`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 33,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "dict_keys(['tokens'])\n",
                        "torch.Size([16, 256])\n"
                    ]
                }
            ],
            "source": [
                "first_batch = train_loader.dataset[: args.batch_size]\n",
                "\n",
                "print(first_batch.keys())\n",
                "print(first_batch[\"tokens\"].shape)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Training Loop\n",
                "\n",
                "If you did the material on [training loops](https://arena-ch0-fundamentals.streamlit.app/[0.3]_ResNets#training-loop) during the first week, this should all be familiar to you. If not, you can skim that section for an overview of the key concepts. The start of the **Training loop** section is most important, and the subsections on [Modularisation](https://arena-ch0-fundamentals.streamlit.app/[0.3]_ResNets#modularisation) and [dataclasses](https://arena-ch0-fundamentals.streamlit.app/[0.3]_ResNets#aside-dataclasses) are also very useful. Lastly, we'll also be using Weights and Biases to train our model - you can read about how to use it [here](https://arena-ch0-fundamentals.streamlit.app/[0.4]_Optimization#what-is-weights-and-biases). Here are (roughly) all the things you should know for the following exercises:\n",
                "                \n",
                "* The key parts of a gradient update step are:\n",
                "    * Calculating the (cross-entropy) loss between a model's output and the true labels,\n",
                "    * `loss.backward()` - calculate gradients of the loss with respect to the model parameters,\n",
                "    * `optimizer.step()` - update the model parameters using the gradients,\n",
                "    * `optimizer.zero_grad()` - zero the gradients so they don't accumulate.\n",
                "* We can nicely package up training loops into a class, which includes methods for training and validation steps among other things. This helps with writing code that can be reused in different contexts.\n",
                "* We can use dataclasses to store all the arguments relevant to training in one place, and then pass them to our trainer class. Autocompletion is one nice bonus of this!\n",
                "    * Be careful of scope here, you want to make sure you're referring to `self.args` within the trainer class, rather than the global `args`.\n",
                "* You can use Weights and Biases to track experiments and log relevant variables. The three essential functions are:\n",
                "    * `wandb.init()` - initialize a new run, takes arguments `project`, `name` and `config` (among others).\n",
                "    * `wandb.log()` - log a dictionary of variables, e.g. `{\"loss\": loss}`. Also takes a `step` argument.\n",
                "    * `wandb.finish()` - called at the end of training (no arguments)."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Exercise - write training loop\n",
                "\n",
                "```c\n",
                "Difficulty: 🟠🟠🟠⚪⚪\n",
                "Importance: 🟠🟠🟠🟠⚪\n",
                "\n",
                "You should spend up to 10-20 minutes on this exercise.\n",
                "```"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "You should fill in the methods below. Some guidance:\n",
                "\n",
                "* Remember we were able to calculate cross entropy loss using the `get_log_probs` function in the previous section.\n",
                "* You should use the optimizer `t.optim.AdamW` (Adam with weight decay), and with hyperparameters `lr` and `weight_decay` taken from your `TransformerTrainingArgs` dataclass instance.\n",
                "* The easiest way to compute accuracy is to have the `validation_step` method return a 1D boolean tensor indicating the positions where the model's prediction was correct. Then you can concatenate all these tensors together and take the mean to get the overall accuracy for the epoch.\n",
                "* We've given you the argument `max_steps_per_epoch`, a hacky way of making sure the training phase in each epoch doesn't go on for too long. You can terminate each training phase after this many steps.\n",
                "* Remember to move tokens to your device, via `tokens.to(device)` (this should be a global variable, defined at the top of your notebook).\n",
                "* You can refer back to the training loops from the [previous chapter of the course](https://arena-ch0-fundamentals.streamlit.app/[0.3]_ResNets#training-loop) if you'd like.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class TransformerTrainer:\n",
                "    def __init__(self, args: TransformerTrainingArgs, model: DemoTransformer):\n",
                "        super().__init__()\n",
                "        self.model = model\n",
                "        self.args = args\n",
                "        self.optimizer = torch.optim.AdamW(\n",
                "            self.model.parameters(), lr=args.lr, weight_decay=args.weight_decay\n",
                "        )\n",
                "        self.step = 0\n",
                "\n",
                "    def training_step(self, batch: Dict[str, Int[Tensor, \"batch seq\"]]) -> Float[Tensor, \"\"]:\n",
                "        \"\"\"\n",
                "        Calculates the loss on the tokens in the batch, performs a gradient update step, and logs the loss.\n",
                "\n",
                "        Remember that `batch` is a dictionary with the single key 'tokens'.\n",
                "        \"\"\"\n",
                "        # YOUR CODE HERE\n",
                "        \n",
                "        tokens = batch['tokens'].to(device)\n",
                "\n",
                "        logits = self.model(tokens)\n",
                "        loss = -get_log_probs(logits, tokens).mean()\n",
                "        loss.backward()\n",
                "        self.optimizer.step()\n",
                "        self.optimizer.zero_grad()\n",
                "        self.step += 1\n",
                "        # wandb.log({\"train_loss\": loss}, step=self.step)\n",
                "        return loss\n",
                "\n",
                "    # For information !\n",
                "    def validation_step(self, batch: Dict[str, Int[Tensor, \"batch seq\"]]):\n",
                "        \"\"\"\n",
                "        Calculates & returns the accuracy on the tokens in the batch (i.e. how often the model's prediction\n",
                "        is correct). Logging should happen in the `train` function (after we've computed the accuracy for\n",
                "        the whole validation set).\n",
                "        \"\"\"\n",
                "        # YOUR CODE HERE\n",
                "        tokens = batch['tokens'].to(device)\n",
                "\n",
                "        logits: Tensor = self.model(tokens)\n",
                "        predicted = logits.argmax(dim=-1)\n",
                "        correct_predictions = (predicted == tokens).flatten()\n",
                "        return correct_predictions\n",
                "\n",
                "    def train(self):\n",
                "        \"\"\"\n",
                "        Trains the model, for `self.args.epochs` epochs. Also handles wandb initialisation, and early stopping\n",
                "        for each epoch at `self.args.max_steps_per_epoch` steps.\n",
                "        \"\"\"\n",
                "        # YOUR CODE HERE\n",
                "        # wandb.init(project=self.args.wandb_project, name=self.args.wandb_name, config=self.args)\n",
                "        accuracy = np.nan\n",
                "        self.model.train()\n",
                "\n",
                "        progress = tqdm(total=self.args.epochs * self.args.max_steps_per_epoch)\n",
                "\n",
                "        for epoch in range(self.args.epochs):\n",
                "            for batch_idx, batch in enumerate(self.train_loader()):\n",
                "                loss = self.training_step(batch)\n",
                "                progress.update()\n",
                "                progress.set_description(f\"Epoch {epoch} - Batch {batch_idx} - Loss {loss.item():.3f} - Accuracy {accuracy:.3f}\")\n",
                "                if batch_idx >= self.args.max_steps_per_epoch:\n",
                "                    break\n",
                "            correct_predictions = torch.concat([self.validation_step(batch) for batch in self.test_loader()])\n",
                "            accuracy = correct_predictions.float().mean().item()\n",
                "            # wandb.log({\"accuracy\": accuracy}, step=self.step)\n",
                "        # wandb.finish()\n",
                "                \n",
                "\n",
                "    def train_loader(self) -> DataLoader:\n",
                "        \"\"\"Returns train loader (as in code above).\"\"\"\n",
                "        return DataLoader(\n",
                "            dataset_dict[\"train\"],\n",
                "            batch_size=self.args.batch_size,\n",
                "            shuffle=True,\n",
                "            num_workers=4,\n",
                "            pin_memory=True,\n",
                "        )\n",
                "\n",
                "    def test_loader(self) -> DataLoader:\n",
                "        \"\"\"Returns test loader (as in code above).\"\"\"\n",
                "        return DataLoader(\n",
                "            dataset_dict[\"test\"],\n",
                "            batch_size=self.args.batch_size,\n",
                "            shuffle=False,\n",
                "            num_workers=4,\n",
                "            pin_memory=True,\n",
                "        )"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "<details>\n",
                "<summary>Solution (one implementation)</summary>\n",
                "\n",
                "\n",
                "```python\n",
                "class TransformerTrainer:\n",
                "\tdef __init__(self, args: TransformerTrainingArgs, model: DemoTransformer):\n",
                "\t\tsuper().__init__()\n",
                "\t\tself.model = model\n",
                "\t\tself.args = args\n",
                "\t\tself.optimizer = t.optim.AdamW(self.model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
                "\t\tself.step = 0\n",
                "\n",
                "\n",
                "\tdef training_step(self, batch: Dict[str, Int[Tensor, \"batch seq\"]]) -> Float[Tensor, \"\"]:\n",
                "\t\t\"\"\"\n",
                "\t\tCalculates the loss on the tokens in the batch, performs a gradient update step, and logs the loss.\n",
                "\n",
                "\t\tRemember that `batch` is a dictionary with the single key 'tokens'.\n",
                "\t\t\"\"\"\n",
                "        # SOLUTION\n",
                "\t\ttokens = batch[\"tokens\"].to(device)\n",
                "\t\tlogits = self.model(tokens)\n",
                "\t\tloss = -get_log_probs(logits, tokens).mean()\n",
                "\t\tloss.backward()\n",
                "\t\tself.optimizer.step()\n",
                "\t\tself.optimizer.zero_grad()\n",
                "\t\tself.step += 1\n",
                "\t\twandb.log({\"train_loss\": loss}, step=self.step)\n",
                "\t\treturn loss\n",
                "\n",
                "\n",
                "\tdef validation_step(self, batch: Dict[str, Int[Tensor, \"batch seq\"]]):\n",
                "\t\t\"\"\"\n",
                "\t\tCalculates & returns the accuracy on the tokens in the batch (i.e. how often the model's prediction\n",
                "\t\tis correct). Logging should happen in the `train` function (after we've computed the accuracy for\n",
                "\t\tthe whole validation set).\n",
                "\t\t\"\"\"\n",
                "        # SOLUTION\n",
                "\t\ttokens = batch[\"tokens\"].to(device)\n",
                "\t\tlogits: Tensor = self.model(tokens)[:, :-1]\n",
                "\t\tpredicted_tokens = logits.argmax(dim=-1)\n",
                "\t\tcorrect_predictions = (predicted_tokens == tokens[:, 1:]).flatten()\n",
                "\t\treturn correct_predictions\n",
                "\n",
                "\n",
                "\tdef train(self):\n",
                "\t\t\"\"\"\n",
                "\t\tTrains the model, for `self.args.epochs` epochs. Also handles wandb initialisation, and early stopping\n",
                "\t\tfor each epoch at `self.args.max_steps_per_epoch` steps.\n",
                "\t\t\"\"\"\n",
                "        # SOLUTION\n",
                "\t\twandb.init(project=self.args.wandb_project, name=self.args.wandb_name, config=self.args)\n",
                "\t\taccuracy = np.nan\n",
                "\n",
                "\t\tprogress_bar = tqdm(total = self.args.max_steps_per_epoch * self.args.epochs)\n",
                "\n",
                "\t\tfor epoch in range(self.args.epochs):\n",
                "\t\t\tfor i, batch in enumerate(self.train_loader()):\n",
                "\t\t\t\tloss = self.training_step(batch)\n",
                "\t\t\t\tprogress_bar.update()\n",
                "\t\t\t\tprogress_bar.set_description(f\"Epoch {epoch+1}, loss: {loss:.3f}, accuracy: {accuracy:.2f}\")\n",
                "\t\t\t\tif i >= self.args.max_steps_per_epoch:\n",
                "\t\t\t\t\tbreak\n",
                "\n",
                "\t\t\tcorrect_predictions = t.concat([self.validation_step(batch) for batch in self.test_loader()])\n",
                "\t\t\taccuracy = correct_predictions.float().mean().item()\n",
                "\t\t\twandb.log({\"accuracy\": accuracy}, step=self.step)\n",
                "\n",
                "\t\twandb.finish()\n",
                "\n",
                "\n",
                "\tdef train_loader(self) -> DataLoader:\n",
                "\t\t\"\"\"Returns train loader (as in code above).\"\"\"\n",
                "\t\treturn DataLoader(dataset_dict[\"train\"], batch_size=self.args.batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
                "\n",
                "\n",
                "\tdef test_loader(self) -> DataLoader:\n",
                "\t\t\"\"\"Returns test loader (as in code above).\"\"\"\n",
                "\t\treturn DataLoader(dataset_dict[\"test\"], batch_size=self.args.batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
                "```\n",
                "</details>"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Note - this section of the course used to use PyTorch Lightning, but this has now been taken out. You can see the old version of the training code which used PyTorch Lightning in the dropdown below.\n",
                "\n",
                "<details>\n",
                "<summary>PyTorch Lighting training loop</summary>\n",
                "\n",
                "```python\n",
                "class LitTransformer(pl.LightningModule):\n",
                "\tdef __init__(self, args: TransformerTrainingArgs, model: DemoTransformer, data_loader: DataLoader):\n",
                "\t\tsuper().__init__()\n",
                "\t\tself.model = model\n",
                "\t\tself.cfg = model.cfg\n",
                "\t\tself.args = args\n",
                "\t\tself.data_loader = data_loader\n",
                "\n",
                "\tdef forward(self, tokens: Int[Tensor, \"batch position\"]) -> Float[Tensor, \"batch position d_vocab\"]:\n",
                "\t\tlogits = self.model(tokens)\n",
                "\t\treturn logits\n",
                "\n",
                "\tdef training_step(self, batch: Dict[str, Tensor], batch_idx: int) -> Float[Tensor, \"\"]:\n",
                "\t\t\"\"\"\n",
                "\t\tHere you compute and return the training loss and some additional metrics for e.g.\n",
                "\t\tthe progress bar or logger.\n",
                "\t\t\"\"\"\n",
                "\t\ttokens = batch[\"tokens\"].to(device)\n",
                "\t\tlogits = self.model(tokens)\n",
                "\t\tloss = -get_log_probs(logits, tokens).mean()\n",
                "\t\tself.log(\"train_loss\", loss)\n",
                "\t\treturn loss\n",
                "\n",
                "\tdef configure_optimizers(self):\n",
                "\t\t\"\"\"\n",
                "\t\tChoose what optimizers and learning-rate schedulers to use in your optimization.\n",
                "\t\t\"\"\"\n",
                "\t\toptimizer = t.optim.AdamW(self.model.parameters(), lr=self.args.lr, weight_decay=self.args.weight_decay)\n",
                "\t\treturn optimizer\n",
                "\n",
                "\tdef train_dataloader(self):\n",
                "\t\treturn self.data_loader\n",
                "\n",
                "\n",
                "litmodel = LitTransformer(args, model, data_loader)\n",
                "logger = WandbLogger(save_dir=args.log_dir, project=args.log_name, name=args.run_name)\n",
                "\n",
                "trainer = pl.Trainer(\n",
                "    max_epochs=args.max_epochs,\n",
                "    logger=logger,\n",
                "    log_every_n_steps=args.log_every_n_steps\n",
                ")\n",
                "trainer.fit(model=litmodel, train_dataloaders=litmodel.data_loader)\n",
                "wandb.finish()\n",
                "```\n",
                "\n",
                "</details>"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 35,
            "metadata": {},
            "outputs": [],
            "source": [
                "model = DemoTransformer(model_cfg).to(device)\n",
                "args = TransformerTrainingArgs()\n",
                "trainer = TransformerTrainer(args, model)\n",
                "# trainer.train()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "When you run the code for the first time, you'll have to login to Weights and Biases, and paste an API key into VSCode. After this is done, your Weights and Biases training run will start. It'll give you a lot of output text, one line of which will look like:\n",
                "\n",
                "```\n",
                "View run at https://wandb.ai/<USERNAME>/<PROJECT-NAME>/runs/<RUN-NAME>\n",
                "```\n",
                "\n",
                "which you can click on to visit the run page.\n",
                "\n",
                "> Note - to see the plots more clearly in Weights and Biases, you can click on the **edit panel** of your plot (the small pencil symbol at the top-right), then move the **smoothing** slider to the right."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### A note on this loss curve (optional)\n",
                "\n",
                "\n",
                "What's up with the shape of our loss curve? It seems like we start at around 10-11, drops down very fast, but then levels out. It turns out, this is all to do with the kinds of algorithms the model learns during training.\n",
                "\n",
                "When it starts out, your model will be outputting random noise, which might look a lot like \"predict each token with approximately uniform probability\", i.e. $Q(x) = 1/d_\\text{vocab}$ for all $x$. This gives us a cross entropy loss of $\\log (d_\\text{vocab})$.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 36,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "d_vocab = 50257\n",
                        "Cross entropy loss on uniform distribution = 10.82490511970208\n"
                    ]
                }
            ],
            "source": [
                "d_vocab = model.cfg.d_vocab\n",
                "\n",
                "print(f\"d_vocab = {d_vocab}\")\n",
                "print(f\"Cross entropy loss on uniform distribution = {math.log(d_vocab)}\")"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The next thing we might expect the model to learn is the frequencies of words in the english language. After all, small common tokens like `\" and\"` or `\" the\"` might appear much more frequently than others. This would give us an average cross entropy loss of:\n",
                "\n",
                "$$\n",
                "- \\sum_x p_x \\log p_x\n",
                "$$\n",
                "\n",
                "where $p_x$ is the actual frequency of the word in our training data.\n",
                "\n",
                "We can evaluate this quantity as follows:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 37,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Entropy of training data = 7.349369525909424\n"
                    ]
                }
            ],
            "source": [
                "toks = tokenized_dataset[:][\"tokens\"].flatten()\n",
                "\n",
                "d_vocab = model.cfg.d_vocab\n",
                "freqs = torch.bincount(toks, minlength=d_vocab)\n",
                "probs = freqs.float() / freqs.sum()\n",
                "\n",
                "distn = torch.distributions.categorical.Categorical(probs=probs)\n",
                "entropy = distn.entropy()\n",
                "\n",
                "print(f\"Entropy of training data = {entropy}\")"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "After unigram frequencies, the next thing our model usually learns is **bigram frequencies** (i.e. the frequency of pairs of adjacent tokens in the training data). For instance, `\"I\"` and `\" am\"` are common tokens, but their bigram frequency is much higher than it would be if they occurred independently. Bigram frequencies actually take you pretty far, since they also help with:\n",
                "\n",
                "* Some simple grammatical rules (e.g. a full stop being followed by a capitalized word)\n",
                "* Weird quirks of tokenization (e.g. `\" manip\"` being followed by `\"ulative\"`)\n",
                "* Common names (e.g. `\"Barack\"` being followed by `\" Obama\"`)\n",
                "\n",
                "\n",
                "After approximating bigram frequencies, we need to start using smarter techniques, like trigrams (which can only be implemented using attention heads), **induction heads** (which we'll learn a lot more about in the next set of exercises!), and fact memorization or more basic grammar and syntax rules. Marginal improvement starts getting a lot harder around here, leading to a flattening of our loss curve."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 4️⃣ (Bonus) Sampling from a Transformer\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "> ##### Learning objectives\n",
                ">\n",
                "> * Learn how to sample from a transformer\n",
                ">     * This includes basic methods like greedy search or top-k, and more advanced methods like beam search\n",
                "> * Learn how to cache the output of a transformer, so that it can be used to generate text more efficiently\n",
                ">     * Optionally, rewrite your sampling functions to make use of your caching methods\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "One obvious method to sample tokens from a distribution would be to always take the token assigned the highest probability. But this can lead to some boring and repetitive outcomes, and at worst it can lock our transformer's output into a loop.\n",
                "\n",
                "First, you should read HuggingFace's blog post [How to generate text: using different decoding methods for language generation with Transformers](https://huggingface.co/blog/how-to-generate).\n",
                "\n",
                "Once you've done that, you can work through the `TransformerSampler` class below, and implement the different sampling methods. Each method will come with its own tests, and demo code for you to run.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 38,
            "metadata": {},
            "outputs": [],
            "source": [
                "model_cfg = Config()\n",
                "model = DemoTransformer(model_cfg).to(device)\n",
                "model.load_state_dict(reference_gpt2.state_dict(), strict=False)\n",
                "\n",
                "tokenizer = reference_gpt2.tokenizer"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 108,
            "metadata": {},
            "outputs": [],
            "source": [
                "class TransformerSampler:\n",
                "    def __init__(self, model: DemoTransformer, tokenizer: GPT2TokenizerFast):\n",
                "        self.model = model\n",
                "        self.cfg = model.cfg\n",
                "        self.tokenizer = tokenizer\n",
                "\n",
                "    @torch.inference_mode()\n",
                "    def sample(self, prompt: str, max_tokens_generated=100, verbose=False, **kwargs):\n",
                "        \"\"\"\n",
                "        Returns a string of autoregressively generated text, starting from the prompt.\n",
                "\n",
                "        Sampling terminates at max_tokens_generated, or when the model generates an\n",
                "        end-of-sequence token.\n",
                "\n",
                "        kwargs are passed to sample_next_token, to give detailed instructions on how\n",
                "        new tokens are chosen.\n",
                "        \"\"\"\n",
                "        self.model.eval()\n",
                "        tokens = self.tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
                "\n",
                "        for i in range(max_tokens_generated):\n",
                "            logits = self.model(tokens[0, :self.cfg.n_ctx].unsqueeze_(0))\n",
                "            logits = logits[0, -1]\n",
                "\n",
                "            next_token = self.sample_next_token(tokens.flatten(), logits, **kwargs)\n",
                "            tokens = torch.cat((tokens, torch.tensor([[next_token]])), dim=-1)\n",
                "\n",
                "            if verbose:\n",
                "                print(self.tokenizer.decode(tokens), end=\"\\r\")\n",
                "            if next_token == self.tokenizer.eos_token_id:\n",
                "                print(f\"End at => {i}\")\n",
                "                break\n",
                "        return self.tokenizer.decode(tokens.flatten())\n",
                "\n",
                "    @torch.inference_mode()\n",
                "    def beam_search(\n",
                "        self,\n",
                "        prompt: str,\n",
                "        num_return_sequences: int,\n",
                "        num_beams: int,\n",
                "        max_new_tokens: int,\n",
                "        no_repeat_ngram_size: int = 0,\n",
                "        verbose=False,\n",
                "    ) -> List[Tuple[float, torch.Tensor]]:\n",
                "        \"\"\"\n",
                "        Returns a string of autoregressively generated text, starting from the prompt.\n",
                "\n",
                "        Sampling terminates at max_tokens_generated, or when the model generates an\n",
                "        end-of-sequence token.\n",
                "\n",
                "        kwargs are passed to sample_next_token, to give detailed instructions on how\n",
                "        new tokens are chosen.\n",
                "        \"\"\"\n",
                "        pass\n",
                "\n",
                "    @staticmethod\n",
                "    def sample_next_token(\n",
                "        input_ids: Int[Tensor, \"seq_len\"],\n",
                "        logits: Float[Tensor, \"seq_len d_vocab\"],\n",
                "        temperature=1.0,\n",
                "        top_k=0,\n",
                "        top_p=0.0,\n",
                "        frequency_penalty=0.0,\n",
                "        seed=None,\n",
                "    ):\n",
                "        assert input_ids.ndim == 1, \"input_ids should be a 1D sequence of token ids\"\n",
                "        assert temperature >= 0, \"Temperature should be non-negative\"\n",
                "        assert 0 <= top_p <= 1.0, \"Top-p must be a probability\"\n",
                "        assert 0 <= top_k, \"Top-k must be non-negative\"\n",
                "        assert not (top_p != 0 and top_k != 0), \"At most one of top-p and top-k supported\"\n",
                "\n",
                "        # Set random seeds for reproducibility\n",
                "        if seed is not None:\n",
                "            torch.manual_seed(seed)\n",
                "            np.random.seed(seed)\n",
                "\n",
                "        # Apply all the specialized sampling methods\n",
                "        if temperature == 0:\n",
                "            return TransformerSampler.greedy_search(logits)\n",
                "        elif temperature != 1.0:\n",
                "            logits = TransformerSampler.apply_temperature(logits, temperature)\n",
                "        if frequency_penalty != 0.0:\n",
                "            logits = TransformerSampler.apply_frequency_penalty(\n",
                "                input_ids, logits, frequency_penalty\n",
                "            )\n",
                "        if top_k > 0:\n",
                "            return TransformerSampler.sample_top_k(logits, top_k)\n",
                "        if top_p > 0.0:\n",
                "            return TransformerSampler.sample_top_p(logits, top_p)\n",
                "        return TransformerSampler.sample_basic(logits)\n",
                "\n",
                "    @staticmethod\n",
                "    def greedy_search(logits: Float[Tensor, \"d_vocab\"]) -> int:\n",
                "        \"\"\"\n",
                "        Returns the most likely token (as an int).\n",
                "        \"\"\"\n",
                "        out = logits.argmax().item()\n",
                "        return out\n",
                "\n",
                "    @staticmethod\n",
                "    def apply_temperature(\n",
                "        logits: Float[Tensor, \"d_vocab\"], temperature: float\n",
                "    ) -> Float[Tensor, \"d_vocab\"]:\n",
                "        \"\"\"\n",
                "        Applies temperature scaling to the logits.\n",
                "        \"\"\"\n",
                "        assert temperature > 0\n",
                "        return logits / temperature\n",
                "\n",
                "    @staticmethod\n",
                "    def apply_frequency_penalty(\n",
                "        input_ids: Int[Tensor, \"seq_len\"],\n",
                "        logits: Float[Tensor, \"d_vocab\"],\n",
                "        freq_penalty: float,\n",
                "    ) -> Float[Tensor, \"d_vocab\"]:\n",
                "        \"\"\"\n",
                "        Applies a frequency penalty to the logits.\n",
                "        \"\"\"\n",
                "        occurences = torch.bincount(input_ids, minlength=logits.shape[0]) # bincount met le nombre d'occurence au bon index.\n",
                "        return logits - occurences * freq_penalty\n",
                "\n",
                "    @staticmethod\n",
                "    def sample_basic(logits: Float[Tensor, \"d_vocab\"]) -> int:\n",
                "        \"\"\"\n",
                "        Samples from the distribution defined by the logits.\n",
                "        \"\"\"\n",
                "        probs = torch.distributions.categorical.Categorical(logits=logits)\n",
                "        return probs.sample().item()\n",
                "\n",
                "    @staticmethod\n",
                "    def sample_top_k(logits: Float[Tensor, \"d_vocab\"], k: int) -> int:\n",
                "        \"\"\"\n",
                "        Samples from the top k most likely tokens.\n",
                "        \"\"\"\n",
                "        top_values, top_idx = torch.topk(input=logits, k=k)\n",
                "        probs = torch.distributions.categorical.Categorical(logits=top_values)\n",
                "        return top_idx[probs.sample().item()].item()\n",
                "\n",
                "    @staticmethod\n",
                "    def sample_top_p(\n",
                "        logits: Float[Tensor, \"d_vocab\"], top_p: float, min_tokens_to_keep: int = 1\n",
                "    ) -> int:\n",
                "        \"\"\"\n",
                "        Samples from the most likely tokens which make up at least p cumulative probability.\n",
                "        \"\"\"\n",
                "        ## Sorting ##\n",
                "        logits_sorted = logits.sort(descending=True)\n",
                "        values, indexes = logits_sorted.values, logits_sorted.indices\n",
                "\n",
                "        ## Softmax for probas ##\n",
                "        values = values.softmax(-1)\n",
                "        ## Cumsum for the cutoff ##\n",
                "        cumsum_top_p = values.cumsum(-1)\n",
                "\n",
                "        ## Searching the min tokens ##\n",
                "        nb_toks = max(torch.searchsorted(cumsum_top_p, top_p, side=\"right\") + 1, min_tokens_to_keep)\n",
                "\n",
                "        ## Indexes of the chosen toks ##\n",
                "        idx_keep = indexes[:nb_toks]\n",
                "\n",
                "        ## Get the specifics logits ##\n",
                "        c_logits = logits[idx_keep]\n",
                "\n",
                "        ## Sample and get from idx_keep ##\n",
                "        probs = torch.distributions.categorical.Categorical(logits=c_logits)\n",
                "        return idx_keep[probs.sample().item()].item()\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Main Sampling Function\n",
                "\n",
                "The first thing you should do is implement the `sample` method.\n",
                "\n",
                "### Exercise - implement `sample`\n",
                "\n",
                "```c\n",
                "Difficulty: 🟠🟠🟠⚪⚪\n",
                "Importance: 🟠🟠🟠⚪⚪\n",
                "\n",
                "You should spend up to 20-25 minutes on this exercise.\n",
                "```\n",
                "\n",
                "This function takes in a prompt (in the form of a string), encodes it as a sequence of token ids using `self.tokenizer.encode`, and then continually generates new tokens by repeating the following steps:\n",
                "\n",
                "1. Passing the tokenized prompt through the model to get logits,\n",
                "2. Taking the logit vector corresponding to the last token in the prompt (i.e. the prediction for the *next* token),\n",
                "3. Sampling from this distribution to get a new token, using `self.sample_next_token(input_ids, logits, **kwargs)` (here, `kwargs` contains all the sampling-specific arguments, e.g. temperature, top-k, etc.),\n",
                "4. Appending this new token to the input tokens, and repeating the process until we meet one of two termination critera:\n",
                "    * We generate `max_tokens_generated` new tokens, or\n",
                "    * We generate the end-of-sequence token (which we can access via `self.tokenizer.eos_token_id`).\n",
                "\n",
                "Finally, we use `self.tokenizer.decode` to convert the generated token ids back into a string, and return this string.\n",
                "\n",
                "We also have a `verbose` argument - when this is true you can print your output while it's being sampled.\n",
                "\n",
                "Below is some code which tests your sampling function by performing greedy sampling (which means always choosing the most likely next token at each step).\n",
                "\n",
                "<details>\n",
                "<summary>Why does <code>temperature=0.0</code> correspond to greedy sampling?</summary>\n",
                "\n",
                "To apply a temperature to our sampling (as we'll see later) means to scale all logits by `(1 / temperature)`. The basic intuition here is:\n",
                "\n",
                "* A higher temperature means a smaller scale factor, so the logits all approach zero, i.e. uniform distribution, and the sampling process is a lot more random (producing more diverse and varied outputs)\n",
                "* A lower temperature means a larger scale factor, so the logits all approach infinity, i.e. a dirac delta function, and the sampling process is a lot more deterministic (producing less varied output)\n",
                "\n",
                "As temperature gets close to zero, the difference between the largest logit and second largest logit becomes very large, so the distribution tends to \"probability of 1 on the highest-likelihood token\", i.e. greedy sampling. You can derive this formally if you prefer.\n",
                "</details>\n",
                "\n",
                "A few hints:\n",
                "\n",
                "* Don't forget about tensor shapes! Your model's input should always have a batch dimension, i.e. it should be shape `(1, seq_len)`.\n",
                "* Also remember to have your tensors be on the same device (we have a global `device` variable).\n",
                "* Remember to put your model in evaluation mode, using `model.eval()`.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 40,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Greedy decoding with prompt: 'Jingle bells, jingle bells, jingle all the way'\n",
                        "\n",
                        "Your model said: 'Jingle bells, jingle bells, jingle all the way up to the top of the mountain.'\n",
                        "\n",
                        "Tests passed!\n"
                    ]
                }
            ],
            "source": [
                "sampler = TransformerSampler(model, tokenizer)\n",
                "\n",
                "prompt = \"Jingle bells, jingle bells, jingle all the way\"\n",
                "print(f\"Greedy decoding with prompt: {prompt!r}\\n\")\n",
                "\n",
                "output = sampler.sample(prompt, max_tokens_generated=8, temperature=0.0)\n",
                "print(f\"Your model said: {output!r}\\n\")\n",
                "\n",
                "expected = \"Jingle bells, jingle bells, jingle all the way up to the top of the mountain.\"\n",
                "assert output == expected\n",
                "\n",
                "print(\"Tests passed!\")"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<details>\n",
                "<summary>Solution</summary>\n",
                "\n",
                "```python\n",
                "@t.inference_mode()\n",
                "def sample(self, prompt, max_tokens_generated=100, verbose=False, **kwargs):\n",
                "    # SOLUTION\n",
                "    self.model.eval()\n",
                "    input_ids = self.tokenizer.encode(prompt, return_tensors=\"pt\").to(device)[0]\n",
                "\n",
                "    for i in range(max_tokens_generated):\n",
                "        # Get new logits (make sure we don't pass in more tokens than the model's context length)\n",
                "        logits = self.model(input_ids[None, -self.cfg.n_ctx:])\n",
                "        # We only take logits for the last token, because this is what we're sampling\n",
                "        logits = logits[0, -1]\n",
                "        # Get next token (as a tensor of size (1, 1) so we can concat it to input_ids)\n",
                "        next_token = t.tensor([self.sample_next_token(input_ids, logits, **kwargs)], device=device)\n",
                "        # Create new input ids string, with shape (1, old_seq_len + 1)\n",
                "        input_ids = t.cat([input_ids, next_token], dim=-1)\n",
                "        # Print out results, if required\n",
                "        if verbose:\n",
                "            print(self.tokenizer.decode(input_ids), end=\"\\r\")\n",
                "        # If our new token was the end-of-text token, stop\n",
                "        if next_token == getattr(self.tokenizer, \"eos_token_id\", None):\n",
                "            break\n",
                "    \n",
                "    return self.tokenizer.decode(input_ids)\n",
                "```\n",
                "```python\n",
                "class TransformerSampler:\n",
                "\n",
                "    def __init__(self, model: DemoTransformer, tokenizer: GPT2TokenizerFast):\n",
                "        self.model = model\n",
                "        self.cfg = model.cfg\n",
                "        self.tokenizer = tokenizer\n",
                "\n",
                "    @t.inference_mode()\n",
                "    def sample(self, prompt: str, max_tokens_generated=100, verbose=False, **kwargs):\n",
                "        \"\"\"\n",
                "        Returns a string of autoregressively generated text, starting from the prompt.\n",
                "\n",
                "        Sampling terminates at max_tokens_generated, or when the model generates an\n",
                "        end-of-sequence token.\n",
                "\n",
                "        kwargs are passed to sample_next_token, to give detailed instructions on how\n",
                "        new tokens are chosen.\n",
                "        \"\"\"\n",
                "        # SOLUTION\n",
                "        self.model.eval()\n",
                "        input_ids = self.tokenizer.encode(prompt, return_tensors=\"pt\").to(device)[0]\n",
                "\n",
                "        for i in range(max_tokens_generated):\n",
                "            # Get new logits (make sure we don't pass in more tokens than the model's context length)\n",
                "            logits = self.model(input_ids[None, -self.cfg.n_ctx:])\n",
                "            # We only take logits for the last token, because this is what we're sampling\n",
                "            logits = logits[0, -1]\n",
                "            # Get next token (as a tensor of size (1, 1) so we can concat it to input_ids)\n",
                "            next_token = t.tensor([TransformerSampler.sample_next_token(input_ids, logits, **kwargs)], device=device)\n",
                "            # Create new input ids string, with shape (1, old_seq_len + 1)\n",
                "            input_ids = t.cat([input_ids, next_token], dim=-1)\n",
                "            # Print out results, if required\n",
                "            if verbose:\n",
                "                print(self.tokenizer.decode(input_ids), end=\"\\r\")\n",
                "            # If our new token was the end-of-text token, stop\n",
                "            if next_token == getattr(self.tokenizer, \"eos_token_id\", None):\n",
                "                break\n",
                "        \n",
                "        return self.tokenizer.decode(input_ids)\n",
                "    \n",
                "    \n",
                "    \n",
                "    @t.inference_mode()\n",
                "    def beam_search(\n",
                "        self,\n",
                "        prompt: str,\n",
                "        num_return_sequences: int,\n",
                "        num_beams: int,\n",
                "        max_new_tokens: int,\n",
                "        no_repeat_ngram_size: int = 0,\n",
                "        verbose=False\n",
                "    ) -> List[Tuple[float, t.Tensor]]:\n",
                "        \"\"\"\n",
                "        Returns a string of autoregressively generated text, starting from the prompt.\n",
                "\n",
                "        Sampling terminates at max_tokens_generated, or when the model generates an\n",
                "        end-of-sequence token.\n",
                "\n",
                "        kwargs are passed to sample_next_token, to give detailed instructions on how\n",
                "        new tokens are chosen.\n",
                "        \"\"\"\n",
                "        pass\n",
                "    \n",
                "\n",
                "    @staticmethod\n",
                "    def sample_next_token(\n",
                "        input_ids: Int[Tensor, \"seq_len\"],\n",
                "        logits: Float[Tensor, \"seq_len d_vocab\"],\n",
                "        temperature=1.0,\n",
                "        top_k=0,\n",
                "        top_p=0.0,\n",
                "        frequency_penalty=0.0,\n",
                "        seed=None\n",
                "    ):\n",
                "        assert input_ids.ndim == 1, \"input_ids should be a 1D sequence of token ids\"\n",
                "        assert temperature >= 0, \"Temperature should be non-negative\"\n",
                "        assert 0 <= top_p <= 1.0, \"Top-p must be a probability\"\n",
                "        assert 0 <= top_k, \"Top-k must be non-negative\"\n",
                "        assert not (top_p != 0 and top_k != 0), \"At most one of top-p and top-k supported\"\n",
                "\n",
                "        # Set random seeds for reproducibility\n",
                "        if seed is not None:\n",
                "            t.manual_seed(seed)\n",
                "            np.random.seed(seed)\n",
                "\n",
                "        # Apply all the specialized sampling methods\n",
                "        if temperature == 0:\n",
                "            return TransformerSampler.greedy_search(logits)\n",
                "        elif temperature != 1.0:\n",
                "            logits = TransformerSampler.apply_temperature(logits, temperature)\n",
                "        if frequency_penalty != 0.0:\n",
                "            logits = TransformerSampler.apply_frequency_penalty(input_ids, logits, frequency_penalty)\n",
                "        if top_k > 0:\n",
                "            return TransformerSampler.sample_top_k(logits, top_k)\n",
                "        if top_p > 0.0:\n",
                "            return TransformerSampler.sample_top_p(logits, top_p)\n",
                "        return TransformerSampler.sample_basic(logits)\n",
                "\n",
                "\n",
                "    @staticmethod\n",
                "    def greedy_search(logits: Float[Tensor, \"d_vocab\"]) -> int:\n",
                "        \"\"\"\n",
                "        Returns the most likely token (as an int).\n",
                "        \"\"\"\n",
                "        out = logits.argmax().item()\n",
                "        return out\n",
                "    \n",
                "\n",
                "    @staticmethod\n",
                "    def apply_temperature(logits: Float[Tensor, \"d_vocab\"], temperature: float) -> Float[Tensor, \"d_vocab\"]:\n",
                "        \"\"\"\n",
                "        Applies temperature scaling to the logits.\n",
                "        \"\"\"\n",
                "        # SOLUTION\n",
                "        return logits / temperature\n",
                "    \n",
                "\n",
                "\n",
                "    @staticmethod\n",
                "    def apply_frequency_penalty(input_ids: Int[Tensor, \"seq_len\"], logits: Float[Tensor, \"d_vocab\"], freq_penalty: float) -> Float[Tensor, \"d_vocab\"]:\n",
                "        \"\"\"\n",
                "        Applies a frequency penalty to the logits.\n",
                "        \"\"\"\n",
                "        # SOLUTION\n",
                "        d_vocab = logits.size(0)\n",
                "        id_freqs = t.bincount(input_ids, minlength=d_vocab)\n",
                "        return logits - freq_penalty * id_freqs\n",
                "    \n",
                "\n",
                "    \n",
                "    @staticmethod\n",
                "    def sample_basic(logits: Float[Tensor, \"d_vocab\"]) -> int:\n",
                "        \"\"\"\n",
                "        Samples from the distribution defined by the logits.\n",
                "        \"\"\"\n",
                "        # SOLUTION\n",
                "        sampled_token = t.distributions.categorical.Categorical(logits=logits).sample()\n",
                "        return sampled_token.item()\n",
                "    \n",
                "\n",
                "\n",
                "    @staticmethod\n",
                "    def sample_top_k(logits: Float[Tensor, \"d_vocab\"], k: int) -> int:\n",
                "        \"\"\"\n",
                "        Samples from the top k most likely tokens.\n",
                "        \"\"\"\n",
                "        # SOLUTION\n",
                "        top_k_logits, top_k_token_ids = logits.topk(k)\n",
                "        # Get sampled token (which is an index corresponding to the list of top-k tokens)\n",
                "        sampled_token_idx = t.distributions.categorical.Categorical(logits=top_k_logits).sample()\n",
                "        # Get the actual token id, as an int\n",
                "        return top_k_token_ids[sampled_token_idx].item()\n",
                "    \n",
                "\n",
                "\n",
                "    @staticmethod\n",
                "    def sample_top_p(logits: Float[Tensor, \"d_vocab\"], top_p: float, min_tokens_to_keep: int = 1) -> int:\n",
                "        \"\"\"\n",
                "        Samples from the most likely tokens which make up at least p cumulative probability.\n",
                "        \"\"\"\n",
                "        # SOLUTION\n",
                "        # Sort logits, and get cumulative probabilities\n",
                "        logits_sorted, indices = logits.sort(descending=True, stable=True)\n",
                "        cumul_probs = logits_sorted.softmax(-1).cumsum(-1)\n",
                "        # Choose which tokens to keep, in the set we sample from\n",
                "        n_keep = t.searchsorted(cumul_probs, top_p, side=\"left\").item() + 1\n",
                "        n_keep = max(n_keep, min_tokens_to_keep)\n",
                "        keep_idx = indices[:n_keep]\n",
                "        keep_logits = logits[keep_idx]\n",
                "        # Perform the sampling\n",
                "        sample = t.distributions.categorical.Categorical(logits=keep_logits).sample()\n",
                "        return keep_idx[sample].item()\n",
                "```\n",
                "</details>\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Sampling with Categorical\n",
                "\n",
                "Now, we'll move into implementing specific sampling methods.\n",
                "\n",
                "PyTorch provides a [`distributions` package](https://pytorch.org/docs/stable/distributions.html#distribution) with a number of convenient methods for sampling from various distributions.\n",
                "\n",
                "For now, we just need [`t.distributions.categorical.Categorical`](https://pytorch.org/docs/stable/distributions.html#categorical). Use this to implement `sample_basic`, which just samples from the provided logits (which may have already been modified by the temperature and frequency penalties).\n",
                "\n",
                "Note that this will be slow since we aren't batching the samples, but don't worry about speed for now.\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Exercise - Basic Sampling\n",
                "\n",
                "```c\n",
                "Difficulty: 🟠🟠⚪⚪⚪\n",
                "Importance: 🟠🟠⚪⚪⚪\n",
                "\n",
                "You should spend up to 5-10 minutes on this exercise.\n",
                "```\n",
                "\n",
                "Implement basic sampling in the `TransformerSampler` class above, then run the code below to verify your solution works.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 41,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "fa795f9da6614d1d933f7b70e2699d6e",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "  0%|          | 0/10000 [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Word: ' church'. Expected freq 0.0648, observed freq 0.0655\n",
                        "Word: ' house' . Expected freq 0.0367, observed freq 0.0371\n",
                        "Word: ' temple'. Expected freq 0.0145, observed freq 0.0134\n",
                        "Word: ' same'  . Expected freq 0.0104, observed freq 0.0102\n",
                        "Word: ' Church'. Expected freq 0.0097, observed freq 0.0095\n",
                        "Tests passed!\n"
                    ]
                }
            ],
            "source": [
                "prompt = \"John and Mary went to the\"\n",
                "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
                "logits = model(input_ids)[0, -1]\n",
                "\n",
                "expected_top_5 = {\n",
                "    \" church\": 0.0648,\n",
                "    \" house\": 0.0367,\n",
                "    \" temple\": 0.0145,\n",
                "    \" same\": 0.0104,\n",
                "    \" Church\": 0.0097,\n",
                "}\n",
                "frequency_of_top_5 = defaultdict(int)\n",
                "\n",
                "N = 10_000\n",
                "for _ in tqdm(range(N)):\n",
                "    token = TransformerSampler.sample_next_token(input_ids.squeeze(), logits)\n",
                "    frequency_of_top_5[tokenizer.decode(token)] += 1\n",
                "\n",
                "for word in expected_top_5:\n",
                "    expected_freq = expected_top_5[word]\n",
                "    observed_freq = frequency_of_top_5[word] / N\n",
                "    print(\n",
                "        f\"Word: {word!r:<9}. Expected freq {expected_freq:.4f}, observed freq {observed_freq:.4f}\"\n",
                "    )\n",
                "    assert (\n",
                "        abs(observed_freq - expected_freq) < 0.01\n",
                "    ), \"Try increasing N if this fails by a small amount.\"\n",
                "\n",
                "print(\"Tests passed!\")"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<details>\n",
                "<summary>Solution</summary>\n",
                "\n",
                "```python\n",
                "def sample_basic(logits: t.Tensor) -> int:\n",
                "    \"\"\"\n",
                "    logits: shape (vocab_size, ) - unnormalized log-probabilities\n",
                "\n",
                "    Return: a sampled token\n",
                "    \"\"\"\n",
                "    distribution = t.distributions.categorical.Categorical(logits=logits)\n",
                "    out = distribution.sample().item()\n",
                "    assert isinstance(out, int)\n",
                "    return out\n",
                "```\n",
                "</details>\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Exercise - Temperature\n",
                "\n",
                "```c\n",
                "Difficulty: 🟠⚪⚪⚪⚪\n",
                "Importance: 🟠🟠⚪⚪⚪\n",
                "\n",
                "You should spend up to 5-10 minutes on this exercise.\n",
                "```\n",
                "\n",
                "Temperature sounds fancy, but it's literally just dividing the logits by the temperature. You should implement this in your `TransformerSampler` class now.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 42,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "A low temperature \"sharpens\" or \"peaks\" the distribution:  tensor([  0.0000, 693.1472])\n",
                        "A high temperature flattens the distribution:  tensor([0.0000, 0.0007])\n",
                        "Tests passed!\n"
                    ]
                }
            ],
            "source": [
                "logits = torch.tensor([1, 2]).log()\n",
                "\n",
                "cold_logits = TransformerSampler.apply_temperature(logits, temperature=0.001)\n",
                "print('A low temperature \"sharpens\" or \"peaks\" the distribution: ', cold_logits)\n",
                "torch.testing.assert_close(cold_logits, 1000.0 * logits)\n",
                "\n",
                "hot_logits = TransformerSampler.apply_temperature(logits, temperature=1000.0)\n",
                "print(\"A high temperature flattens the distribution: \", hot_logits)\n",
                "torch.testing.assert_close(hot_logits, 0.001 * logits)\n",
                "\n",
                "print(\"Tests passed!\")"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<details>\n",
                "<summary>Solution</summary>\n",
                "\n",
                "```python\n",
                "def apply_temperature(logits: t.Tensor, temperature: float) -> t.Tensor:\n",
                "    \"\"\"\n",
                "    logits: shape (vocab_size, )\n",
                "\n",
                "    Return: shape (vocab_size, )\n",
                "    \"\"\"\n",
                "    assert temperature > 0\n",
                "    return logits / temperature\n",
                "```\n",
                "</details>\n",
                "\n",
                "<details>\n",
                "<summary>Question - what is the limit of applying 'sample_basic' after adjusting with temperature, when temperature goes to zero? How about when temperature goes to infinity?</summary>\n",
                "\n",
                "The limit when temperature goes to zero is greedy search (because dividing by a small number makes the logits very big, in other words the difference between the maximum logit one and all the others will grow).\n",
                "\n",
                "The limit when temperature goes to infinity is uniform random sampling over all words (because all logits will be pushed towards zero).\")\n",
                "</details>\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Exercise - Frequency Penalty\n",
                "\n",
                "```c\n",
                "Difficulty: 🟠🟠⚪⚪⚪\n",
                "Importance: 🟠⚪⚪⚪⚪\n",
                "\n",
                "You should spend up to 10-15 minutes on this exercise.\n",
                "```\n",
                "\n",
                "The frequency penalty is simple as well: count the number of occurrences of each token, then subtract `freq_penalty` for each occurrence. Hint: use `t.bincount` (documentation [here](https://pytorch.org/docs/stable/generated/torch.bincount.html)) to do this in a vectorized way.\n",
                "\n",
                "You should implement the `apply_frequency_penalty` method in your `TransformerSampler` class now, then run the cell below to check your solution.\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<details>\n",
                "<summary>Help - I'm getting a <code>RuntimeError</code>; my tensor sizes don't match.</summary>\n",
                "\n",
                "Look at the documentation page for `t.bincount`. You might need to use the `minlength` argument - why?\n",
                "</details>\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 43,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Tests passed!\n"
                    ]
                }
            ],
            "source": [
                "bieber_prompt = \"And I was like Baby, baby, baby, oh Like, Baby, baby, baby, no Like, Baby, baby, baby, oh I thought you'd always be mine, mine\"\n",
                "input_ids = tokenizer.encode(bieber_prompt, return_tensors=\"pt\")\n",
                "logits = torch.ones(tokenizer.vocab_size)\n",
                "penalized_logits = TransformerSampler.apply_frequency_penalty(input_ids.squeeze(), logits, 2.0)\n",
                "\n",
                "assert (\n",
                "    penalized_logits[5156].item() == -11\n",
                "), \"Expected 6 occurrences of ' baby' with leading space, 1-2*6=-11\"\n",
                "assert (\n",
                "    penalized_logits[14801].item() == -5\n",
                "), \"Expected 3 occurrences of ' Baby' with leading space, 1-2*3=-5\"\n",
                "\n",
                "print(\"Tests passed!\")"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<details>\n",
                "<summary>Solution</summary>\n",
                "\n",
                "```python\n",
                "def apply_freq_penalty(input_ids: t.Tensor, logits: t.Tensor, freq_penalty: float) -> t.Tensor:\n",
                "    \"\"\"\n",
                "    input_ids: shape (seq, )\n",
                "    logits: shape (vocab_size, )\n",
                "    Return: shape (vocab_size, )\n",
                "    \"\"\"\n",
                "    (vocab_size,) = logits.shape\n",
                "    id_freqs = t.bincount(input_ids, minlength=vocab_size)\n",
                "    return logits - freq_penalty * id_freqs\n",
                "```\n",
                "</details>\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Sampling - Manual Testing\n",
                "\n",
                "Run the below cell to get a sense for the `temperature` and `freq_penalty` arguments. Play with your own prompt and try other values.\n",
                "\n",
                "Note: your model can generate newlines or non-printing characters, so calling `print` on generated text sometimes looks awkward on screen. You can call `repr` on the string before printing to have the string escaped nicely.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 44,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/html": [
                            "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                                             Sampling - Manual Testing                                             </span>\n",
                            "┏━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
                            "┃<span style=\"font-weight: bold\"> Name                  </span>┃<span style=\"font-weight: bold\"> Kwargs                       </span>┃<span style=\"font-weight: bold\"> Output                                                   </span>┃\n",
                            "┡━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
                            "│ High freq penalty     │ {'frequency_penalty': 100.0} │ 'Jingle bells, jingle bells, jingle all the way to State │\n",
                            "│                       │                              │ University School of Humanities.\\nThe main theme this    │\n",
                            "│                       │                              │ week is that we needed teamwork among students more than │\n",
                            "│                       │                              │ before'                                                  │\n",
                            "│                       │                              │                                                          │\n",
                            "│ Negative freq penalty │ {'frequency_penalty': -3.0}  │ 'Jingle bells, jingle bells, jingle all the way, jingle  │\n",
                            "│                       │                              │ bells, jingle bells, jingle, jingle, jingle, jingle,     │\n",
                            "│                       │                              │ jingle,'                                                 │\n",
                            "│                       │                              │                                                          │\n",
                            "│ Too hot!              │ {'temperature': 2.0}         │ 'Jingle bells, jingle bells, jingle all the way Around   │\n",
                            "│                       │                              │ brides vans Bills shel220 Seatamount by DESIG stabl WL   │\n",
                            "│                       │                              │ -------- (@tori6 Ideally Jan finished'                   │\n",
                            "│                       │                              │                                                          │\n",
                            "│ Pleasantly cool       │ {'temperature': 0.7}         │ \"Jingle bells, jingle bells, jingle all the way down.    │\n",
                            "│                       │                              │ It's not about the money, it's about the size of the     │\n",
                            "│                       │                              │ venue, the size of the room,\"                            │\n",
                            "│                       │                              │                                                          │\n",
                            "│ Pleasantly warm       │ {'temperature': 0.9}         │ \"Jingle bells, jingle bells, jingle all the way down to  │\n",
                            "│                       │                              │ my house. It's always going to be pretty loud, but       │\n",
                            "│                       │                              │ sometimes, once in awhile, I'll have\"                    │\n",
                            "│                       │                              │                                                          │\n",
                            "│ Too cold!             │ {'temperature': 0.01}        │ 'Jingle bells, jingle bells, jingle all the way up to    │\n",
                            "│                       │                              │ the top of the mountain.\\n\\nThe first time I saw the     │\n",
                            "│                       │                              │ mountain, I was in the middle of'                        │\n",
                            "│                       │                              │                                                          │\n",
                            "└───────────────────────┴──────────────────────────────┴──────────────────────────────────────────────────────────┘\n",
                            "</pre>\n"
                        ],
                        "text/plain": [
                            "\u001b[3m                                             Sampling - Manual Testing                                             \u001b[0m\n",
                            "┏━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
                            "┃\u001b[1m \u001b[0m\u001b[1mName                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mKwargs                      \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput                                                  \u001b[0m\u001b[1m \u001b[0m┃\n",
                            "┡━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
                            "│ High freq penalty     │ {'frequency_penalty': 100.0} │ 'Jingle bells, jingle bells, jingle all the way to State │\n",
                            "│                       │                              │ University School of Humanities.\\nThe main theme this    │\n",
                            "│                       │                              │ week is that we needed teamwork among students more than │\n",
                            "│                       │                              │ before'                                                  │\n",
                            "│                       │                              │                                                          │\n",
                            "│ Negative freq penalty │ {'frequency_penalty': -3.0}  │ 'Jingle bells, jingle bells, jingle all the way, jingle  │\n",
                            "│                       │                              │ bells, jingle bells, jingle, jingle, jingle, jingle,     │\n",
                            "│                       │                              │ jingle,'                                                 │\n",
                            "│                       │                              │                                                          │\n",
                            "│ Too hot!              │ {'temperature': 2.0}         │ 'Jingle bells, jingle bells, jingle all the way Around   │\n",
                            "│                       │                              │ brides vans Bills shel220 Seatamount by DESIG stabl WL   │\n",
                            "│                       │                              │ -------- (@tori6 Ideally Jan finished'                   │\n",
                            "│                       │                              │                                                          │\n",
                            "│ Pleasantly cool       │ {'temperature': 0.7}         │ \"Jingle bells, jingle bells, jingle all the way down.    │\n",
                            "│                       │                              │ It's not about the money, it's about the size of the     │\n",
                            "│                       │                              │ venue, the size of the room,\"                            │\n",
                            "│                       │                              │                                                          │\n",
                            "│ Pleasantly warm       │ {'temperature': 0.9}         │ \"Jingle bells, jingle bells, jingle all the way down to  │\n",
                            "│                       │                              │ my house. It's always going to be pretty loud, but       │\n",
                            "│                       │                              │ sometimes, once in awhile, I'll have\"                    │\n",
                            "│                       │                              │                                                          │\n",
                            "│ Too cold!             │ {'temperature': 0.01}        │ 'Jingle bells, jingle bells, jingle all the way up to    │\n",
                            "│                       │                              │ the top of the mountain.\\n\\nThe first time I saw the     │\n",
                            "│                       │                              │ mountain, I was in the middle of'                        │\n",
                            "│                       │                              │                                                          │\n",
                            "└───────────────────────┴──────────────────────────────┴──────────────────────────────────────────────────────────┘\n"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "sampler = TransformerSampler(model, tokenizer)\n",
                "\n",
                "N_RUNS = 1\n",
                "your_prompt = \"Jingle bells, jingle bells, jingle all the way\"\n",
                "cases = [\n",
                "    (\"High freq penalty\", dict(frequency_penalty=100.0)),\n",
                "    (\"Negative freq penalty\", dict(frequency_penalty=-3.0)),\n",
                "    (\"Too hot!\", dict(temperature=2.0)),\n",
                "    (\"Pleasantly cool\", dict(temperature=0.7)),\n",
                "    (\"Pleasantly warm\", dict(temperature=0.9)),\n",
                "    (\"Too cold!\", dict(temperature=0.01)),\n",
                "]\n",
                "\n",
                "table = Table(\"Name\", \"Kwargs\", \"Output\", title=\"Sampling - Manual Testing\")\n",
                "\n",
                "for name, kwargs in cases:\n",
                "    for i in range(N_RUNS):\n",
                "        output = sampler.sample(your_prompt, max_tokens_generated=24, **kwargs)\n",
                "        table.add_row(name, repr(kwargs), repr(output) + \"\\n\")\n",
                "\n",
                "rprint(table)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Top-K Sampling\n",
                "\n",
                "Conceptually, the steps in top-k sampling are:\n",
                "- Find the `top_k` largest probabilities (you can use [`torch.topk`](https://pytorch.org/docs/stable/generated/torch.topk.html))\n",
                "- Set all other probabilities to zero\n",
                "- Normalize and sample\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Exercise - implement `sample_top_k`\n",
                "\n",
                "```c\n",
                "Difficulty: 🟠🟠⚪⚪⚪\n",
                "Importance: 🟠⚪⚪⚪⚪\n",
                "\n",
                "You should spend up to 5-10 minutes on this exercise.\n",
                "```\n",
                "\n",
                "Implement the method `sample_top_k` now. Your implementation should stay in log-space throughout (don't exponentiate to obtain probabilities). This means you don't actually need to worry about normalizing, because `Categorical` accepts unnormalised logits.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 94,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "df343d2e88f24cba963815c8101fec9a",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "  0%|          | 0/10000 [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Word: ' church'. Expected freq = 0.4761, observed freq = 0.4700\n",
                        "Word: ' house' . Expected freq = 0.2697, observed freq = 0.2719\n",
                        "Word: ' temple'. Expected freq = 0.1065, observed freq = 0.1082\n",
                        "Word: ' same'  . Expected freq = 0.0764, observed freq = 0.0804\n",
                        "Word: ' Church'. Expected freq = 0.0713, observed freq = 0.0695\n"
                    ]
                }
            ],
            "source": [
                "prompt = \"John and Mary went to the\"\n",
                "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
                "logits = model(input_ids)[0, -1]\n",
                "\n",
                "expected_top_5 = {\n",
                "    \" church\": 0.0648,\n",
                "    \" house\": 0.0367,\n",
                "    \" temple\": 0.0145,\n",
                "    \" same\": 0.0104,\n",
                "    \" Church\": 0.0097,\n",
                "}\n",
                "topk_5_sum = sum(expected_top_5.values())\n",
                "\n",
                "observed_freqs = defaultdict(int)\n",
                "\n",
                "N = 10000\n",
                "for _ in tqdm(range(N)):\n",
                "    token = TransformerSampler.sample_next_token(input_ids.squeeze(), logits, top_k=5)\n",
                "    observed_freqs[tokenizer.decode(token)] += 1\n",
                "\n",
                "for word in expected_top_5:\n",
                "    expected_freq = expected_top_5[word] / topk_5_sum\n",
                "    observed_freq = observed_freqs[word] / N\n",
                "    print(\n",
                "        f\"Word: {word!r:<9}. Expected freq = {expected_freq:.4f}, observed freq = {observed_freq:.4f}\"\n",
                "    )\n",
                "    assert (\n",
                "        abs(observed_freq - expected_freq) < 0.015\n",
                "    ), \"Try increasing N if this fails by a small amount.\""
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<details>\n",
                "<summary>Solution</summary>\n",
                "\n",
                "```python\n",
                "def sample_top_k(logits: t.Tensor, top_k: int) -> int:\n",
                "    \"\"\"\n",
                "    logits: shape (vocab_size, ) - unnormalized log-probabilities\n",
                "    top_k: only consider this many of the most likely tokens for sampling\n",
                "\n",
                "    Return: a sampled token\n",
                "    \"\"\"\n",
                "    top_logits, top_idx = t.topk(logits, top_k)\n",
                "    idx = t.distributions.categorical.Categorical(logits=top_logits).sample()\n",
                "    return top_idx[idx].item()\n",
                "```\n",
                "</details>\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Top-K Sampling - Example\n",
                "\n",
                "The [GPT-2 paper](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) famously included an example prompt about unicorns. Now it's your turn to see just how cherry picked this example was.\n",
                "\n",
                "The paper claims they used `top_k=40` and best of 10 samples.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 46,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/html": [
                            "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Your model said:\n",
                            "\n",
                            "<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold\">In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in</span>\n",
                            "<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold\">the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.</span>\n",
                            "\n",
                            "<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold\">\"This discovery is a surprise to all of us who have been in the region for the past decade,\"</span><span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold\"> said Dr. Stephen J. </span>\n",
                            "<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold\">O'Dell, a University of Utah assistant professor of geology and biology. \"The new discovery is an important step </span>\n",
                            "<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold\">toward understanding whether the unicorns live in this region</span>\n",
                            "</pre>\n"
                        ],
                        "text/plain": [
                            "Your model said:\n",
                            "\n",
                            "\u001b[1;38;5;208mIn a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in\u001b[0m\n",
                            "\u001b[1;38;5;208mthe Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\u001b[0m\n",
                            "\n",
                            "\u001b[1;38;5;208m\"This discovery is a surprise to all of us who have been in the region for the past decade,\"\u001b[0m\u001b[1;38;5;208m said Dr. Stephen J. \u001b[0m\n",
                            "\u001b[1;38;5;208mO'Dell, a University of Utah assistant professor of geology and biology. \"The new discovery is an important step \u001b[0m\n",
                            "\u001b[1;38;5;208mtoward understanding whether the unicorns live in this region\u001b[0m\n"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "sampler = TransformerSampler(model, tokenizer)\n",
                "\n",
                "your_prompt = \"In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\"\n",
                "output = sampler.sample(your_prompt, temperature=0.7, top_k=40, max_tokens_generated=64)\n",
                "rprint(f\"Your model said:\\n\\n[bold dark_orange]{output}\")"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Top-p aka Nucleus Sampling\n",
                "\n",
                "The basic idea is that we choose the most likely words, up until the total probability of words we've chosen crosses some threshold. Then we sample from those chosen words based on their logits.\n",
                "\n",
                "The steps are:\n",
                "\n",
                "- Sort the probabilities from largest to smallest\n",
                "- Find the cutoff point where the cumulative probability first equals or exceeds `top_p`. We do the cutoff inclusively, keeping the first probability above the threshold.\n",
                "- If the number of kept probabilities is less than `min_tokens_to_keep`, keep that many tokens instead.\n",
                "- Set all other probabilities to zero\n",
                "- Normalize and sample\n",
                "\n",
                "Optionally, refer to the paper [The Curious Case of Neural Text Degeneration](https://arxiv.org/pdf/1904.09751.pdf) for some comparison of different methods.\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Exercise - implement `sample_top_p`\n",
                "\n",
                "```c\n",
                "Difficulty: 🟠🟠🟠⚪⚪\n",
                "Importance: 🟠⚪⚪⚪⚪\n",
                "\n",
                "You should spend up to 15-20 minutes on this exercise.\n",
                "```\n",
                "\n",
                "<details>\n",
                "<summary>Example of top-p sampling (if you're confused)</summary>\n",
                "\n",
                "If our probabilities were `(0.4, 0.3, 0.2, 0.1)` and our cutoff was `top_p=0.8`, then we'd sample from the first three elements (because their total probability is `0.9` which is over the threshold, but the first two only have a total prob of `0.7` which is under the threshold). Once we've chosen to sample from those three, we would renormalise them by dividing by their sum (so the probabilities we use when sampling are `(4/9, 3/9, 2/9)`.\n",
                "</details>\n",
                "\n",
                "<details>\n",
                "<summary>Help - I'm stuck on how to implement this function.</summary>\n",
                "\n",
                "First, sort the logits using the `sort(descending=True)` method (this returns values and indices). Then you can get `cumulative_probs` by applying softmax to these logits and taking the cumsum. Then, you can decide how many probabilities to keep by using the `t.searchsorted` function.\n",
                "    \n",
                "Once you've decided which probabilities to keep, it's easiest to sample from them using the original logits (you should have preserved the indices when you called `logits.sort`). This way, you don't need to worry about renormalising like you would if you were using probabilities.\n",
                "</details>\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 109,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "ca94203d4a79470688202a3122833ac1",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "  0%|          | 0/10100 [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Word: ' church'. Expected freq 0.6384, observed freq 0.6330\n",
                        "Word: ' house' . Expected freq 0.3616, observed freq 0.3670\n"
                    ]
                }
            ],
            "source": [
                "prompt = \"John and Mary went to the\"\n",
                "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
                "logits = model(input_ids)[0, -1]\n",
                "\n",
                "expected_top_10pct = {\n",
                "    \" church\": 0.0648,\n",
                "    \" house\": 0.0367,  # These are the two most likely tokens, and add up to >10%\n",
                "}\n",
                "top_10pct_sum = sum(expected_top_10pct.values())\n",
                "\n",
                "observed_freqs = defaultdict(int)\n",
                "\n",
                "N = 10100\n",
                "for _ in tqdm(range(N)):\n",
                "    token = TransformerSampler.sample_next_token(input_ids.squeeze(), logits, top_p=0.1)\n",
                "    observed_freqs[tokenizer.decode(token)] += 1\n",
                "\n",
                "for word in expected_top_10pct:\n",
                "    expected_freq = expected_top_10pct[word] / top_10pct_sum\n",
                "    observed_freq = observed_freqs[word] / N\n",
                "    print(\n",
                "        f\"Word: {word!r:<9}. Expected freq {expected_freq:.4f}, observed freq {observed_freq:.4f}\"\n",
                "    )\n",
                "    assert (\n",
                "        abs(observed_freq - expected_freq) < 0.01\n",
                "    ), \"Try increasing N if this fails by a small amount.\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 110,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "defaultdict(<class 'int'>, {' house': 3707, ' church': 6393})\n"
                    ]
                }
            ],
            "source": [
                "print(observed_freqs)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<details>\n",
                "<summary>Solution</summary>\n",
                "\n",
                "```python\n",
                "def sample_top_p(logits: t.Tensor, top_p: float, min_tokens_to_keep: int = 1) -> int:\n",
                "    \"\"\"\n",
                "    logits: shape (vocab_size, ) - unnormalized log-probabilities\n",
                "    Return: a sampled token\n",
                "    \"\"\"\n",
                "    logits_sorted, indices = logits.sort(descending=True, stable=True)\n",
                "    cumul_probs = logits_sorted.softmax(-1).cumsum(-1)\n",
                "    n_keep = t.searchsorted(cumul_probs, top_p, side=\"right\").item() + 1\n",
                "    n_keep = max(n_keep, min_tokens_to_keep)\n",
                "    keep_idx = indices[:n_keep]\n",
                "    keep_logits = logits[keep_idx]\n",
                "    sample = t.distributions.categorical.Categorical(logits=keep_logits).sample()\n",
                "    return keep_idx[sample].item()\n",
                "```\n",
                "</details>\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Top-p Sampling - Example\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 111,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/html": [
                            "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Your model said:\n",
                            "\n",
                            "<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold\">Eliezer Shlomo Yudkowsky (born September </span><span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold\">11</span><span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold\">, </span><span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold\">1979</span><span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold\">) is an American decision and artificial intelligence (AI) </span>\n",
                            "<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold\">theorist and writer, best known for his work on the theory of evolutionary psychology and for his work on the human</span>\n",
                            "<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold\">potential. He has published more than </span><span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold\">200</span><span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold\"> books, including the book, </span><span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold\">\"Beyond the Human Potential: A Theory of the </span>\n",
                            "<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold\">Human Potential\"</span><span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold\">, and the book, </span><span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold\">\"The Human Potential: The Machine and the Brain\"</span><span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold\">, which was published in </span><span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold\">2011</span><span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold\">. He</span>\n",
                            "</pre>\n"
                        ],
                        "text/plain": [
                            "Your model said:\n",
                            "\n",
                            "\u001b[1;38;5;208mEliezer Shlomo Yudkowsky \u001b[0m\u001b[1;38;5;208m(\u001b[0m\u001b[1;38;5;208mborn September \u001b[0m\u001b[1;38;5;208m11\u001b[0m\u001b[1;38;5;208m, \u001b[0m\u001b[1;38;5;208m1979\u001b[0m\u001b[1;38;5;208m)\u001b[0m\u001b[1;38;5;208m is an American decision and artificial intelligence \u001b[0m\u001b[1;38;5;208m(\u001b[0m\u001b[1;38;5;208mAI\u001b[0m\u001b[1;38;5;208m)\u001b[0m\u001b[1;38;5;208m \u001b[0m\n",
                            "\u001b[1;38;5;208mtheorist and writer, best known for his work on the theory of evolutionary psychology and for his work on the human\u001b[0m\n",
                            "\u001b[1;38;5;208mpotential. He has published more than \u001b[0m\u001b[1;38;5;208m200\u001b[0m\u001b[1;38;5;208m books, including the book, \u001b[0m\u001b[1;38;5;208m\"Beyond the Human Potential: A Theory of the \u001b[0m\n",
                            "\u001b[1;38;5;208mHuman Potential\"\u001b[0m\u001b[1;38;5;208m, and the book, \u001b[0m\u001b[1;38;5;208m\"The Human Potential: The Machine and the Brain\"\u001b[0m\u001b[1;38;5;208m, which was published in \u001b[0m\u001b[1;38;5;208m2011\u001b[0m\u001b[1;38;5;208m. He\u001b[0m\n"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "sampler = TransformerSampler(model, tokenizer)\n",
                "\n",
                "your_prompt = \"Eliezer Shlomo Yudkowsky (born September 11, 1979) is an American decision and artificial intelligence (AI) theorist and writer, best known for\"\n",
                "output = sampler.sample(your_prompt, temperature=0.7, top_p=0.95, max_tokens_generated=64)\n",
                "rprint(f\"Your model said:\\n\\n[bold dark_orange]{output}\")"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Beam search\n",
                "\n",
                "Finally, we'll implement a more advanced way of searching over output: **beam search**. You should read the [HuggingFace page](https://huggingface.co/blog/how-to-generate#beam-search) on beam search before moving on.\n",
                "\n",
                "In beam search, we maintain a list of size `num_beams` completions which are the most likely completions so far as measured by the product of their probabilities. Since this product can become very small, we use the sum of log probabilities instead. Note - log probabilities are *not* the same as your model's output. We get log probabilities by first taking softmax of our output and then taking log. You can do this with the [`log_softmax`](https://pytorch.org/docs/stable/generated/torch.nn.functional.log_softmax.html) function / tensor method.\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<details>\n",
                "<summary>Log probabilities are equal to the logit output after being translated by some amount X (where X is a function of the original logit output). Can you prove this?</summary>\n",
                "\n",
                "Suppose our vector of logits is $x$, and we take softmax to get a vector of probabilities $p$, then log again to get a vector of log probabilities $l$. Then the $i$-th element of this vector of logprobs is:\n",
                "\n",
                "$$\n",
                "\\begin{align}\n",
                "l_i &= \\log p_i \\\\\n",
                "&= \\log \\frac{\\exp(x_i)}{\\sum_j \\exp(x_j)} \\\\\n",
                "&= x_i - \\log \\sum_j \\exp(x_j) \\\\\n",
                "&= x_i - C\n",
                "\\end{align}\n",
                "$$\n",
                "\n",
                "where $C = \\log \\sum_j \\exp(x_j)$ is the same for all elements. So we can see that $l_i$ is equal to the logit output $x_i$ after being translated by $C$.\n",
                "\n",
                "It's important not to mix up logits and logprobs!\n",
                "</details>\n",
                "\n",
                "<details>\n",
                "<summary>Why do you think we use log softmax rather than logit output?</summary>\n",
                "\n",
                "Logit output is translation invariant. If we had two different beams and we were generating the next tokens in those beams, there would be no reasonable way to compare the two beams to each other, because we could shift the logit vector for one beam by a constant amount without changing the distribution.\n",
                "\n",
                "</details>\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "At each iteration, we run the batch of completions through the model and take the log-softmax to obtain `d_vocab` log-probs for each completion, or `num_beams * d_vocab` possible next completions in total.\n",
                "\n",
                "If we kept all of these, then we would have `num_beams * d_vocab * d_vocab` completions after the next iteration which is way too many, so instead we sort them by their score and loop through from best (highest) log probability to worst (lowest).\n",
                "\n",
                "The illustration below might help (based on real results from this method). Here, we have the following hyperparameters:\n",
                "\n",
                "```python\n",
                "num_beams = 3\n",
                "max_new_tokens = 3\n",
                "num_return_sequences = 2\n",
                "```\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<img src=\"https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/beam-search.png\" width=\"1000\">\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Note how after each \"generate\" stage, we have `num_beams ** 2` possible completions, which we then filter down to `num_beams`. Can you see why we need to generate this many (and what might happen if we generated fewer)?\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "How do we deal with sequences that terminate early (i.e. by generating an EOS token)? Answer - we append them to the list of completions which we'll return at the end, and remove them from the generation tree. Our algorithm terminates when either all our sequences have length `max_new_tokens` larger than the initial prompt length, or we've generated `num_returns_sequences` terminating sequences.\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Exercise - implement `beam_search`\n",
                "\n",
                "```c\n",
                "Difficulty: 🟠🟠🟠🟠⚪\n",
                "Importance: 🟠⚪⚪⚪⚪\n",
                "\n",
                "You should spend up to 30-40 minutes on this exercise.\n",
                "```\n",
                "\n",
                "You should now complete the `beam_search` method in your `TransformerSampler` class.\n",
                "\n",
                "We've provided one possible template for you to use: the class `Beams`, with important methods `generate` and `filter` for you to fit in (which correspond to the two stages in the diagram above). There are also a few of helper functions in this class:\n",
                "\n",
                "* `new_beams`, which creates a new `Beams` object from an old one.\n",
                "* `__getitem__`, which allows you to index into a `Beams` object to get a specific batch of beams.\n",
                "* `logprobs_and_completions`, which turns a `Beams` object into a list of (logprob sum, string completion) tuples (useful for getting your final output).\n",
                "* `print`, which prints out the current state of the beams (useful for debugging, if you run `beam_search` with `verbose=True`).\n",
                "\n",
                "You can then fill in the `beam_search` function, using this class and its methods.\n",
                "\n",
                "We've provided unit tests for the `generate` and `filter` functions, so you can verify that these are correct before moving on to the full `beam_search` function.\n",
                "\n",
                "**Note that using the `Beams` class is not strictly necessary, you could fill in the `beam_search` function directly if you prefer.** The `Beams` class is just meant to provide one example way you might implement this function. Often, modular code like this is easier to write and debug, and easier to extend to cover new use cases (e.g. when we use caching in the bonus exercises).\n",
                "\n",
                "#### Why all the n-gram repetition?\n",
                "\n",
                "You should observe that, while the output of beam search is sometimes more fluent than some of the other sampling methods you implement, it also has an unfortunate tendency to repeat sentences or sequences. This makes sense - if the model produces a sentence with a relatively high logit sum, then it will want to produce the same sentence again even if it doesn't make a lot of sense in context.\n",
                "\n",
                "A common solution is to ban repetition of n-grams. We've provided the argument `no_repeat_ngram_size` in the `generate` method for this purpose. Using this argument should prevent the model from repeating any n-grams of that size. Good values of this parameter to try are 2 or 3.\n",
                "\n",
                "However, first you should focus on getting a working version of beam search *without* using this argument.\n",
                "\n",
                "<details>\n",
                "<summary>Hint (for <code>no_repeat_ngram_size</code>)</summary>\n",
                "\n",
                "It might be helpful to implement the following method first. You can use this rather than `torch.topk` in your `generate` method.\n",
                "\n",
                "```python\n",
                "def get_topk_non_repeating(\n",
                "    self,\n",
                "    logprobs: Float[Tensor, \"batch d_vocab\"],\n",
                "    no_repeat_ngram_size: int,\n",
                "    k: int,\n",
                ") -> Tuple[Float[Tensor, \"k\"], Int[Tensor, \"k\"]]:\n",
                "    \"\"\"\n",
                "    logprobs:\n",
                "        tensor of the log-probs for the next token\n",
                "    no_repeat_ngram_size:\n",
                "        size of ngram to avoid repeating\n",
                "    k:\n",
                "        number of top logits to return, for each beam in our collection\n",
                "\n",
                "    Returns:\n",
                "        equivalent to the output of `logprobs.topk(dim=-1)`, but makes sure\n",
                "        that no returned tokens would produce an ngram of size  `no_repeat_ngram_size`\n",
                "        which has already appeared in `self.tokens`.\n",
                "    \"\"\"\n",
                "    pass\n",
                "```\n",
                "</details>\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "@dataclass\n",
                "class Beams:\n",
                "    \"\"\"Class to store beams during beam search.\"\"\"\n",
                "\n",
                "    model: DemoTransformer\n",
                "    tokenizer: GPT2TokenizerFast\n",
                "    logprob_sums: Float[Tensor, \"batch\"]\n",
                "    tokens: Int[Tensor, \"batch seq\"]\n",
                "\n",
                "    def new_beams(self, logprob_sums, tokens) -> \"Beams\":\n",
                "        \"\"\"Creates a new Beams object with the same model and tokenizer.\"\"\"\n",
                "        return Beams(self.model, self.tokenizer, logprob_sums, tokens)\n",
                "\n",
                "    def __getitem__(self, idx) -> \"Beams\":\n",
                "        \"\"\"Allows you to take a slice of the beams object along the batch dimension.\"\"\"\n",
                "        return self.new_beams(self.logprob_sums[idx], self.tokens[idx])\n",
                "\n",
                "    @property\n",
                "    def logprobs_and_completions(self) -> List[Tuple[float, str]]:\n",
                "        \"\"\"Returns self as a list of logprob sums and completions (useful for getting final output).\"\"\"\n",
                "        return [\n",
                "            (logprob_sum.item(), self.tokenizer.decode(tokens))\n",
                "            for (logprob_sum, tokens) in zip(self.logprob_sums, self.tokens)\n",
                "        ]\n",
                "\n",
                "    def generate(self, toks_per_beam: int, no_repeat_ngram_size: Optional[int] = None) -> \"Beams\":\n",
                "        \"\"\"\n",
                "        Starting from the current set of beams (which has length `num_beams`), returns a new\n",
                "        set of `num_beams * toks_per_beam`, containing the best `toks_per_beam` continuations for each\n",
                "        of the original beams.\n",
                "\n",
                "        Optional argument `no_repeat_ngram_size` means your model won't generate any sequences with\n",
                "        a repeating n-gram of this length.\n",
                "        \"\"\"\n",
                "        pass\n",
                "\n",
                "    def filter(self, num_beams: int) -> Tuple[\"Beams\", \"Beams\"]:\n",
                "        \"\"\"\n",
                "        Returns:\n",
                "            best_beams: Beams\n",
                "                filtered version of self, containing all best `num_beams` which are also not terminated.\n",
                "\n",
                "            early_terminations: Beams\n",
                "                filtered version of self, containing all best `num_beams` which are also terminated.\n",
                "                i.e. the sum of lengths of these two should equal `num_beams`.\n",
                "        \"\"\"\n",
                "        pass\n",
                "\n",
                "    def print(self, title=\"Best completions\", max_print_chars=80) -> None:\n",
                "        \"\"\"\n",
                "        Prints out a set of sequences with their corresponding logitsums.\n",
                "        \"\"\"\n",
                "        if len(self.tokens) == 0:\n",
                "            return\n",
                "        table = Table(\"logitsum\", \"completion\", title=title)\n",
                "        for logprob_sum, tokens in zip(self.logprob_sums, self.tokens):\n",
                "            text = self.tokenizer.decode(tokens)\n",
                "            if len(repr(text)) > max_print_chars:\n",
                "                text = (\n",
                "                    text[: int(0.3 * max_print_chars)]\n",
                "                    + \" ... \"\n",
                "                    + text[-int(0.7 * max_print_chars) :]\n",
                "                )\n",
                "            table.add_row(f\"{logprob_sum:>8.3f}\", repr(text))\n",
                "        rprint(table)\n",
                "\n",
                "    def get_topk_non_repeating(\n",
                "        self,\n",
                "        logprobs: Float[Tensor, \"batch d_vocab\"],\n",
                "        no_repeat_ngram_size: Optional[int],\n",
                "        k: int,\n",
                "    ) -> Tuple[Float[Tensor, \"k\"], Int[Tensor, \"k\"]]:\n",
                "        \"\"\"\n",
                "        logprobs:\n",
                "            tensor of the log-probs for the next token\n",
                "        no_repeat_ngram_size:\n",
                "            size of ngram to avoid repeating\n",
                "        k:\n",
                "            number of top logits to return, for each beam in our collection\n",
                "\n",
                "        Returns:\n",
                "            equivalent to the output of `logprobs.topk(dim=-1)`, but makes sure\n",
                "            that no returned tokens would produce an ngram of size  `no_repeat_ngram_size`\n",
                "            which has already appeared in `self.tokens`.\n",
                "        \"\"\"\n",
                "        batch, seq_len = self.tokens.shape\n",
                "        neg_inf = torch.tensor(-1.0e4).to(device)\n",
                "\n",
                "        # If completion isn't long enough for a repetition, or we have no restructions, just return topk\n",
                "        if (no_repeat_ngram_size is not None) and (seq_len > no_repeat_ngram_size - 1):\n",
                "            # Otherwise, we need to check for ngram repetitions\n",
                "            # First, get the most recent `no_repeat_ngram_size-1` tokens\n",
                "            last_ngram_prefix = self.tokens[:, seq_len - (no_repeat_ngram_size - 1) :]\n",
                "            # Next, find all the tokens we're not allowed to generate (by going iterating through past ngrams and seeing if those ngram prefixes match the last one)\n",
                "            for i in range(seq_len - (no_repeat_ngram_size - 1)):\n",
                "                ngrams = self.tokens[:, i : i + no_repeat_ngram_size]  # (batch, ngram)\n",
                "                ngrams_are_repeated = (ngrams[:, :-1] == last_ngram_prefix).all(-1)  # (batch,)\n",
                "                ngram_end_tokens = ngrams[:, [-1]]  # (batch, 1)\n",
                "                # Fill logprobs with neginf wherever the ngrams are repeated\n",
                "                logprobs[range(batch), ngram_end_tokens] = torch.where(\n",
                "                    ngrams_are_repeated,\n",
                "                    neg_inf,\n",
                "                    logprobs[range(batch), ngram_end_tokens],\n",
                "                )\n",
                "\n",
                "        # Finally, get our actual tokens\n",
                "        return logprobs.topk(k=k, dim=-1)\n",
                "\n",
                "\n",
                "@torch.inference_mode()\n",
                "def beam_search(\n",
                "    self: TransformerSampler,\n",
                "    prompt: str,\n",
                "    num_return_sequences: int,\n",
                "    num_beams: int,\n",
                "    max_new_tokens: int,\n",
                "    no_repeat_ngram_size: Optional[int] = None,\n",
                "    verbose=False,\n",
                ") -> List[Tuple[float, Tensor]]:\n",
                "    \"\"\"\n",
                "    Implements a beam search, by repeatedly performing the `generate` and `filter` steps (starting\n",
                "    from the initial prompt) until either of the two stopping criteria are met:\n",
                "\n",
                "        (1) we've generated `max_new_tokens` tokens, or\n",
                "        (2) we've generated `num_returns_sequences` terminating sequences.\n",
                "\n",
                "    To modularize this function, most of the actual complexity is in the Beams class,\n",
                "    in the `generate` and `filter` methods.\n",
                "    \"\"\"\n",
                "\n",
                "    assert num_return_sequences <= num_beams\n",
                "    self.model.eval()\n",
                "\n",
                "    ..."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Example usage of the `Beams` class, and the `print` method (not the logitsums aren't necessarily accurate, this example is just an illustration):\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "beams = Beams(\n",
                "    model,\n",
                "    tokenizer,\n",
                "    logprob_sums=torch.tensor([-10.0, -15.0, -20.0]).to(device),\n",
                "    tokens=torch.tensor(\n",
                "        [\n",
                "            [5661, 318, 262, 2368],\n",
                "            [5661, 318, 262, 1218],\n",
                "            [5661, 318, 262, 717],\n",
                "        ]\n",
                "    ).to(device),\n",
                ")\n",
                "\n",
                "beams.print()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "And here are some unit tests for your `generate` and `filter` methods:\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"Testing generate, without no_repeat_ngram_size argument:\")\n",
                "new_beams = beams.generate(toks_per_beam=2)\n",
                "new_beams.print()\n",
                "assert new_beams.logprobs_and_completions[0][1] == \"this is the third time\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"Testing generate, with no_repeat_ngram_size argument:\")\n",
                "\n",
                "bigram_beams = Beams(\n",
                "    model,\n",
                "    tokenizer,\n",
                "    logprob_sums=torch.tensor([-0.0]).to(device),\n",
                "    tokens=torch.tensor([[530, 734, 530, 734]]).to(device),\n",
                "    # tokens are \" one two one two\"\n",
                ")\n",
                "\n",
                "# With no_repeat_ngram_size=1, should not generate the token \" one\" or \" two\"\n",
                "new_bigram_beams = bigram_beams.generate(toks_per_beam=3, no_repeat_ngram_size=1)\n",
                "new_bigram_beams.print()\n",
                "assert all(\n",
                "    [\n",
                "        not (completion[1].endswith(\" one\") or completion[1].endswith(\" two\"))\n",
                "        for completion in new_bigram_beams.logprobs_and_completions\n",
                "    ]\n",
                ")\n",
                "\n",
                "# With no_repeat_ngram_size=2, it can generate \" two\" (which it should), but not \" one\"\n",
                "new_bigram_beams = bigram_beams.generate(toks_per_beam=3, no_repeat_ngram_size=2)\n",
                "new_bigram_beams.print()\n",
                "assert all(\n",
                "    [not completion[1].endswith(\" one\") for completion in new_bigram_beams.logprobs_and_completions]\n",
                ")\n",
                "assert any(\n",
                "    [not completion[1].endswith(\" two\") for completion in new_bigram_beams.logprobs_and_completions]\n",
                ")\n",
                "\n",
                "print(\"All tests for `generate` passed!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "logprob_sums = torch.tensor([-1.0, -2.0]).to(device)\n",
                "tokens = torch.tensor([[19485, 13], [19485, tokenizer.eos_token_id]]).to(device)\n",
                "\n",
                "beams_with_eos = Beams(model, tokenizer, logprob_sums, tokens)\n",
                "best_beams, early_terminations = beams_with_eos.filter(2)\n",
                "\n",
                "torch.testing.assert_close(best_beams.logprob_sums, logprob_sums[[0]])\n",
                "torch.testing.assert_close(best_beams.tokens, tokens[[0]])\n",
                "\n",
                "assert early_terminations.logprobs_and_completions == [(-2.0, \"Stop\" + tokenizer.eos_token)]\n",
                "\n",
                "print(\"All tests for `filter` passed!\")"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<details>\n",
                "<summary>Solutions (for <code>generate</code> and <code>filter</code>)</summary>\n",
                "\n",
                "```python\n",
                "def generate(self, toks_per_beam: int, no_repeat_ngram_size: Optional[int] = None) -> \"Beams\":\n",
                "    \"\"\"\n",
                "    Starting from the current set of beams (which has length `num_beams`), returns a new\n",
                "    set of `num_beams * toks_per_beam`, containing the best `toks_per_beam` continuations for each\n",
                "    of the original beams.\n",
                "\n",
                "    Optional argument `no_repeat_ngram_size` means your model won't generate any sequences with\n",
                "    a repeating n-gram of this length (don't worry about implementing this until later).\n",
                "    \"\"\"\n",
                "    # SOLUTION\n",
                "\n",
                "    # Get the output logprobs for the next token (for every sequence in current beams)\n",
                "    logprobs: Tensor = self.model(self.tokens)[:, -1, :].log_softmax(-1)\n",
                "\n",
                "    # Get the top `toks_per_beam` tokens for each sequence\n",
                "    topk_logprobs, topk_tokenIDs = logprobs.topk(k=toks_per_beam)\n",
                "\n",
                "    # Get all of the new possible beams, via einops operations\n",
                "    #   Here, we're effectively flattening out the batch dimension and k dimension, to give us tensors\n",
                "    #   with every possible combination of (original sequence, new token) pairs.)\n",
                "    new_logprob_sums = sum([\n",
                "        einops.repeat(self.logprob_sums, \"batch -> batch k\", k=toks_per_beam),\n",
                "        einops.rearrange(topk_logprobs, \"batch k -> (batch k)\")\n",
                "    ])\n",
                "    new_tokens = t.concat([\n",
                "        einops.repeat(self.tokens, \"batch seq -> (batch k) seq\", k=toks_per_beam),\n",
                "        einops.rearrange(topk_tokenIDs, \"batch k -> (batch k) 1\")\n",
                "    ], dim=-1)\n",
                "    return self.new_beams(new_logprob_sums, new_tokens)\n",
                "\n",
                "\n",
                "def filter(self, num_beams: int) -> Tuple[\"Beams\", \"Beams\"]:\n",
                "    \"\"\"\n",
                "    Returns:\n",
                "        best_beams: Beams\n",
                "            filtered version of self, containing all best `num_beams` which are also not terminated.\n",
                "\n",
                "        early_terminations: Beams\n",
                "            filtered version of self, containing all best `num_beams` which are also terminated.\n",
                "            i.e. the sum of lengths of these two should equal `num_beams`.\n",
                "    \"\"\"\n",
                "    # SOLUTION\n",
                "\n",
                "    # Get the indices of top `num_beams` beams\n",
                "    top_beam_indices = self.logprob_sums.topk(k=num_beams, dim=0).indices.tolist()\n",
                "    # Get the indices of terminated sequences\n",
                "    new_tokens = self.tokens[:, -1]\n",
                "    terminated_indices = t.nonzero(new_tokens == self.tokenizer.eos_token_id)\n",
                "\n",
                "    # Get the indices of the `num_beams` best sequences (some terminated, some not terminated)\n",
                "    best_continuing = [i for i in top_beam_indices if i not in terminated_indices]\n",
                "    best_terminated = [i for i in top_beam_indices if i in terminated_indices]\n",
                "\n",
                "    # Return the beam objects from these indices\n",
                "    best_beams_continuing = self.new_beams(self.logprob_sums[best_continuing], self.tokens[best_continuing])\n",
                "    best_beams_terminated = self.new_beams(self.logprob_sums[best_terminated], self.tokens[best_terminated])\n",
                "    return best_beams_continuing, best_beams_terminated\n",
                "```\n",
                "\n",
                "</details>\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Once you've passed both these unit tests, you can try implementing the full beam search function. It should create a `Beams` object from the initial prompt, and then repeatedly call `generate` and `filter` until the stopping criteria are met.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "TransformerSampler.beam_search = beam_search\n",
                "\n",
                "sampler = TransformerSampler(model, tokenizer)\n",
                "\n",
                "prompt = \"The ships hung in the sky in much the same way that\"\n",
                "orig_len = len(tokenizer.encode(prompt))\n",
                "\n",
                "final_logitsums_and_completions = sampler.beam_search(\n",
                "    prompt=prompt,\n",
                "    num_return_sequences=3,\n",
                "    num_beams=40,\n",
                "    max_new_tokens=60,\n",
                "    no_repeat_ngram_size=2,\n",
                "    verbose=False,\n",
                ")\n",
                "\n",
                "# Print all the best output\n",
                "for logprob_sum, text in final_logitsums_and_completions:\n",
                "    avg_logprob_as_prob = (\n",
                "        torch.tensor(logprob_sum / (len(tokenizer.encode(text)) - orig_len)).exp().item()\n",
                "    )\n",
                "    print(\"=\" * 25 + f\" Avg logprob (as probability) = {avg_logprob_as_prob:.3f} \" + \"=\" * 25)\n",
                "    rprint(\"Best output:\\n\\n[bold dark_orange]\" + text)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<details>\n",
                "<summary>Solution (full)</summary>\n",
                "\n",
                "A solution for the class method `get_topk_non_repeating`:\n",
                "\n",
                "```python\n",
                "def get_topk_non_repeating(\n",
                "    self,\n",
                "    logprobs: Float[Tensor, \"batch d_vocab\"],\n",
                "    no_repeat_ngram_size: Optional[int],\n",
                "    k: int,\n",
                ") -> Tuple[Float[Tensor, \"k\"], Int[Tensor, \"k\"]]:\n",
                "    \"\"\"\n",
                "    logprobs:\n",
                "        tensor of the log-probs for the next token\n",
                "    no_repeat_ngram_size:\n",
                "        size of ngram to avoid repeating\n",
                "    k:\n",
                "        number of top logits to return, for each beam in our collection\n",
                "\n",
                "    Returns:\n",
                "        equivalent to the output of `logprobs.topk(dim=-1)`, but makes sure\n",
                "        that no returned tokens would produce an ngram of size  `no_repeat_ngram_size`\n",
                "        which has already appeared in `self.tokens`.\n",
                "    \"\"\"\n",
                "    batch, seq_len = self.tokens.shape\n",
                "    neg_inf = t.tensor(-1.0e4).to(device)\n",
                "\n",
                "    # If completion isn't long enough for a repetition, or we have no restructions, just return topk\n",
                "    if (no_repeat_ngram_size is not None) and (seq_len > no_repeat_ngram_size-1):\n",
                "        # Otherwise, we need to check for ngram repetitions\n",
                "        # First, get the most recent `no_repeat_ngram_size-1` tokens\n",
                "        last_ngram_prefix = self.tokens[:, seq_len - (no_repeat_ngram_size-1):]\n",
                "        # Next, find all the tokens we're not allowed to generate (by going iterating through past ngrams and seeing if those ngram prefixes match the last one)\n",
                "        for i in range(seq_len - (no_repeat_ngram_size-1)):\n",
                "            ngrams = self.tokens[:, i:i+no_repeat_ngram_size] # (batch, ngram)\n",
                "            ngrams_are_repeated = (ngrams[:, :-1] == last_ngram_prefix).all(-1) # (batch,)\n",
                "            ngram_end_tokens = ngrams[:, [-1]] # (batch, 1)\n",
                "            # Fill logprobs with neginf wherever the ngrams are repeated\n",
                "            logprobs[range(batch), ngram_end_tokens] = t.where(\n",
                "                ngrams_are_repeated,\n",
                "                neg_inf,\n",
                "                logprobs[range(batch), ngram_end_tokens],\n",
                "        )\n",
                "\n",
                "    # Finally, get our actual tokens\n",
                "    return logprobs.topk(k=k, dim=-1)\n",
                "```\n",
                "\n",
                "and for the main function:\n",
                "\n",
                "```python\n",
                "@t.inference_mode()\n",
                "def beam_search(\n",
                "    self: TransformerSampler,\n",
                "    prompt: str,\n",
                "    num_return_sequences: int,\n",
                "    num_beams: int,\n",
                "    max_new_tokens: int,\n",
                "    no_repeat_ngram_size: Optional[int] = None,\n",
                "    verbose=False\n",
                ") -> List[Tuple[float, Tensor]]:\n",
                "    \"\"\"\n",
                "    Implements a beam search, by repeatedly performing the `generate` and `filter` steps (starting\n",
                "    from the initial prompt) until either of the two stopping criteria are met:\n",
                "\n",
                "        (1) we've generated `max_new_tokens` tokens, or\n",
                "        (2) we've generated `num_returns_sequences` terminating sequences.\n",
                "\n",
                "    To modularize this function, most of the actual complexity is in the Beams class,\n",
                "    in the `generate` and `filter` methods.\n",
                "    \"\"\"\n",
                "\n",
                "    assert num_return_sequences <= num_beams\n",
                "    self.model.eval()\n",
                "\n",
                "    # SOLUTION\n",
                "    tokens = self.tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
                "\n",
                "    # List for final beams to return (and early terminations)\n",
                "    final_logprobs_and_completions: List[Tuple[float, str]] = []\n",
                "    # Keep track of all best beams after each step\n",
                "    best_beams = Beams(self.model, self.tokenizer, t.tensor([0.0]).to(device), tokens)\n",
                "\n",
                "    for n in tqdm(range(max_new_tokens)):\n",
                "\n",
                "        # Generation step\n",
                "        best_beams = best_beams.generate(toks_per_beam=num_beams, no_repeat_ngram_size=no_repeat_ngram_size)\n",
                "\n",
                "        # Filtering step\n",
                "        best_beams, best_beams_terminated = best_beams.filter(num_beams=num_beams)\n",
                "        final_logprobs_and_completions.extend(best_beams_terminated.logprobs_and_completions)\n",
                "\n",
                "        # Print output\n",
                "        if verbose:\n",
                "            best_beams.print()\n",
                "\n",
                "        # Check stopping condition\n",
                "        if len(final_logprobs_and_completions) >= num_return_sequences:\n",
                "            return final_logprobs_and_completions[:num_return_sequences]\n",
                "\n",
                "    final_logprobs_and_completions.extend(best_beams.logprobs_and_completions)\n",
                "    final_logprobs_and_completions = final_logprobs_and_completions[:num_return_sequences]\n",
                "    return final_logprobs_and_completions\n",
                "```\n",
                "\n",
                "</details>\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Caching\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "*This section is also designed to be challenging, and take quite some time. There are many different ways to solve it, and you're expected to try and find your own way (you should think about this for a while before looking at the suggestions in the dropdowns). Additionally, you might not find it as interesting as some of the other sections. In this case, and if you have a lot of extra time, you might want to start on the \"building BERT\" exercises from this chapter.*\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### How can caching help us?\n",
                "\n",
                "The text generation we've done so far is needlessly re-computing certain values, which is very noticeable when you try to generate longer sequences.\n",
                "\n",
                "Suppose you're generating text, and you've already run GPT on the sentence \"My life motto:\". Now you want to run the model on the sentence \"My life motto: Always\". Which computations from the first sentence can you reuse?\n",
                "\n",
                "<details>\n",
                "<summary>Answer</summary>\n",
                "\n",
                "At each attention layer, the only things the attention layer needs from the previous sequence positions are the key and value vectors. This is explained in the following diagram:\n",
                "\n",
                "<img src=\"https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/tl-cache.png\" width=\"600\">\n",
                "\n",
                "</details>\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Exercise - implement caching\n",
                "\n",
                "```c\n",
                "Difficulty: 🟠🟠🟠🟠🟠\n",
                "Importance: 🟠⚪⚪⚪⚪\n",
                "\n",
                "You are expected to spend well over an hour on this exercise, if you choose to do it.\n",
                "```\n",
                "\n",
                "Modify your GPT-2 to optionally use a cache. When you run your GPT on `\"My life motto:\"`, it should store the necessary values in the cache. Then in the next forward pass with just `\" Always\"` as input, it should load the cached values instead of recomputing them (and update the cache). This only needs to work with a single input sequence (batch size of 1), and you can assume that after the first forward pass, the input will be just one token.\n",
                "\n",
                "The design of the cache is completely up to you - discuss possible designs with your partner before writing code. It should be possible to have only one GPT2 instance and many different cache instances at one time. Imagine that you want to use one instance to serve multiple users submitting requests for text generation like in [AI Dungeon](https://aidungeon.io/).\n",
                "\n",
                "You'll also need to rewrite parts of your `DemoTransformer` code, in order to get this to work. The tests have been built to accommodate modules which return their output as the first element in a tuple (i.e. `(output, cache)`) rather than just returning the output, so you should use the tests to verify that your modules still work as expected.\n",
                "\n",
                "Some example considerations:\n",
                "\n",
                "* Which GPT-2 classes need to interact with the cache?\n",
                "    * Will you need to change the positional embedding, and if so then how?\n",
                "* Should the cache be mutable and be updated in place, or does updating actually just create a separate instance?\n",
                "    * *(Hint here - think about how you might use the cache during beam search.)*\n",
                "* Is it possible for other programmers to incorrectly use your cache? Is there a way to prevent this failure mode or at least detect this and complain loudly?\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<details>\n",
                "<summary>Cache implentation (example)</summary>\n",
                "\n",
                "This KeyValueCache object is structured as just a fancy tensor (it inherits all the methods from Tensor). The main difference is that it has a few extra helper methods, e.g. constructing an empty cache from a Config object.\n",
                "\n",
                "There are other ways you could do this, e.g. having your `KeyValueCache` class contain list of `KeyValueCacheEntry` objects (where each of these corresponds to a different layer).\n",
                "\n",
                "```python\n",
                "# Define a type for a single layer's cache entry (useful for type checking in later functions)\n",
                "KeyValueCacheTensor = Float[Tensor, \"2 batch seq_len n_heads d_head\"]\n",
                "\n",
                "class KeyValueCache(Tensor):\n",
                "    \"\"\"\n",
                "    This class holds tensors of key and value vectors, to be used for caching.\n",
                "\n",
                "    If we define it using cfg and batch then it's initialized as empty, but\n",
                "    we can also define it from kv_cache_entries.\n",
                "    \"\"\"\n",
                "    @classmethod\n",
                "    def new_empty(cls, cfg: Config, batch: int = 1) -> \"KeyValueCache\":\n",
                "        \"\"\"\n",
                "        Doing a forward pass on a cache created in this way indicates \"we don't\n",
                "        yet have a cache, but we want this forward pass to return a cache\".\n",
                "        Whereas using cache=None in a forward pass indicates we don't want to\n",
                "        return a cache.\n",
                "        \"\"\"\n",
                "        shape = (cfg.n_layers, 2, batch, 0, cfg.n_heads, cfg.d_head)\n",
                "        return cls(*shape).to(device)\n",
                "\n",
                "    # Define a handful of properties, so they can be referenced directly rather than\n",
                "    # indexing (which is more likely to lead to mistakes)\n",
                "\n",
                "    @property\n",
                "    def k(self) -> Tensor:\n",
                "        return self[:, 0]\n",
                "\n",
                "    @property\n",
                "    def v(self) -> Tensor:\n",
                "        return self[:, 1]\n",
                "\n",
                "    @property\n",
                "    def batch(self) -> int:\n",
                "        return self.shape[2]\n",
                "    \n",
                "    @property\n",
                "    def seq_len(self) -> int:\n",
                "        return self.shape[3]\n",
                "\n",
                "\n",
                "# Example implementation:\n",
                "cfg = model.cfg\n",
                "batch = 6\n",
                "kv_cache = KeyValueCache.new_empty(cfg, batch)\n",
                "\n",
                "print(f\"Shape of all kv-cache = {tuple(kv_cache.shape)}\")\n",
                "print(f\"Shape of just k-cache = {tuple(kv_cache.k.shape)}\")\n",
                "for kv_cache_entry in kv_cache:\n",
                "    print(f\"Shape of cache entry for one layer = {tuple(kv_cache_entry.shape)}\")\n",
                "    break\n",
                "print(f\"Batch size = {kv_cache.batch}\")\n",
                "print(f\"Current sequence length = {kv_cache.seq_len}\")\n",
                "```\n",
                "\n",
                "</details>\n",
                "\n",
                "<details>\n",
                "<summary>New <code>DemoTransformer</code> components (and testing)</summary>\n",
                "\n",
                "```python\n",
                "# Define new model parts where necessary, and create a new model & test it\n",
                "# Note that sometimes our modules return a tuple of (tensor output, cache) rather than just output. The\n",
                "# tests have been built to accommodate this.\n",
                "\n",
                "\n",
                "class PosEmbed(nn.Module):\n",
                "    def __init__(self, cfg: Config):\n",
                "        super().__init__()\n",
                "        self.cfg = cfg\n",
                "        self.W_pos = nn.Parameter(t.empty((cfg.n_ctx, cfg.d_model)))\n",
                "        nn.init.normal_(self.W_pos, std=self.cfg.init_range)\n",
                "\n",
                "    def forward(\n",
                "        self,\n",
                "        tokens: Int[Tensor, \"batch position\"],\n",
                "        past_kv_pos_offset: int = 0\n",
                "    ) -> Float[Tensor, \"batch position d_model\"]:\n",
                "        \n",
                "        # SOLUTION\n",
                "        batch, seq_len = tokens.shape\n",
                "        return einops.repeat(\n",
                "            self.W_pos[past_kv_pos_offset: seq_len+past_kv_pos_offset],\n",
                "            \"seq d_model -> batch seq d_model\",\n",
                "            batch=batch\n",
                "        )\n",
                "\n",
                "\n",
                "class Attention(nn.Module):\n",
                "    IGNORE: Float[Tensor, \"\"]\n",
                "\n",
                "    def __init__(self, cfg: Config):\n",
                "        super().__init__()\n",
                "        self.cfg = cfg\n",
                "        self.W_Q = nn.Parameter(t.empty((cfg.n_heads, cfg.d_model, cfg.d_head)))\n",
                "        self.W_K = nn.Parameter(t.empty((cfg.n_heads, cfg.d_model, cfg.d_head)))\n",
                "        self.W_V = nn.Parameter(t.empty((cfg.n_heads, cfg.d_model, cfg.d_head)))\n",
                "        self.W_O = nn.Parameter(t.empty((cfg.n_heads, cfg.d_head, cfg.d_model)))\n",
                "        self.b_Q = nn.Parameter(t.zeros((cfg.n_heads, cfg.d_head)))\n",
                "        self.b_K = nn.Parameter(t.zeros((cfg.n_heads, cfg.d_head)))\n",
                "        self.b_V = nn.Parameter(t.zeros((cfg.n_heads, cfg.d_head)))\n",
                "        self.b_O = nn.Parameter(t.zeros((cfg.d_model)))\n",
                "        nn.init.normal_(self.W_Q, std=self.cfg.init_range)\n",
                "        nn.init.normal_(self.W_K, std=self.cfg.init_range)\n",
                "        nn.init.normal_(self.W_V, std=self.cfg.init_range)\n",
                "        nn.init.normal_(self.W_O, std=self.cfg.init_range)\n",
                "        self.register_buffer(\"IGNORE\", t.tensor(-1e5, dtype=t.float32, device=device))\n",
                "\n",
                "    def forward(\n",
                "        self,\n",
                "        normalized_resid_pre: Float[Tensor, \"batch posn d_model\"],\n",
                "        kv_cache_entry: Optional[KeyValueCacheTensor] = None,\n",
                "    ) -> Tuple[\n",
                "        Float[Tensor, \"batch posn d_model\"],\n",
                "        Optional[KeyValueCacheTensor]\n",
                "    ]:\n",
                "        \"\"\"\n",
                "        Returns the result of applying attention layer to normlized_resid_pre, as well as\n",
                "        the new cached key and value vectors (which we get from concatenating the old cached\n",
                "        ones with the new key and value vectors).\n",
                "        \"\"\"\n",
                "        \n",
                "        # SOLUTION\n",
                "\n",
                "        # Calculate the new query, key and value vectors\n",
                "        q = einops.einsum(\n",
                "            normalized_resid_pre, self.W_Q,\n",
                "            \"batch posn d_model, nheads d_model d_head -> batch posn nheads d_head\"\n",
                "        ) + self.b_Q\n",
                "        k = einops.einsum(\n",
                "            normalized_resid_pre, self.W_K,\n",
                "            \"batch posn d_model, nheads d_model d_head -> batch posn nheads d_head\"\n",
                "        ) + self.b_K\n",
                "        v = einops.einsum(\n",
                "            normalized_resid_pre, self.W_V,\n",
                "            \"batch posn d_model, nheads d_model d_head -> batch posn nheads d_head\"\n",
                "        ) + self.b_V\n",
                "\n",
                "        # If cache_entry is not None, this means we use the previous key and value vectors\n",
                "        # Also we'll need to get a new cache entry which will be used later to construct a new cache\n",
                "        if kv_cache_entry is not None:\n",
                "            k = t.concat([kv_cache_entry[0], k], dim=1)\n",
                "            v = t.concat([kv_cache_entry[1], v], dim=1)\n",
                "            kv_cache_entry = t.stack([k, v])\n",
                "\n",
                "        # Calculate attention scores, then scale and mask, and apply softmax to get probabilities\n",
                "        attn_scores = einops.einsum(\n",
                "            q, k,\n",
                "            \"batch posn_Q nheads d_head, batch posn_K nheads d_head -> batch nheads posn_Q posn_K\"\n",
                "        )\n",
                "        attn_scores_masked = self.apply_causal_mask(attn_scores / self.cfg.d_head ** 0.5)\n",
                "        attn_pattern = attn_scores_masked.softmax(-1)\n",
                "\n",
                "        # Take weighted sum of value vectors, according to attention probabilities\n",
                "        z = einops.einsum(\n",
                "            v, attn_pattern,\n",
                "            \"batch posn_K nheads d_head, batch nheads posn_Q posn_K -> batch posn_Q nheads d_head\"\n",
                "        )\n",
                "\n",
                "        # Calculate output (by applying matrix W_O and summing over heads, then adding bias b_O)\n",
                "        out = einops.einsum(\n",
                "            z, self.W_O,\n",
                "            \"batch posn_Q nheads d_head, nheads d_head d_model -> batch posn_Q d_model\"\n",
                "        ) + self.b_O\n",
                "\n",
                "        return out, kv_cache_entry\n",
                "\n",
                "    def apply_causal_mask(\n",
                "        self, attn_scores: Float[Tensor, \"batch n_heads query_pos key_pos\"]\n",
                "    ) -> Float[Tensor, \"batch n_heads query_pos key_pos\"]:\n",
                "        \"\"\"\n",
                "        Here, attn_scores have shape (batch, n_heads, query_pos, key_pos), where query_pos represents the\n",
                "        new (non-cached) positions, and key_pos represent all the positions (cached and non-cached).\n",
                "\n",
                "        So when we create our mask, the query indices and key indices will both go up to the same value\n",
                "        (the full sequence length), but the query indices will start at >0.\n",
                "        \"\"\"\n",
                "        new_seq_len, full_seq_len = attn_scores.shape[-2:]\n",
                "        assert new_seq_len <= full_seq_len\n",
                "        q_posn = einops.repeat(attn_scores.new_tensor(range(full_seq_len-new_seq_len, full_seq_len)), \"q -> q k\", k=full_seq_len)\n",
                "        k_posn = einops.repeat(attn_scores.new_tensor(range(full_seq_len)), \"k -> q k\", q=new_seq_len)\n",
                "        mask = q_posn < k_posn\n",
                "        attn_scores = attn_scores.masked_fill(mask, self.IGNORE)\n",
                "        return attn_scores\n",
                "\n",
                "\n",
                "class TransformerBlock(nn.Module):\n",
                "    def __init__(self, cfg: Config):\n",
                "        super().__init__()\n",
                "        self.cfg = cfg\n",
                "        self.ln1 = LayerNorm(cfg)\n",
                "        self.attn = Attention(cfg)\n",
                "        self.ln2 = LayerNorm(cfg)\n",
                "        self.mlp = MLP(cfg)\n",
                "\n",
                "    def forward(\n",
                "        self,\n",
                "        resid_pre: Float[Tensor, \"batch position d_model\"],\n",
                "        kv_cache_entry: Optional[KeyValueCacheTensor] = None,\n",
                "    ) -> Float[Tensor, \"batch position d_model\"]:\n",
                "\n",
                "        # SOLUTION\n",
                "        attn_out, kv_cache_entry = self.attn(self.ln1(resid_pre), kv_cache_entry)\n",
                "        resid_mid = attn_out + resid_pre\n",
                "        resid_post = self.mlp(self.ln2(resid_mid)) + resid_mid\n",
                "        return resid_post, kv_cache_entry\n",
                "\n",
                "\n",
                "\n",
                "class DemoTransformer(nn.Module):\n",
                "    def __init__(self, cfg: Config):\n",
                "        super().__init__()\n",
                "        self.cfg = cfg\n",
                "        self.embed = Embed(cfg)\n",
                "        self.pos_embed = PosEmbed(cfg)\n",
                "        self.blocks = nn.ModuleList([TransformerBlock(cfg) for _ in range(cfg.n_layers)])\n",
                "        self.ln_final = LayerNorm(cfg)\n",
                "        self.unembed = Unembed(cfg)\n",
                "\n",
                "    def forward(\n",
                "        self,\n",
                "        tokens: Int[Tensor, \"batch seq_pos\"],\n",
                "        kv_cache: Optional[KeyValueCache] = None\n",
                "    ) -> Float[Tensor, \"batch position d_vocab\"]:\n",
                "        \n",
                "        using_kv_cache = kv_cache is not None\n",
                "\n",
                "        if using_kv_cache:\n",
                "            # If using kv_cache, then we only need to pass forward the newest tokens\n",
                "            # Remember to add positional offset!\n",
                "            n_cached_tokens = kv_cache.seq_len\n",
                "            tokens = tokens[:, n_cached_tokens:]\n",
                "            residual = self.embed(tokens) + self.pos_embed(tokens, n_cached_tokens)\n",
                "        else:\n",
                "            # If not using cache, turn it into a list of None's (so we can iterate through it)\n",
                "            kv_cache = [None for _ in range(self.cfg.n_layers)]\n",
                "            residual = self.embed(tokens) + self.pos_embed(tokens)\n",
                "        \n",
                "        # Apply all layers, and create a (new) kv_cache from the key & value vectors\n",
                "        new_kv_cache_entries: List[KeyValueCacheTensor] = []\n",
                "        for block, kv_cache_entry in zip(self.blocks, kv_cache):\n",
                "            residual, kv_cache_entry = block(residual, kv_cache_entry)\n",
                "            if using_kv_cache: new_kv_cache_entries.append(kv_cache_entry)\n",
                "        \n",
                "        logits = self.unembed(self.ln_final(residual))\n",
                "        \n",
                "        if using_kv_cache:\n",
                "            return logits, KeyValueCache(t.stack(new_kv_cache_entries))\n",
                "        else:\n",
                "            return logits, None\n",
                "\n",
                "\n",
                "tokens = reference_gpt2.to_tokens(reference_text).to(device)\n",
                "logits, cache = reference_gpt2.run_with_cache(tokens)\n",
                "\n",
                "rand_int_test(PosEmbed, [2, 4])\n",
                "load_gpt2_test(PosEmbed, reference_gpt2.pos_embed, tokens)\n",
                "rand_float_test(Attention, [2, 4, 768])\n",
                "load_gpt2_test(Attention, reference_gpt2.blocks[0].attn, cache[\"normalized\", 0, \"ln1\"])\n",
                "rand_float_test(TransformerBlock, [2, 4, 768])\n",
                "load_gpt2_test(TransformerBlock, reference_gpt2.blocks[0], cache[\"resid_pre\", 0])\n",
                "rand_int_test(DemoTransformer, [2, 4])\n",
                "load_gpt2_test(DemoTransformer, reference_gpt2, tokens)\n",
                "```\n",
                "\n",
                "</details>\n",
                "\n",
                "<details>\n",
                "<summary>New sampling function</summary>\n",
                "\n",
                "```python\n",
                "@t.inference_mode()\n",
                "def sample_with_cache(\n",
                "    self: TransformerSampler,\n",
                "    prompt: str,\n",
                "    max_tokens_generated=100,\n",
                "    kv_cache: Optional[KeyValueCache] = None,\n",
                "    verbose=False,\n",
                "    seed: Optional[int] = None,\n",
                "    **kwargs\n",
                ") -> str:\n",
                "    \n",
                "    # SOLUTION\n",
                "    self.model.eval()\n",
                "    input_ids = self.tokenizer.encode(prompt, return_tensors=\"pt\").to(device)[0]\n",
                "    if seed is not None:\n",
                "        np.random.seed(seed)\n",
                "        t.manual_seed(seed)\n",
                "\n",
                "    for i in tqdm(range(max_tokens_generated)):\n",
                "        # Get new logits (make sure we don't pass in more tokens than the model's context length)\n",
                "        logits, kv_cache = self.model(input_ids[None, -self.cfg.n_ctx:], kv_cache)\n",
                "        # We only take logits for the last token, because this is what we're sampling\n",
                "        logits = logits[0, -1]\n",
                "        # Get next token (as a tensor of size (1, 1) so we can concat it to input_ids)\n",
                "        next_token = t.tensor([TransformerSampler.sample_next_token(input_ids, logits, **kwargs)], device=device)\n",
                "        # Create new input ids string, with shape (1, old_seq_len + 1)\n",
                "        input_ids = t.cat([input_ids, next_token], dim=-1)\n",
                "        # Print out results, if required\n",
                "        if verbose:\n",
                "            print(self.tokenizer.decode(input_ids), end=\"\\r\")\n",
                "        # If our new token was the end-of-text token, stop\n",
                "        if next_token == getattr(self.tokenizer, \"eos_token_id\", None):\n",
                "            break\n",
                "    \n",
                "    return self.tokenizer.decode(input_ids)\n",
                "\n",
                "\n",
                "TransformerSampler.sample = sample_with_cache\n",
                "```\n",
                "</details>\n",
                "\n",
                "<details>\n",
                "<summary>Code to verify that the same output is being produced by cache and no-cache versions (and to compare speeds)</summary>\n",
                "\n",
                "```python\n",
                "device = t.device(\"cuda\") # can also try \"cpu\"\n",
                "\n",
                "model = DemoTransformer(Config()).to(device)\n",
                "model.load_state_dict(reference_gpt2.state_dict(), strict=False);\n",
                "\n",
                "initial_text = \"Eliezer Shlomo Yudkowsky (born September 11, 1979) is an American decision and artificial intelligence (AI) theorist and writer, best known for\"\n",
                "# input_ids = tokenizer.encode(initial_text, return_tensors=\"pt\").squeeze()\n",
                "\n",
                "sampler = TransformerSampler(model, tokenizer)\n",
                "\n",
                "# Run the noncached version\n",
                "t0 = time.time()\n",
                "text = sampler.sample(\n",
                "    initial_text,\n",
                "    temperature=0.7,\n",
                "    top_p=0.95,\n",
                "    seed=0,\n",
                ")\n",
                "print(f\"Time taken (without cache): {time.time() - t0:.2f} seconds\")\n",
                "rprint(f\"Model output:\\n\\n[bold dark_orange]{text}[/]\")\n",
                "\n",
                "# Run the cached version\n",
                "t0 = time.time()\n",
                "text_with_cache = sampler.sample(\n",
                "    initial_text,\n",
                "    temperature=0.7,\n",
                "    top_p=0.95,\n",
                "    seed=0,\n",
                "    kv_cache=KeyValueCache.new_empty(sampler.cfg)\n",
                ")\n",
                "print(f\"Time taken (with cache): {time.time() - t0:.2f} seconds\")\n",
                "rprint(f\"Model output:\\n\\n[bold dark_orange]{text_with_cache}[/]\")\n",
                "\n",
                "# # Check they are the same\n",
                "assert text == text_with_cache, \"Your outputs are different, meaning you've probably made a mistake in your cache implementation (or failed to use random seeds).\"\n",
                "print(\"Tests passed!\")\n",
                "```\n",
                "\n",
                "</details>\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "You may find that your cache implementation provides a modest speedup, but probably not close to the `seq_len`-factor speedup you'd expect from the fact that you only compute one additional token at each step rather than all of them. Why is this? The answer is that, much like everything to do with computational and memory costs in deep learning, it's not so simple. There are a host of different factors which might be bottlenecking our model's forward pass speed. If you try this on the CPU, you should get a much more noticeable speedup.\n",
                "\n",
                "For a bit more on these topics, see [here](https://kipp.ly/blog/transformer-inference-arithmetic/#kv-cache).\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Bonus - cached beam search\n",
                "\n",
                "Can you modify your beam search function to use caching?\n",
                "\n",
                "Depending on how you implemented your cache earlier, you might find that a different form of caching is better suited to beam search.\n",
                "\n",
                "Again, we've provided an example implementation in a dropdown below, which is based on the cache implementation above and the previous solution for `beam_search`.\n",
                "\n",
                "<details>\n",
                "<summary>Cached beam search function</summary>\n",
                "\n",
                "As we touched on earlier, thanks to our modular code, not a lot needs to be changed when adding cache support.\n",
                "\n",
                "```python\n",
                "@dataclass\n",
                "class Beams:\n",
                "    \"\"\"Class to store beams during beam search.\"\"\"\n",
                "    model: DemoTransformer\n",
                "    tokenizer: GPT2TokenizerFast\n",
                "    logprob_sums: Float[Tensor, \"batch\"]\n",
                "    tokens: Int[Tensor, \"batch seq\"]\n",
                "    kv_cache: Optional[KeyValueCache] = None\n",
                "\n",
                "    def new_beams(self, logprob_sums, tokens, kv_cache) -> \"Beams\":\n",
                "        \"\"\"Creates a new Beams object with the same model and tokenizer.\"\"\"\n",
                "        return Beams(self.model, self.tokenizer, logprob_sums, tokens, kv_cache)\n",
                "\n",
                "    def __getitem__(self, idx) -> \"Beams\":\n",
                "        \"\"\"Helpful function allowing you to take a slice of the beams object along the batch dimension.\"\"\"\n",
                "        return self.new_beams(\n",
                "            self.logprob_sums[idx],\n",
                "            self.tokens[idx],\n",
                "            self.kv_cache[:, :, idx] if self.kv_cache is not None else None\n",
                "        )\n",
                "\n",
                "    @property\n",
                "    def logprobs_and_completions(self) -> List[Tuple[float, str]]:\n",
                "        \"\"\"Returns self as a list of logprob sums and completions (useful for getting final output).\"\"\"\n",
                "        return [\n",
                "            (logprob_sum.item(), self.tokenizer.decode(tokens))\n",
                "            for (logprob_sum, tokens) in zip(self.logprob_sums, self.tokens)\n",
                "        ]\n",
                "    \n",
                "\n",
                "    def generate(self, new_beams: int, no_repeat_ngram_size: Optional[int] = None) -> \"Beams\":\n",
                "        \"\"\"\n",
                "        Starting from the current set of beams (which has length `num_beams`), returns a new\n",
                "        set of `num_beams * new_beams`, containing the best `new_beams` continuations for each\n",
                "        of the original beams.\n",
                "\n",
                "        Optional argument `no_repeat_ngram_size` means your model won't generate any sequences with\n",
                "        a repeating n-gram of this length (don't worry about implementing this until later).\n",
                "        \"\"\"\n",
                "        # SOLUTION\n",
                "\n",
                "        # Get the output logprobs for the next token (for every sequence in current beams)\n",
                "        logprobs, kv_cache = self.model(self.tokens, self.kv_cache)\n",
                "        logprobs = logprobs[:, -1, :].log_softmax(-1)\n",
                "\n",
                "        # Get the top `new_beams` tokens for each sequence\n",
                "        topk_logprobs, topk_tokenIDs = self.get_topk_non_repeating(logprobs, no_repeat_ngram_size, k=new_beams)\n",
                "\n",
                "        # Get all of the new possible beams, via einops operations\n",
                "        #   Here, we're effectively flattening out the batch dimension and k dimension, to give us tensors\n",
                "        #   with every possible combination of (original sequence, new token) pairs.)\n",
                "        new_logprob_sums = sum([\n",
                "            einops.repeat(self.logprob_sums, \"batch -> (batch k)\", k=new_beams),\n",
                "            einops.rearrange(topk_logprobs, \"batch k -> (batch k)\")\n",
                "        ])\n",
                "        new_tokens = t.concat([\n",
                "            einops.repeat(self.tokens, \"batch seq -> (batch k) seq\", k=new_beams),\n",
                "            einops.rearrange(topk_tokenIDs, \"batch k -> (batch k) 1\")\n",
                "        ], dim=-1)\n",
                "        new_kv_cache = None if (self.kv_cache is None) else einops.repeat(\n",
                "            kv_cache, \"layer k_and_v batch ... -> layer k_and_v (batch k) ...\", k=new_beams\n",
                "        )\n",
                "        return self.new_beams(new_logprob_sums, new_tokens, new_kv_cache)\n",
                "\n",
                "\n",
                "    def filter(self, num_beams: int) -> Tuple[\"Beams\", \"Beams\"]:\n",
                "        \"\"\"\n",
                "        Returns:\n",
                "            best_beams: Beams\n",
                "                filtered version of self, containing all best `num_beams` which are also not terminated.\n",
                "\n",
                "            early_terminations: Beams\n",
                "                filtered version of self, containing all best `num_beams` which are also terminated.\n",
                "                i.e. the sum of lengths of these two should equal `num_beams`.\n",
                "        \"\"\"\n",
                "        # SOLUTION\n",
                "\n",
                "        # Get the indices of top `num_beams` beams\n",
                "        top_beam_indices = self.logprob_sums.topk(k=num_beams, dim=0).indices.tolist()\n",
                "        # Get the indices of terminated sequences\n",
                "        new_tokens = self.tokens[:, -1]\n",
                "        terminated_indices = t.nonzero(new_tokens == self.tokenizer.eos_token_id)\n",
                "\n",
                "        # Get the indices of the `num_beams` best sequences (some terminated, some not terminated)\n",
                "        best_continuing = [i for i in top_beam_indices if i not in terminated_indices]\n",
                "        best_terminated = [i for i in top_beam_indices if i in terminated_indices]\n",
                "\n",
                "        # Return the beam objects from these indices\n",
                "        return self[best_continuing], self[best_terminated]\n",
                "\n",
                "            \n",
                "    def get_topk_non_repeating(\n",
                "        self,\n",
                "        logprobs: Float[Tensor, \"batch d_vocab\"],\n",
                "        no_repeat_ngram_size: Optional[int],\n",
                "        k: int,\n",
                "    ) -> Tuple[Float[Tensor, \"k\"], Int[Tensor, \"k\"]]:\n",
                "        \"\"\"\n",
                "        logprobs:\n",
                "            tensor of the log-probs for the next token\n",
                "        no_repeat_ngram_size:\n",
                "            size of ngram to avoid repeating\n",
                "        k:\n",
                "            number of top logits to return, for each beam in our collection\n",
                "\n",
                "        Returns:\n",
                "            equivalent to the output of `logprobs.topk(dim=-1)`, but makes sure\n",
                "            that no returned tokens would produce an ngram of size  `no_repeat_ngram_size`\n",
                "            which has already appeared in `self.tokens`.\n",
                "        \"\"\"\n",
                "        batch, seq_len = self.tokens.shape\n",
                "        neg_inf = t.tensor(-1.0e4).to(device)\n",
                "\n",
                "        # If completion isn't long enough for a repetition, or we have no restructions, just return topk\n",
                "        if (no_repeat_ngram_size is not None) and (seq_len > no_repeat_ngram_size-1):\n",
                "            # Otherwise, we need to check for ngram repetitions\n",
                "            # First, get the most recent `no_repeat_ngram_size-1` tokens\n",
                "            last_ngram_prefix = self.tokens[:, seq_len - (no_repeat_ngram_size-1):]\n",
                "            # Next, find all the tokens we're not allowed to generate (by going iterating through past ngrams and seeing if those ngram prefixes match the last one)\n",
                "            for i in range(seq_len - (no_repeat_ngram_size-1)):\n",
                "                ngrams = self.tokens[:, i:i+no_repeat_ngram_size] # (batch, ngram)\n",
                "                ngrams_are_repeated = (ngrams[:, :-1] == last_ngram_prefix).all(-1) # (batch,)\n",
                "                ngram_end_tokens = ngrams[:, [-1]] # (batch, 1)\n",
                "                # Fill logprobs with neginf wherever the ngrams are repeated\n",
                "                logprobs[range(batch), ngram_end_tokens] = t.where(\n",
                "                    ngrams_are_repeated,\n",
                "                    neg_inf,\n",
                "                    logprobs[range(batch), ngram_end_tokens],\n",
                "            )\n",
                "\n",
                "        # Finally, get our actual tokens\n",
                "        return logprobs.topk(k=k, dim=-1)\n",
                "\n",
                "    def print(self, title=\"Best completions\", max_print_chars=80) -> None:\n",
                "        \"\"\"\n",
                "        Prints out a set of sequences with their corresponding logitsums.\n",
                "        \"\"\"\n",
                "        if len(self.tokens) == 0:\n",
                "            return\n",
                "        table = Table(\"logitsum\", \"completion\", title=title)\n",
                "        for logprob_sum, tokens in zip(self.logprob_sums, self.tokens):\n",
                "            text = self.tokenizer.decode(tokens)\n",
                "            if len(repr(text)) > max_print_chars:\n",
                "                text = text[:int(0.3 * max_print_chars)] + \" ... \" + text[-int(0.7 * max_print_chars):]\n",
                "            table.add_row(f\"{logprob_sum:>8.3f}\", repr(text))\n",
                "        rprint(table)\n",
                "\n",
                "\n",
                "@t.inference_mode()\n",
                "def beam_search(\n",
                "    self: TransformerSampler,\n",
                "    prompt: str,\n",
                "    num_return_sequences: int,\n",
                "    num_beams: int,\n",
                "    max_new_tokens: int,\n",
                "    no_repeat_ngram_size: Optional[int] = None,\n",
                "    kv_cache: Optional[KeyValueCache] = None,\n",
                "    verbose=False\n",
                ") -> List[Tuple[float, Tensor]]:\n",
                "    \"\"\"\n",
                "    Implements a beam search, by repeatedly performing the `generate` and `filter` steps (starting\n",
                "    from the initial prompt) until either of the two stopping criteria are met:\n",
                "\n",
                "        (1) we've generated `max_new_tokens` tokens, or\n",
                "        (2) we've generated `num_returns_sequences` terminating sequences.\n",
                "\n",
                "    To modularize this function, most of the actual complexity is in the Beams class,\n",
                "    in the `generate` and `filter` methods.\n",
                "    \"\"\"\n",
                "\n",
                "    assert num_return_sequences <= num_beams\n",
                "    self.model.eval()\n",
                "\n",
                "    # SOLUTION\n",
                "    tokens = self.tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
                "\n",
                "    # List for final beams to return (and early terminations)\n",
                "    final_logprobs_and_completions: List[Tuple[float, str]] = []\n",
                "    # Keep track of all best beams after each step\n",
                "    best_beams = Beams(self.model, self.tokenizer, t.tensor([0.0]).to(device), tokens, kv_cache)\n",
                "\n",
                "    for n in tqdm(range(max_new_tokens)):\n",
                "\n",
                "        # Generation step\n",
                "        best_beams = best_beams.generate(num_beams, no_repeat_ngram_size)\n",
                "\n",
                "        # Filtering step\n",
                "        best_beams, best_beams_terminated = best_beams.filter(num_beams)\n",
                "        final_logprobs_and_completions.extend(best_beams_terminated.logprobs_and_completions)\n",
                "\n",
                "        # Print output\n",
                "        if verbose:\n",
                "            best_beams.print()\n",
                "\n",
                "        # Check stopping condition\n",
                "        if len(final_logprobs_and_completions) >= num_return_sequences:\n",
                "            return final_logprobs_and_completions[:num_return_sequences]\n",
                "\n",
                "    final_logprobs_and_completions.extend(best_beams.logprobs_and_completions)\n",
                "    final_logprobs_and_completions = final_logprobs_and_completions[:num_return_sequences]\n",
                "    return final_logprobs_and_completions\n",
                "\n",
                "\n",
                "TransformerSampler.beam_search = beam_search\n",
                "```\n",
                "\n",
                "</details>\n",
                "\n",
                "<details>\n",
                "<summary>Code to verify that the same output is being produced by cache and no-cache versions (and to compare speeds)</summary>\n",
                "\n",
                "```python\n",
                "prompt = \"For you, the day Bison graced your village was the most important day of your life. But for me, it was\"\n",
                "orig_len = len(tokenizer.encode(prompt))\n",
                "\n",
                "beam_search_kwargs = dict(\n",
                "    prompt=prompt,\n",
                "    num_return_sequences=3,\n",
                "    num_beams=20,\n",
                "    max_new_tokens=60,\n",
                "    no_repeat_ngram_size=2,\n",
                "    verbose=False\n",
                ")\n",
                "\n",
                "sampler = TransformerSampler(model, tokenizer)\n",
                "\n",
                "# Run the noncached version\n",
                "t0 = time.time()\n",
                "final_logitsums_and_completions = sampler.beam_search(**beam_search_kwargs)\n",
                "logprob_sum, text = final_logitsums_and_completions[0]\n",
                "avg_logprob_as_prob = t.tensor(logprob_sum / (len(tokenizer.encode(text)) - orig_len)).exp().item()\n",
                "print(f\"Time (without cache): {time.time() - t0:.2f} seconds\")\n",
                "print(f\"Avg logprob (expressed as a probability) = {avg_logprob_as_prob:.3f}\")\n",
                "rprint(f\"Output:\\n\\n[bold dark_orange]{text}[/]\\n\\n\")\n",
                "\n",
                "# Run the cached version\n",
                "t0 = time.time()\n",
                "beam_search_kwargs[\"kv_cache\"] = KeyValueCache.new_empty(model.cfg)\n",
                "final_logitsums_and_completions = sampler.beam_search(**beam_search_kwargs)\n",
                "logprob_sum, text_with_cache = final_logitsums_and_completions[0]\n",
                "avg_logprob_as_prob = t.tensor(logprob_sum / (len(tokenizer.encode(text)) - orig_len)).exp().item()\n",
                "print(f\"Time (with cache): {time.time() - t0:.2f} seconds\")\n",
                "print(f\"Avg logprob (as probability) = {avg_logprob_as_prob:.3f}\", end=\"\")\n",
                "rprint(f\"Output:\\n\\n[bold dark_orange]{text_with_cache}[/]\\n\\n\")\n",
                "\n",
                "# Check they are the same\n",
                "assert text == text_with_cache, \"Your outputs are different, meaning you've probably made a mistake in your cache implementation.\"\n",
                "print(\"Tests passed!\")\n",
                "```\n",
                "\n",
                "</details>\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "rahulenv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.9.16"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}
